{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Test (numerical) gradients\n",
    "\n",
    "Differentiate through rollouts over different lengths. Answer the following questions:\n",
    "-  How do gradients of $G$ compare to finite difference gradients of $F$? Make plots showing FD gradients of various perturbation sizes. Repeat this for increasing rollout time $T$, until both the FD and autograd gradients become numerically unstable.\n",
    "- Compare FD and $G$-autograd gradients to symbolic gradients of $F$. As a test case, include a system with a closed form solution to its diffeqs, or better yet, to its discretized iterations. System of linear diffeqs seems to fit this. If that's too easy for a NN to learn, do a static invertible differentiable transformation of state space, so the NN has to learn nonlinear dynamics but we can still get accurate symbolic gradients.\n",
    "- Discuss how For L96 true symbolic gradients may be high order polynomials, so $G$ might be `better' in the sense of giving a closer match to FDs when put into numerical practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device, as_tensor\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate some toy data\n",
    "- note we're simulating with the numba simulator, so that one will potentially have an edge over the pytorch emulator in predicting it's own output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data\n",
    "from L96_emulator.run import sel_dataset_class\n",
    "\n",
    "K, J, T, dt, N_trials = 40, 0, 605, 0.05, 1\n",
    "spin_up_time, train_frac = 5., 0.8\n",
    "normalize_data = False\n",
    "\n",
    "F, h, b, c = 8., 1., 10., 10.\n",
    "\n",
    "out, datagen_dict = get_data(K=K, J=J, T=T, dt=dt, N_trials=N_trials, F=F, h=h, b=b, c=c, \n",
    "                             resimulate=True, solver=rk4_default,\n",
    "                             save_sim=False, data_dir=data_dir)\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task, N_trials=N_trials)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(T/dt*train_frac)))\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "if N_trials > 1:\n",
    "    plt.imshow(out[0].T, aspect='auto')\n",
    "else:\n",
    "    plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences\n",
    "- scipy.optimize.approx_fprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "from L96_emulator.util import predictor_corrector, rk4_default\n",
    "import time\n",
    "\n",
    "\n",
    "n_trials = 20 # number of parallel solves\n",
    "n_starts = 5*int(spin_up_time/dt) * np.arange(1,n_trials+1) # initial conditions, taken from long simulation\n",
    "n_rollout = 50 # rollout steps\n",
    "\n",
    "from L96sim.L96_base import f1, pf2 # parallelized numba simulator\n",
    "\n",
    "dX_dt = np.empty((K*(J+1), n_trials), dtype=dtype_np)\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return pf2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "             \n",
    "times = dt * np.arange(0, n_rollout+1) # time points for numba solver, will only use np.diff(times)\n",
    "\n",
    "def pred(x):\n",
    "    return rk4_default(fun=fun, y0=x.copy().reshape(-1,n_trials), times=times)[-1,:]\n",
    "\n",
    "def loss_np(x, target): \n",
    "    return ((pred(x)-target)**2).mean()\n",
    "\n",
    "target = out[n_starts+n_rollout].T.copy() # predict after rollout\n",
    "x = out[n_starts].T.copy()                # initialization estimate\n",
    "\n",
    "# potentially corrupt initialization estimate (for non-zero gradients...)\n",
    "eps = np.ones((x.shape))\n",
    "eps[:K,:] *= np.mean(x[:K,:])\n",
    "eps[K:,:] *= np.mean(x[K:,:])\n",
    "x += np.random.normal(size=x.shape) * ( 0.1 * eps ) # corrupting with 0.1 std noise\n",
    "\n",
    "def loss_sp(x):\n",
    "    return loss_np(x, target=target)\n",
    "\n",
    "# finite-difference gradients\n",
    "t = time.time()\n",
    "fprime_sp = approx_fprime(xk=x.reshape(-1), f=loss_sp, epsilon=1e-5).reshape(-1, len(n_starts))\n",
    "print(f'took {time.time()-t}s')\n",
    "\n",
    "plt.plot(fprime_sp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emulator gradients\n",
    "- loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.networks import named_network\n",
    "from L96_emulator.eval import load_model_from_exp_conf, named_network\n",
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 26\n",
    "\n",
    "if exp_id is None: \n",
    "    # loading 'perfect' (up to machine-precision-level quirks) L96 model in pytorch\n",
    "    model, model_forwarder = named_network(\n",
    "        model_name='MinimalConvNetL96',\n",
    "        n_input_channels=J+1,\n",
    "        n_output_channels=J+1,\n",
    "        seq_length=1,\n",
    "        **{'filters': [0],\n",
    "           'kernel_sizes': [4],\n",
    "           'init_net': 'analytical',\n",
    "           'K_net': K,\n",
    "           'J_net': J,\n",
    "           'dt_net': 0.05,\n",
    "           'model_forwarder': 'rk4_default'}\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # loading trained L96 model in pytorch\n",
    "    exp_names = os.listdir('experiments/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "    print('conf_exp', conf_exp)\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "\n",
    "    args['model_forwarder'] = 'rk4_default'  # update numerical integration method to RK4\n",
    "    args['dt_net'] = dt                      # with current step size\n",
    "\n",
    "    model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "    if not training_outputs is None:\n",
    "        training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        seq_length = args['seq_length']\n",
    "        plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "        plt.title('training')\n",
    "        plt.ylabel('validation error')\n",
    "        plt.legend()\n",
    "        fig.patch.set_facecolor('xkcd:white')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels, Rollout\n",
    "\n",
    "def loss_torch(x, target):\n",
    "    roller_outer.X = x    \n",
    "    x_pred = roller_outer.forward()\n",
    "    return ((x_pred - target)**2).mean()\n",
    "\n",
    "# rollout operator\n",
    "roller_outer = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                       N=n_trials, T=n_rollout)\n",
    "\n",
    "x_torch = as_tensor(sortL96intoChannels(np.atleast_2d(x.T),J=J))\n",
    "x_torch = torch.nn.Parameter(x_torch)\n",
    "target_torch = as_tensor(sortL96intoChannels(np.atleast_2d(target.T),J=J))\n",
    "\n",
    "def loss_(x):\n",
    "    return loss_torch(x, target=target_torch)\n",
    "\n",
    "# torch loss\n",
    "t = time.time()\n",
    "loss = loss_(x_torch)\n",
    "loss.backward()\n",
    "grad_torch = sortL96fromChannels(roller_outer.X.grad.detach().cpu().numpy()).reshape(n_trials, -1).T\n",
    "print(f'took {time.time()-t}s')\n",
    "\n",
    "plt.plot(grad_torch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "for i in range(n_trials):\n",
    "    plt.subplot(np.ceil(n_trials/2), 2, i+1)\n",
    "    plt.plot(grad_torch[:,i], label='torch')\n",
    "    plt.plot(fprime_sp[:,i], '--', label='scipy fd')\n",
    "plt.subplot(np.ceil(n_trials/2), 2, 1)\n",
    "plt.legend()\n",
    "plt.suptitle(f'rollout through {n_rollout} steps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# share notebook results via html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html numerical_gradients.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
