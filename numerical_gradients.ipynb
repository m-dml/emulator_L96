{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate some toy data\n",
    "- note we're simulating with the numba simulator, so that one will potentially have an edge over the pytorch emulator in predicting it's own output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, pf2\n",
    "from L96_emulator.util import predictor_corrector, rk4_default\n",
    "from L96_emulator.run import sel_dataset_class\n",
    "\n",
    "try: \n",
    "    K, J, T, dt = args['K'], args['J'], args['T'], args['dt']\n",
    "    spin_up_time, train_frac = args['spin_up_time'], args['train_frac']\n",
    "    normalize_data = bool(args['normalize_data'])\n",
    "except:\n",
    "    K, J, T, dt = 36, 10, 605, 0.01\n",
    "    spin_up_time, train_frac = 5., 0.8\n",
    "    normalize_data = False\n",
    "\n",
    "F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "resimulate, save_sim = True, True\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,50)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, int(np.floor(T/dt)+1))\n",
    "    \n",
    "    out = rk4_default(fun=fun, y0=X_init.copy(), times=times)\n",
    "\n",
    "    # filename for data storage\n",
    "    if save_sim: \n",
    "        np.save(data_dir + fn_data, out.astype(dtype=dtype_np))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(out.shape[0]*train_frac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finite differences\n",
    "- scipy.optimize.approx_fprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import approx_fprime\n",
    "from L96_emulator.util import predictor_corrector, rk4_default\n",
    "import time\n",
    "\n",
    "\n",
    "n_trials = 20 # number of parallel solves\n",
    "n_starts = 5*int(spin_up_time/dt) * np.arange(1,n_trials+1) # initial conditions, taken from long simulation\n",
    "n_rollout = 50 # rollout steps\n",
    "\n",
    "from L96sim.L96_base import pf2 # parallelized numba simulator\n",
    "\n",
    "dX_dt = np.empty((K*(J+1), n_trials), dtype=dtype_np)\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return pf2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "             \n",
    "times = dt * np.arange(0, n_rollout+1) # time points for numba solver, will only use np.diff(times)\n",
    "\n",
    "def pred(x):\n",
    "    return rk4_default(fun=fun, y0=x.copy().reshape(-1,n_trials), times=times)[-1,:]\n",
    "\n",
    "def loss_np(x, target): \n",
    "    return ((pred(x)-target)**2).mean()\n",
    "\n",
    "target = out[n_starts+n_rollout].T.copy() # predict after rollout\n",
    "x = out[n_starts].T.copy()                # initialization estimate\n",
    "\n",
    "# potentially corrupt initialization estimate (for non-zero gradients...)\n",
    "eps = np.ones((x.shape))\n",
    "eps[:K,:] *= np.mean(x[:K,:])\n",
    "eps[K:,:] *= np.mean(x[K:,:])\n",
    "x += np.random.normal(size=x.shape) * ( 0.1 * eps ) # corrupting with 0.1 std noise\n",
    "\n",
    "def loss_sp(x):\n",
    "    return loss_np(x, target=target)\n",
    "\n",
    "# finite-difference gradients\n",
    "t = time.time()\n",
    "fprime_sp = approx_fprime(xk=x.reshape(-1), f=loss_sp, epsilon=1e-5).reshape(-1, len(n_starts))\n",
    "print(f'took {time.time()-t}s')\n",
    "\n",
    "plt.plot(fprime_sp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# emulator gradients\n",
    "- loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.networks import named_network\n",
    "from L96_emulator.eval import load_model_from_exp_conf, named_network\n",
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 20\n",
    "\n",
    "if exp_id is None: \n",
    "    # loading 'perfect' (up to machine-precision-level quirks) L96 model in pytorch\n",
    "    model, model_forwarder = named_network(\n",
    "        model_name='MinimalConvNetL96',\n",
    "        n_input_channels=J+1,\n",
    "        n_output_channels=J+1,\n",
    "        seq_length=1,\n",
    "        **{'filters': [0],\n",
    "           'kernel_sizes': [4],\n",
    "           'init_net': 'analytical',\n",
    "           'K_net': 36,\n",
    "           'J_net': 10,\n",
    "           'dt_net': 0.01,\n",
    "           'model_forwarder': 'rk4_default'}\n",
    "    )\n",
    "\n",
    "else:\n",
    "    # loading trained L96 model in pytorch\n",
    "    exp_names = os.listdir('experiments/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "    print('conf_exp', conf_exp)\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "\n",
    "    args['model_forwarder'] = 'rk4_default'  # update numerical integration method to RK4\n",
    "    args['dt_net'] = dt                      # with current step size\n",
    "\n",
    "    model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "    if not training_outputs is None:\n",
    "        training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        seq_length = args['seq_length']\n",
    "        plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "        plt.title('training')\n",
    "        plt.ylabel('validation error')\n",
    "        plt.legend()\n",
    "        fig.patch.set_facecolor('xkcd:white')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels, Rollout\n",
    "\n",
    "\n",
    "def loss_torch(x, target):\n",
    "    roller_outer.X = x    \n",
    "    x_pred = roller_outer.forward()\n",
    "    return ((x_pred - target)**2).mean()\n",
    "\n",
    "# rollout operator\n",
    "roller_outer = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                       N=n_trials, T=n_rollout)\n",
    "\n",
    "x_torch = torch.as_tensor(sortL96intoChannels(np.atleast_2d(x.T),J=J), \n",
    "                          dtype=dtype, \n",
    "                          device=device)\n",
    "x_torch = torch.nn.Parameter(x_torch)\n",
    "target_torch = torch.as_tensor(sortL96intoChannels(np.atleast_2d(target.T),J=J),\n",
    "                               dtype=dtype,\n",
    "                               device=device)\n",
    "\n",
    "def loss_(x):\n",
    "    return loss_torch(x, target=target_torch)\n",
    "\n",
    "# torch loss\n",
    "t = time.time()\n",
    "loss = loss_(x_torch)\n",
    "loss.backward()\n",
    "grad_torch = sortL96fromChannels(roller_outer.X.grad.detach().cpu().numpy()).reshape(n_trials, -1).T\n",
    "print(f'took {time.time()-t}s')\n",
    "\n",
    "plt.plot(grad_torch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,24))\n",
    "for i in range(n_trials):\n",
    "    plt.subplot(np.ceil(n_trials/2), 2, i+1)\n",
    "    plt.plot(grad_torch[:,i], label='torch')\n",
    "    plt.plot(fprime_sp[:,i], '--', label='scipy fd')\n",
    "plt.subplot(np.ceil(n_trials/2), 2, 1)\n",
    "plt.legend()\n",
    "plt.suptitle(f'rollout through {n_rollout} steps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
