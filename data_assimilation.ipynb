{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, J1, J1_init, f1_juliadef, f2_juliadef\n",
    "from L96_emulator.util import predictor_corrector\n",
    "from L96_emulator.run import sel_dataset_class\n",
    "\n",
    "F, h, b, c = 10, 1, 10, 10\n",
    "K, J, T, dt = 36, 10, 65, 0.001\n",
    "spin_up_time, train_frac = 5., 0.8\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "resimulate, save_sim = True, False\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,10)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, int(np.floor(T/dt)+1))\n",
    "\n",
    "    out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "    # filename for data storage\n",
    "    if save_sim: \n",
    "        np.save(data_dir + fn_data, out.astype(dtype=dtype_np))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=False, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(out.shape[0]*train_frac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# symmetric solver (one-level Lorenz-96 for now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## leapfrog \n",
    "- method for 2nd-order differential equation, so let's try getting the second derivative... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, J1, J1_init, f1_juliadef, f2_juliadef\n",
    "from L96_emulator.util import predictor_corrector\n",
    "from L96_emulator.run import sel_dataset_class\n",
    "\n",
    "F, h, b, c = 10, 1, 10, 10\n",
    "K, J, T, dt = 36, 0, 65, 0.001\n",
    "spin_up_time, train_frac = 5., 0.8\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "resimulate, save_sim = True, False\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,10)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, int(np.floor(T/dt)+1))\n",
    "\n",
    "    out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "    # filename for data storage\n",
    "    if save_sim: \n",
    "        np.save(data_dir + fn_data, out.astype(dtype=dtype_np))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import AnalyticModel_oneLevel, AnalyticModel_twoLevel\n",
    "\n",
    "model = AnalyticModel_oneLevel(K=K)#, J=J)\n",
    "X_init = F * (0.5 + np.random.randn(K) * 1.0).astype(dtype=dtype_np) / np.maximum(J,10)\n",
    "dX_dt = np.empty(K, dtype=X_init.dtype)\n",
    "    \n",
    "kplus1, kminus1, kminus2 = model.td_mat(K,1), model.td_mat(K,-1), model.td_mat(K,-2)\n",
    "d2fdt2 = lambda x: - fun(0,x) - kminus1.dot(fun(0,x).copy())*(kplus1-kminus2).dot(x) - (kplus1-kminus2).dot(fun(0,x).copy())*kminus1.dot(x)\n",
    "# check second derivative numerically\n",
    "T = 5000\n",
    "plt.plot((fun(0.,out[T+1]).copy()-fun(0.,out[T]))/dt)\n",
    "plt.plot(-d2fdt2(out[T]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.001\n",
    "T = 1.0\n",
    "times = np.linspace(0, T, int(np.floor(T/dt)+1))\n",
    "\n",
    "def fun2(t,x):\n",
    "    return - d2fdt2(x)\n",
    "\n",
    "def leapfrog(fun, y0, z0, times):\n",
    "    \n",
    "    y = np.zeros((len(times), *y0.shape), dtype=y0.dtype)\n",
    "    z = np.zeros_like(y)\n",
    "    y[0] = y0\n",
    "    z[0] = z0\n",
    "    for i in range(1,len(times)):        \n",
    "        dt = times[i] - times[i-1]\n",
    "\n",
    "        a = fun(times[i-1], y[i-1]).copy()\n",
    "        y[i] = y[i-1] + dt * z[i-1] + dt**2/2. * a\n",
    "        z[i] = z[i-1] + dt * (a + fun(times[i], y[i]))/2.\n",
    "\n",
    "    return y, z\n",
    "\n",
    "out2, _ = leapfrog(fun=fun2, y0=X_init.copy(), z0=fun(0., X_init).copy(), times=times)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out2.T, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96_emulator.networks import MinimalNetL96\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch \n",
    "\n",
    "model = MinimalNetL96(K,J,F,b,c,h,skip_conn=True,loc=1e3)\n",
    "std_out = torch.as_tensor(dg_train.std, device=device, dtype=dtype)\n",
    "mean_out = torch.as_tensor(dg_train.mean, device=device, dtype=dtype)\n",
    "\n",
    "def model_forward(x):\n",
    "    alpha = 0.5\n",
    "    ndim = x.ndim\n",
    "\n",
    "    x = sortL96fromChannels(x * std_out + mean_out) if ndim == 3 else x\n",
    "\n",
    "    f0 = model.forward(x)\n",
    "    f1 = model.forward(x + dt*f0)\n",
    "\n",
    "    x = x + dt * (alpha*f0 + (1-alpha)*f1)\n",
    "    x = (sortL96intoChannels(x, J=J) - mean_out) / std_out\n",
    "\n",
    "    return  sortL96fromChannels(x) if ndim == 2 else x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "from L96_emulator.eval import solve_from_init\n",
    "\n",
    "model_simulate = get_rollout_fun(dg_train, model_forward, prediction_task)\n",
    "\n",
    "T_start, T_dur = 100*int(spin_up_time/dt), 10000\n",
    "out_model = model_simulate(y0=dg_train[T_start].copy(), \n",
    "                           dy0=dg_train[T_start]-dg_train[T_start-dg_train.offset],\n",
    "                           T=T_dur)\n",
    "out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "solver_comparison = True \n",
    "if solver_comparison:\n",
    "    try: \n",
    "        print(F, h, b, c)\n",
    "    except: \n",
    "        F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "    out2 = solve_from_init(K, J, \n",
    "                           T_burnin=T_start, T_=T_dur, dt=dt, \n",
    "                           F=F, h=h, b=b, c=c, \n",
    "                           data=out, dilation=2, norm_mean=0., norm_std=1.)\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out2, T_start=T_start, T_dur=T_dur, K=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a fully-observed inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T, N = 10, len(T_start)\n",
    "\n",
    "roller_outer = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N)\n",
    "x_init = roller_outer.X.detach().numpy().copy()\n",
    "\n",
    "target = torch.as_tensor(out[T_start+T], dtype=dtype, device=device)\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 5e-2, 0.\n",
    "roller_outer.train()\n",
    "optimizer = torch.optim.Adam(roller_outer.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_vals = np.zeros(n_steps)\n",
    "for i in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = ((roller_outer.forward(T=T) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals[i] = loss.detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM, solve across full rollout time in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "x_init = out[T_start+T_rollout].copy()\n",
    "roller_outer_ADAM = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "x_init = roller_outer_ADAM.X.detach().cpu().numpy().copy()\n",
    "\n",
    "target = torch.as_tensor(out[T_start+T_rollout], dtype=dtype, device=device)\n",
    "\n",
    "n_steps, lr, weight_decay = 2000, 0.01, 0.0\n",
    "roller_outer_ADAM.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(roller_outer_ADAM.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "print(((roller_outer_ADAM.forward(T=T_rollout).detach().cpu().numpy() - out[T_start+T_rollout])**2).mean())\n",
    "\n",
    "loss_vals_ADAM = np.zeros(n_steps)\n",
    "time_vals_ADAM = time.time() * np.ones(n_steps)\n",
    "for i in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = ((roller_outer_ADAM.forward(T=T_rollout) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals_ADAM[i] = loss.detach().cpu().numpy()\n",
    "        time_vals_ADAM[i] = time.time() - time_vals_ADAM[i]\n",
    "        print((time_vals_ADAM[i], loss_vals_ADAM[i]))\n",
    "        \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_ADAM, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADAM, split rollout time into chunks, solve sequentially from end to beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "\n",
    "n_steps, lr, weight_decay = 2000, 0.01, 0.0\n",
    "\n",
    "loss_vals_test = np.zeros(n_steps)\n",
    "time_vals_test = time.time() * np.ones(n_steps)\n",
    "\n",
    "n_chunks = 10\n",
    "T_rollout_i = (T_rollout//n_chunks) * np.ones(n_chunks, dtype=np.int)\n",
    "\n",
    "x_inits = np.zeros((n_chunks, N, K*(J+1)))\n",
    "x_init = out[T_start+T_rollout].copy()\n",
    "\n",
    "targets = np.zeros((n_chunks, N, K*(J+1)))\n",
    "targets[0] = out[T_start+T_rollout]\n",
    "\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_test = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "    x_inits[j] = roller_outer_test.X.detach().cpu().numpy().copy()\n",
    "    optimizer = torch.optim.Adam(roller_outer_test.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    target = torch.as_tensor(targets[j], dtype=dtype, device=device)\n",
    "    roller_outer_test.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = ((roller_outer_test.forward(T=T_rollout_i[j]) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals_test[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_test[i_] = time.time() - time_vals_test[i_]\n",
    "        print((time_vals_test[i_], loss_vals_test[i_]))\n",
    "        \n",
    "        i_ += 1\n",
    "\n",
    "    x_init = roller_outer_test.X.detach().cpu().numpy().copy()\n",
    "    if j < n_chunks - 1:\n",
    "        targets[j+1] = roller_outer_test.X.detach().cpu().numpy().copy()\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_test, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_ADAM, label='in one go')\n",
    "plt.semilogy(loss_vals_test, label='in 10 chunks')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('gradient step')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.as_tensor(targets[0], dtype=dtype, device=device)\n",
    "((roller_outer_test.forward(T=T_rollout) - target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.as_tensor(targets[0], dtype=dtype, device=device)\n",
    "((roller_outer_ADAM.forward(T=T_rollout) - target)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "for i in range(N):\n",
    "    plt.subplot(2,N,i+1)\n",
    "    plt.plot(roller_outer_ADAM.X.detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.X.detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,N,N+i+1)\n",
    "    plt.plot(roller_outer_ADAM.forward(T=T_rollout).detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.forward(T=T_rollout).detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with plain gradient descent (SGD with single data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = out[T_start+T_rollout].copy()\n",
    "roller_outer_SGD = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "x_init = roller_outer_SGD.X.detach().cpu().numpy().copy()\n",
    "\n",
    "n_steps, lr, weight_decay = 2000, 0.01, 0.0\n",
    "roller_outer_SGD.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(roller_outer_SGD.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\"\"\"\n",
    "optimizer = torch.optim.LBFGS(params=roller_outer.parameters(), \n",
    "                              lr=lr, \n",
    "                              max_iter=20, \n",
    "                              max_eval=None, \n",
    "                              tolerance_grad=1e-07, \n",
    "                              tolerance_change=1e-09, \n",
    "                              history_size=100, \n",
    "                              line_search_fn=None)\n",
    "\"\"\"\n",
    "loss_vals_SGD = np.zeros(n_steps)\n",
    "time_vals_SGD = time.time() * np.ones(n_steps)\n",
    "for i in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = ((roller_outer_SGD.forward(T=T_rollout) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals_SGD[i] = loss.detach().cpu().numpy()\n",
    "        time_vals_SGD[i] = time.time() - time_vals_SGD[i]\n",
    "        print((time_vals_SGD[i], loss_vals_SGD[i]))\n",
    "        \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_SGD, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.semilogy(loss_vals_ADAM, '--', label=f'ADAM, lr=0.01')\n",
    "try:\n",
    "    plt.semilogy(loss_vals_SGD, label=f'SGD, lr=0.01')\n",
    "except:\n",
    "    pass\n",
    "#plt.legend()\n",
    "#plt.ylabel('MSE')\n",
    "plt.xlabel('# gradient steps')\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.semilogy(time_vals_ADAM, loss_vals_ADAM, '--', label=f'ADAM, lr=0.01')\n",
    "try:\n",
    "    plt.semilogy(time_vals_SGD, loss_vals_SGD, label=f'SGD, lr=0.01')\n",
    "except:\n",
    "    pass\n",
    "plt.legend()\n",
    "plt.suptitle('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('time [s]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, split rollout time into chunks, solve sequentially from end to beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_chunks = time.time() * np.ones(n_steps)\n",
    "\n",
    "n_chunks = 10\n",
    "T_rollout_i = (T_rollout//n_chunks) * np.ones(n_chunks, dtype=np.int)\n",
    "\n",
    "x_inits = np.zeros((n_chunks, N, K*(J+1)))\n",
    "x_init = out[T_start+T_rollout].copy()\n",
    "\n",
    "targets = np.zeros((n_chunks, N, K*(J+1)))\n",
    "targets[0] = out[T_start+T_rollout]\n",
    "\n",
    "x_sols = np.zeros_like(x_inits)\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "    x_inits[j] = roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()\n",
    "    #optimizer = torch.optim.Adam(roller_outer_LBFGS_chunks.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=20, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    target = torch.as_tensor(targets[j], dtype=dtype, device=device)\n",
    "    roller_outer_LBFGS_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward(T=T_rollout_i[j]) - target)**2).mean()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_chunks.forward(T=T_rollout_i[j]) - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_chunks[i_] = time.time() - time_vals_LBFGS_chunks[i_]\n",
    "        print((time_vals_LBFGS_chunks[i_], loss_vals_LBFGS_chunks[i_]))\n",
    "        i_ += 1\n",
    "\n",
    "    x_init = roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()\n",
    "    x_sols[j] = roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()\n",
    "    if j < n_chunks - 1:\n",
    "        targets[j+1] = roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_chunks = time.time() * np.ones(n_steps)\n",
    "x_init = out[T_start+T_rollout].copy()\n",
    "\n",
    "n_chunks = 10\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=20, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    target = torch.as_tensor(out[T_start+T_rollout], dtype=dtype, device=device)\n",
    "    roller_outer_LBFGS_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_chunks[i_] = time.time() - time_vals_LBFGS_chunks[i_]\n",
    "        print((time_vals_LBFGS_chunks[i_], loss_vals_LBFGS_chunks[i_]))\n",
    "        i_ += 1\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go, initialize from chunked approach with 10-step delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_chunks = time.time() * np.ones(n_steps)\n",
    "\n",
    "n_chunks = 10\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    x_init = x_inits[j]\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=20, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    target = torch.as_tensor(out[T_start+T_rollout], dtype=dtype, device=device)\n",
    "    roller_outer_LBFGS_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_chunks[i_] = time.time() - time_vals_LBFGS_chunks[i_]\n",
    "        print((time_vals_LBFGS_chunks[i_], loss_vals_LBFGS_chunks[i_]))\n",
    "        i_ += 1\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go, initialize from chunked approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "T_start = np.array([5000, 10000, 150000])\n",
    "T_rollout, N = 100, len(T_start)\n",
    "\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_chunks = time.time() * np.ones(n_steps)\n",
    "\n",
    "n_chunks = 10\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    x_init = x_sols[j]\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=20, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    target = torch.as_tensor(out[T_start+T_rollout], dtype=dtype, device=device)\n",
    "    roller_outer_LBFGS_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "        #optimizer.zero_grad()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_chunks[i_] = time.time() - time_vals_LBFGS_chunks[i_]\n",
    "        print((time_vals_LBFGS_chunks[i_], loss_vals_LBFGS_chunks[i_]))\n",
    "        i_ += 1\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs_chunks = np.zeros(n_chunks)\n",
    "MSEs_direct__init_chunks = np.zeros(n_chunks)\n",
    "MSEs_direct__init_prev = np.zeros(n_chunks)\n",
    "\n",
    "target = torch.as_tensor(out[T_start+T_rollout], dtype=dtype, device=device)\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_sols[j])\n",
    "    MSEs_chunks[j] = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean().detach().cpu().numpy()\n",
    "\n",
    "    #roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_sols[j])\n",
    "    #MSEs_chunks[j] = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean().detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "for i in range(N):\n",
    "    plt.subplot(2,N,i+1)\n",
    "    plt.plot(roller_outer_ADAM.X.detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.X.detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.plot(roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()[i], label='in 10 chunks, L-BFGS')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,N,N+i+1)\n",
    "    plt.plot(roller_outer_ADAM.forward(T=T_rollout).detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.forward(T=T_rollout).detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.plot(roller_outer_LBFGS_chunks.forward(T=T_rollout).detach().cpu().numpy().copy()[i], '--', label='in 10 chunks, L-BFGS')\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## more plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_max = 2 # chose N_max << N if N is very large and you don't want hundreds of subplots\n",
    "\n",
    "plt.figure(figsize=(16,2*N_max))\n",
    "for n in range(N_max):\n",
    "    plt.subplot(np.ceil(N_max/2),2,n+1)\n",
    "    plt.plot(x_init[n], 'k', label='init', alpha=0.2)\n",
    "    plt.plot(roller_outer.X.detach().cpu().numpy()[n,:], color='orange', linewidth=1.5, label='target')\n",
    "    plt.plot(out[T_start[n]].T, 'b--', linewidth=0.5, label='est.')\n",
    "    plt.xlabel('state dimension')\n",
    "    plt.ylabel(f'iniital state at T = {T_start[n]}')\n",
    "    plt.legend()\n",
    "plt.suptitle('estimated initial state')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,2*N_max))\n",
    "for n in range(N_max):\n",
    "    plt.subplot(np.ceil(N_max/2),2,n+1)\n",
    "    plt.plot(out[T_start[n]].flatten() - roller_outer.X.detach().cpu().numpy()[n,:], 'orange', \n",
    "             label='est. - true initial state')\n",
    "    plt.plot(out[T_start[n]].flatten() - out[T_start[n]+T_rollout].flatten(), 'k', alpha=0.3, \n",
    "             label='future - initial state')\n",
    "    plt.legend()\n",
    "    plt.xlabel('state dimension')\n",
    "    plt.ylabel(f'initial state error at T = {T_start[n]}')\n",
    "plt.suptitle('error of estimated initial state')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(16,2*N_max))\n",
    "roller_outer2 = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=out[T_start])\n",
    "for n in range(N_max):\n",
    "    plt.subplot(np.ceil(N_max/2),2,n+1)\n",
    "    plt.plot(out[T_start[n]+T_rollout].flatten() - roller_outer.forward(T=T_rollout).detach().cpu().numpy()[n], \n",
    "             'orange', label='true final state - rollout from est. init. state')\n",
    "    plt.plot(out[T_start[n]+T_rollout].flatten() - roller_outer2.forward(T=T_rollout).detach().cpu().numpy()[n], \n",
    "             'k', alpha=0.3, label='true final state - rollout from true init. state')\n",
    "    plt.legend()\n",
    "    plt.xlabel('state dimension')\n",
    "    plt.ylabel(f'final state error at T = {T_start[n]}+{T_rollout}')\n",
    "plt.suptitle('error of estimated final state (under the learned model)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
