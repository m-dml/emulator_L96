{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Solve a fully-observed inverse problem\n",
    "\n",
    "Given $x_T$, estimate $x_0$ by matching $G^{(T)}(x_0)$ to $x_T$. Use autodiff on $G$ to calculate gradients of an error metric w.r.t. $x_0$. Compare the resulting rollout to the original `true' simulation.\n",
    "\n",
    "Compare three approaches:\n",
    "- $argmin_{x_0} || x_T - G^{(T)}(x_i) ||$, i.e. $T$ steps in one go\n",
    "- $argmin_{x_i} || x_{i+T_i} - G^{(T_i)}(x_i) ||$, i.e. $T_i$ steps at a time, with $\\sum_i T_i = T$. In the extreme case of $T_i=1$, this becomes very similar to implicit numerical methods. Can invertible neural networks help beyond providing better initializations for $x_i$ ? \n",
    "- solving backwards: more of the extreme case of $\\forall i: T_i=1$, however: Only for some forward numerical solvers can we just reverse time [1] and expect to return to initial conditions. Leap-frog works, but e.g. forward-Euler time-reversed is backward-Euler. \n",
    "\n",
    "Generally, how do these approaches differ around \\& beyond the horizon of predictability? Which solutions do they pick, and how easy is it to get uncertainties from them?\n",
    "\n",
    "[1] https://scicomp.stackexchange.com/questions/32736/forward-and-backward-integration-cause-of-errors?noredirect=1&lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# static case (setup_DA, run_DA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run_DA import setup_DA\n",
    "exp_id = '05'\n",
    "exp_names = os.listdir('experiments_DA/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "args = setup_DA(conf_exp=f'experiments_DA/{conf_exp}.yml')\n",
    "args.pop('conf_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "save_dir = 'results/data_assimilation/' + args['exp_id'] + '/'\n",
    "fn = save_dir + 'res.npy'\n",
    "\n",
    "res = np.load(res_dir + fn, allow_pickle=True)[()]\n",
    "\n",
    "targets=res['targets']\n",
    "initial_states=res['initial_states']\n",
    "\n",
    "J = res['J']\n",
    "n_steps = res['n_steps']\n",
    "n_chunks = res['n_chunks']\n",
    "n_chunks_recursive = res['n_chunks_recursive']\n",
    "T_rollout = res['T_rollout']\n",
    "dt = res['dt']\n",
    "back_solve_dt_fac=res['back_solve_dt_fac']\n",
    "n_starts = res['n_starts']\n",
    "\n",
    "optimiziation_schemes = res['optimiziation_schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "from L96_emulator.data_assimilation import get_init\n",
    "from L96_emulator.data_assimilation import get_model\n",
    "from L96_emulator.likelihood import GenModel, ObsOp_identity, ObsOp_subsampleGaussian\n",
    "import torch \n",
    "\n",
    "# get model\n",
    "model_pars = {\n",
    "    'exp_id' : args['model_exp_id'],\n",
    "    'model_forwarder' : res['model_forwarder'],\n",
    "    'K_net' : res['K'],\n",
    "    'J_net' : res['J'],\n",
    "    'dt_net' : res['dt']\n",
    "}\n",
    "model, model_forwarder, _ = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "\n",
    "# ### instantiate observation operator\n",
    "if res['obs_operator']=='ObsOp_subsampleGaussian':\n",
    "    obs_operator = ObsOp_subsampleGaussian\n",
    "elif res['obs_operator']=='ObsOp_identity':\n",
    "    obs_operator = ObsOp_identity\n",
    "\n",
    "model_observer = obs_operator(**res['obs_operator_args'])\n",
    "\n",
    "# ### define prior over initial states\n",
    "prior = torch.distributions.normal.Normal(loc=torch.zeros((1,res['J']+1,res['K'])), \n",
    "                                          scale=1.*torch.ones((1,res['J']+1,res['K'])))\n",
    "\n",
    "# ### define generative model for observed data\n",
    "gen = GenModel(model_forwarder, model_observer, prior, T=T_rollout, x_init=None)\n",
    "\n",
    "#\"\"\"\n",
    "appr_sel = {\n",
    "    'LBFGS_chunks' : False,\n",
    "    'LBFGS_full_chunks' : False,\n",
    "    'backsolve' : True, \n",
    "    'LBFGS_full_backsolve' : True,\n",
    "    'LBFGS_full_persistence' : False, \n",
    "    'LBFGS_recurse_chunks' : False\n",
    "}\n",
    "#\"\"\"\n",
    "appr_sel = optimiziation_schemes\n",
    "\n",
    "appr_names = {\n",
    "    #'LBFGS_chunks' : 'optim over single chunk (current chunk error)',\n",
    "    'LBFGS_chunks' : r'implicit backward solver t=1$\\rightarrow$t=0',\n",
    "    'LBFGS_full_backsolve' : 'optim. initialized from forward solve in reverse', \n",
    "    'LBFGS_full_chunks' : 'full optim, init from chunks',\n",
    "    'LBFGS_full_persistence' : 'optim. initialized from persistence',\n",
    "    'LBFGS_recurse_chunks' : r'recursive optimization through rollout t-k$\\rightarrow$t',\n",
    "    'backsolve' : r'explicit forward solve in reverse t=1$\\rightarrow$t=0'\n",
    "}\n",
    "\n",
    "plt_styles = {\n",
    "    'LBFGS_chunks' : 'r--',\n",
    "    'LBFGS_full_backsolve' : 'b-', \n",
    "    'LBFGS_full_chunks' : 'm-',\n",
    "    'LBFGS_full_persistence' : 'g-',\n",
    "    'LBFGS_recurse_chunks' : 'k-',\n",
    "    'backsolve' : 'c--'        \n",
    "}\n",
    "\n",
    "\n",
    "# compute state differences\n",
    "\n",
    "state_diff, pred_mses = {},{}\n",
    "if 'LBFGS_chunks' in appr_sel.keys() and appr_sel['LBFGS_chunks']: \n",
    "    state_diff['LBFGS_chunks'] = [((sortL96fromChannels(res['targets'][0]) - res['x_sols_LBFGS_chunks'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "    state_diff['LBFGS_chunks'] += [(np.diff(res['x_sols_LBFGS_chunks'],axis=0)**2).mean(axis=2)]\n",
    "    state_diff['LBFGS_chunks'] = np.concatenate(state_diff['LBFGS_chunks'])\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_LBFGS_chunks'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['LBFGS_chunks'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "if 'LBFGS_full_chunks' in appr_sel.keys() and appr_sel['LBFGS_full_chunks']: \n",
    "    x_init = res['x_sols_LBFGS_chunks'][res['recursions_per_chunks']-1::res['recursions_per_chunks']]\n",
    "    state_diff['LBFGS_full_chunks'] = ((x_init-res['x_sols_LBFGS_full_chunks'])**2).mean(axis=-1)\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_LBFGS_full_chunks'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['LBFGS_full_chunks'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "if 'backsolve' in appr_sel.keys() and appr_sel['backsolve']: \n",
    "    state_diff['backsolve'] = [((sortL96fromChannels(res['targets'][0]) - res['x_sols_backsolve'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "    state_diff['backsolve'] += [(np.diff(res['x_sols_backsolve'],axis=0)**2).mean(axis=2)]\n",
    "    state_diff['backsolve'] = np.concatenate(state_diff['backsolve'])\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_backsolve'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['backsolve'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "if 'LBFGS_full_backsolve' in appr_sel.keys() and appr_sel['LBFGS_full_backsolve']: \n",
    "    x_init = res['x_sols_backsolve'][res['recursions_per_chunks']-1::res['recursions_per_chunks']]\n",
    "    state_diff['LBFGS_full_backsolve'] = ((x_init-res['x_sols_LBFGS_full_backsolve'])**2).mean(axis=-1)\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_LBFGS_full_backsolve'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['LBFGS_full_backsolve'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "if 'LBFGS_full_persistence' in appr_sel.keys() and appr_sel['LBFGS_full_persistence']: \n",
    "    x_init = sortL96fromChannels(res['targets'])\n",
    "    state_diff['LBFGS_full_persistence'] = ((x_init-res['x_sols_LBFGS_full_persistence'])**2).mean(axis=-1)\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_LBFGS_full_persistence'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['LBFGS_full_persistence'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "if 'LBFGS_recurse_chunks' in appr_sel.keys() and appr_sel['LBFGS_recurse_chunks']: \n",
    "    state_diff['LBFGS_recurse_chunks'] = [((sortL96fromChannels(res['targets'][0]) - res['x_sols_LBFGS_recurse_chunks'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "    state_diff['LBFGS_recurse_chunks'] += [(np.diff(res['x_sols_LBFGS_recurse_chunks'],axis=0)**2).mean(axis=2)]\n",
    "    state_diff['LBFGS_recurse_chunks'] = np.concatenate(state_diff['LBFGS_recurse_chunks'])\n",
    "\n",
    "    x_sols = sortL96intoChannels(res['x_sols_LBFGS_recurse_chunks'], J=res['J'])\n",
    "    x_pred = np.zeros_like(x_sols)\n",
    "    for n in range(x_sols.shape[1]):\n",
    "        gen.set_state(x_sols[:,n])\n",
    "        x_pred[:,n] = gen.forward(T_obs=[res['T_pred']-1])[0].detach().cpu().numpy()\n",
    "\n",
    "    pred_mses['LBFGS_recurse_chunks'] = ((res['test_state'] - x_pred)**2).mean(axis=(-2,-1))\n",
    "\n",
    "state_MSE_peristence = ((res['initial_states'] - res['targets'])**2).mean(axis=(-2,-1))\n",
    "\n",
    "x_init = get_init(sortL96intoChannels(res['targets_obs'][0],J=J), res['loss_mask'][0], method='interpolate')\n",
    "state_MSE_peristence_obs = ((x_init - res['initial_states'])**2).mean(axis=(-2,-1))\n",
    "\n",
    "all_appr_names, all_losses, all_times, all_mses, all_stdfs, all_plt_styles = [], [], [], [], [], []\n",
    "all_pred_mses = []\n",
    "for scheme_str in list(appr_sel):\n",
    "    if optimiziation_schemes[scheme_str] and appr_sel[scheme_str]:\n",
    "\n",
    "        all_appr_names.append(appr_names[scheme_str])\n",
    "        all_losses.append(res['loss_vals_'+scheme_str])\n",
    "        all_times.append(res['time_vals_'+scheme_str])\n",
    "        all_mses.append(res['state_mses_'+scheme_str])\n",
    "        all_stdfs.append(state_diff[scheme_str])\n",
    "        all_plt_styles.append(plt_styles[scheme_str])\n",
    "        all_pred_mses.append(pred_mses[scheme_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg = True\n",
    "if plot_avg:\n",
    "    all_losses = [np.nanmean(l, axis=1).reshape(-1,1) for l in all_losses]\n",
    "    all_times = [np.nanmean(l, axis=1).reshape(-1,1) for l in all_times]\n",
    "    all_mses = [np.nanmean(l, axis=1).reshape(-1,1) for l in all_mses]\n",
    "\n",
    "plt.figure(figsize=(16,20))\n",
    "plt.subplot(4,1,1)\n",
    "for i,loss in enumerate(all_losses):\n",
    "    neg_log_loss_offset = np.nanmin(loss) # (res['K']*(res['J']+1) * np.log(2.*np.pi))/2.\n",
    "    xx = np.linspace(0, T_rollout, loss.shape[0])\n",
    "    plt.semilogy(xx, loss[:,0] - neg_log_loss_offset, all_plt_styles[i], label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,loss in enumerate(all_losses):\n",
    "        neg_log_loss_offset = np.nanmin(loss) # (res['K']*(res['J']+1) * np.log(2.*np.pi))/2.\n",
    "        xx = np.linspace(0, T_rollout, loss.shape[0])\n",
    "        plt.semilogy(xx, loss[:,1:] - neg_log_loss_offset, all_plt_styles[i])        \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('rollout step')\n",
    "plt.ylabel('neg. log-likelihood (up to constant)')\n",
    "plt.title('loss during optimization for different initialization methods')\n",
    "\n",
    "\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "for i,mse in enumerate(all_mses):\n",
    "    xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, mse[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,mse in enumerate(all_mses):\n",
    "        xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, mse[:,1:], all_plt_styles[i], marker='.')        \n",
    "\n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, state_MSE_peristence[:,0], ':', color='orange', marker='x', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, state_MSE_peristence[:,1:], ':', color='orange', marker='x')        \n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('initial state MSE')\n",
    "plt.title('initial state error for different initialization methods')\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "for i,stdf in enumerate(all_stdfs):\n",
    "    xx = np.linspace(0, T_rollout, stdf.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, stdf[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,stdf in enumerate(all_stdfs):\n",
    "        xx = np.linspace(0, T_rollout, stdf.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, stdf[:,1:], all_plt_styles[i], marker='.')        \n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, state_MSE_peristence[:,0], ':', color='orange', marker='x', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, state_MSE_peristence[:,1:], ':', color='orange', marker='x')        \n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('mean-squared distance to initialization')\n",
    "plt.title('difference to initial state estimate initialization')\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "for i, tms in enumerate(all_times):\n",
    "    tms[np.where(tms > 1e6)[0]] = np.nan\n",
    "    xx = np.linspace(1, T_rollout, tms.shape[0])\n",
    "    plt.semilogy(xx, tms[:,0], all_plt_styles[i], label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i, tms in enumerate(all_times):\n",
    "        tms[np.where(tms > 1e6)[0]] = np.nan\n",
    "        xx = np.linspace(1, T_rollout, tms.shape[0])\n",
    "        plt.semilogy(xx, tms[:,1:], all_plt_styles[i])        \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('computation time [s]')\n",
    "plt.title('full computation time for different initialization methods')\n",
    "\n",
    "plt.suptitle('exp_id : ' + conf_exp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['state_mses_LBFGS_full_backsolve'].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backsolve figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(6,4))\n",
    "for i,mse in enumerate(all_mses):\n",
    "    mse = np.flipud(mse)\n",
    "    xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, mse[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,mse in enumerate(all_mses):\n",
    "        mse = np.flipud(mse)\n",
    "        xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, mse[:,1:], all_plt_styles[i], marker='.')        \n",
    "\n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, np.flipud(state_MSE_peristence[:,:1]), ':', color='orange', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, np.flipud(state_MSE_peristence[:,1:]), ':', color='orange')        \n",
    "plt.legend()\n",
    "plt.xlabel('time step T-k')\n",
    "plt.ylabel('state MSE')\n",
    "#plt.title('initial state error for different initialization methods')\n",
    "plt.xticks([1, 10, 20, 30, 40])\n",
    "#plt.savefig()\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# static 4D-Var figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "for i,mse in enumerate(all_mses):\n",
    "    xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, mse[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,mse in enumerate(all_mses):\n",
    "        xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, mse[:,1:], all_plt_styles[i], marker='.')        \n",
    "\n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, state_MSE_peristence[:,0], ':', color='orange', marker='x', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, state_MSE_peristence[:,1:], ':', color='orange', marker='x')        \n",
    "#plt.legend()\n",
    "plt.xlabel('time step t')\n",
    "plt.ylabel('initial state MSE')\n",
    "plt.xticks([1, 10, 20, 30, 40])\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i,mse in enumerate(all_pred_mses):\n",
    "    xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, mse[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,mse in enumerate(all_pred_mses):\n",
    "        xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, mse[:,1:], all_plt_styles[i], marker='.')        \n",
    "\n",
    "xx = [1, T_rollout]\n",
    "state_MSE_peristence_pred = ((res['initial_states'][:1] - res['test_state'])**2).mean(axis=(-2,-1))\n",
    "plt.semilogy(xx, np.ones((2,1))*state_MSE_peristence_pred[:,0], ':', color='orange', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, np.ones((2,1))*state_MSE_peristence_pred[:,1:], ':', color='orange')        \n",
    "plt.legend()\n",
    "plt.xlabel('time step t')\n",
    "T_pred = res['T_pred']\n",
    "plt.ylabel(f'state prediction error MSE T={T_pred}')\n",
    "plt.xticks([1, 10, 20, 30, 40])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['state_mses_LBFGS_full_backsolve'][9,:].T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check log-likelihoods and likelihood surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.data_assimilation import ObsOp_identity, ObsOp_subsampleGaussian, GenModel, get_model, as_tensor\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch\n",
    "\n",
    "K,J = res['K'], res['J']\n",
    "\n",
    "model_pars = {\n",
    "    'exp_id' : args['model_exp_id'],\n",
    "    'model_forwarder' : 'rk4_default',\n",
    "    'K_net' : res['K'],\n",
    "    'J_net' : res['J'],\n",
    "    'dt_net' : res['dt_net']\n",
    "}\n",
    "\n",
    "model, model_forwarder, args = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "\n",
    "ObsOp = ObsOp_subsampleGaussian if res['obs_operator']=='ObsOp_subsampleGaussian' else ObsOp_identity\n",
    "\n",
    "# ### instantiate observation operator\n",
    "model_observer = ObsOp(**res['obs_operator_args'])\n",
    "\n",
    "prior = torch.distributions.normal.Normal(loc=torch.zeros((1,J+1,K)), \n",
    "                                          scale=1.*torch.ones((1,J+1,K)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'x_sols_LBFGS_full_persistence' in res.keys():\n",
    "    x_sols = res['x_sols_LBFGS_full_persistence'] \n",
    "else:\n",
    "    x_sols = res['x_sols_LBFGS_full_backsolve'] \n",
    "    print('persistence-initialized not found, using backsolve-initialized results')\n",
    "    \n",
    "n = 4\n",
    "for j_chunks in range(x_sols.shape[0]):\n",
    "\n",
    "    T_test = (j_chunks+1) * res['recursions_per_chunks']\n",
    "    j = T_test-1\n",
    "\n",
    "    T_obs = [(j+1)*(T_rollout//n_chunks)-1 for j in range(j_chunks+1)]\n",
    "\n",
    "    gen = GenModel(model_forwarder, model_observer, prior, T=T_test, x_init = None)\n",
    "\n",
    "    nx = 100\n",
    "    xx = np.arange(-0.1,1.1,1./nx)\n",
    "\n",
    "    x = sortL96intoChannels(x_sols[j_chunks][n:n+1],J=J)\n",
    "    xs = [res['initial_states'][j][n:n+1] + a * ( x- res['initial_states'][j][n:n+1]) for a in xx]\n",
    "    lls = [gen.log_prob(\n",
    "                 y=as_tensor(sortL96intoChannels(res['targets_obs'][:len(T_obs),n:n+1],J=J)), \n",
    "                 x=as_tensor(x), \n",
    "                 m=as_tensor(res['loss_mask'][:len(T_obs),n:n+1]),\n",
    "                 T_obs=T_obs).detach().cpu().numpy() for x in xs]\n",
    "\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(0., lls[np.argmin((xx-0.)**2)], 'gx', markersize=10., markeredgewidth=3, label='true initial state')\n",
    "    plt.plot(1., lls[np.argmin((xx-1.)**2)], 'mo', markersize=10., markeredgewidth=3, label='est. initial state')\n",
    "    plt.legend()\n",
    "    plt.ylabel('log-likelihood')\n",
    "    plt.xlabel('position on line from true to est. init state')\n",
    "    plt.plot(xx, np.array(lls))\n",
    "    plt.title(f'loss-surface for T_rollout={T_test} and estimated initial states (LBFGS, full optim, init with persistence)')\n",
    "\n",
    "    m = np.where(sortL96fromChannels(res['loss_mask'][len(T_obs)-1, n:n+1]))[1]\n",
    "\n",
    "    plt.subplot(3,1,2)\n",
    "    x = gen.forward(as_tensor(sortL96intoChannels(x_sols[j_chunks][n:n+1],J=J)),\n",
    "                    T_obs=T_obs)[-1]\n",
    "    plt.plot(np.arange(K*(J+1))+1, sortL96fromChannels(x).flatten(), 'm', label='from est. initial state')\n",
    "    plt.plot(m+1, sortL96fromChannels(x).T[m], 'o', color='m', label='observed value')\n",
    "\n",
    "    x = gen.forward(as_tensor(res['initial_states'][j][n:n+1]), \n",
    "                    T_obs=T_obs)[-1]\n",
    "    plt.plot(np.arange(K*(J+1))+1, sortL96fromChannels(x).flatten(), 'g', label='from true initial state')\n",
    "    plt.plot(m+1, sortL96fromChannels(x).T[m], 'x', color='g', label='observed value')\n",
    "\n",
    "    plt.plot(m+1, sortL96fromChannels(res['targets_obs'][len(T_obs)-1, n:n+1]).flatten()[m], 'kx', \n",
    "             label='observed state')\n",
    "\n",
    "    plt.xlabel('position k')\n",
    "    plt.ylabel('value X_k')\n",
    "    plt.legend()\n",
    "    plt.title('rollout from retrieved initial state')\n",
    "\n",
    "\n",
    "    plt.subplot(3,1,1)\n",
    "    x = x_sols[j_chunks][n:n+1].flatten()\n",
    "    plt.plot(np.arange(K*(J+1))+1,x, 'm', label='est. initial state')\n",
    "    plt.plot(m+1, x[m], 'o', color='m', label='observed value')\n",
    "\n",
    "    x = sortL96fromChannels(res['initial_states'][j][n:n+1]).flatten()\n",
    "    plt.plot(np.arange(K*(J+1))+1, x, 'g', label='true initial state')\n",
    "    plt.plot(m+1, x[m], 'x', color='g', label='observed value')\n",
    "\n",
    "    plt.xlabel('position k')\n",
    "    plt.ylabel('value X_k')\n",
    "    plt.legend()\n",
    "    plt.title('initial state - true and estimate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check recurisvely computed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_sols = res['x_sols_LBFGS_chunks'] \n",
    "\n",
    "n = 1\n",
    "for j_chunks in range(x_sols.shape[0]):\n",
    "\n",
    "    T_test = (j_chunks+1)\n",
    "    j = T_test-1\n",
    "\n",
    "    T_obs = [j_chunks]\n",
    "\n",
    "    print('j_chunks, T_obs', j_chunks, T_obs)\n",
    "    \n",
    "    gen = GenModel(model_forwarder, model_observer, prior, T=T_test, x_init = None)\n",
    "\n",
    "    nx = 100\n",
    "    xx = np.arange(-0.1,1.1,1./nx)\n",
    "\n",
    "    x = sortL96intoChannels(x_sols[j_chunks][n:n+1],J=J)\n",
    "    xs = [res['initial_states'][j][n:n+1] + a * ( x- res['initial_states'][j][n:n+1]) for a in xx]\n",
    "    lls = [gen.log_prob(\n",
    "                 y=as_tensor(sortL96intoChannels(res['targets_obs'][:len(T_obs),n:n+1],J=J)), \n",
    "                 x=as_tensor(x), \n",
    "                 m=as_tensor(res['loss_mask'][:len(T_obs),n:n+1]),\n",
    "                 T_obs=T_obs).detach().cpu().numpy() for x in xs]\n",
    "\n",
    "    plt.figure(figsize=(12, 16))\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.plot(0., lls[np.argmin((xx-0.)**2)], 'gx', markersize=10., markeredgewidth=3, label='true initial state')\n",
    "    plt.plot(1., lls[np.argmin((xx-1.)**2)], 'mo', markersize=10., markeredgewidth=3, label='est. initial state')\n",
    "    plt.legend()\n",
    "    plt.ylabel('log-likelihood')\n",
    "    plt.xlabel('position on line from true to est. init state')\n",
    "    plt.plot(xx, np.array(lls))\n",
    "    plt.title(f'loss-surface for T_rollout={T_test} and estimated initial states (LBFGS, full optim, init with persistence)')\n",
    "\n",
    "    m = np.where(sortL96fromChannels(res['loss_mask'][len(T_obs)-1, n:n+1]))[1]\n",
    "\n",
    "    plt.subplot(3,1,2)\n",
    "    x = gen.forward(as_tensor(sortL96intoChannels(x_sols[j_chunks][n:n+1],J=J)),\n",
    "                    T_obs=T_obs)[-1]\n",
    "    plt.plot(np.arange(K*(J+1))+1, sortL96fromChannels(x).flatten(), 'm', label='from est. initial state')\n",
    "    plt.plot(m+1, sortL96fromChannels(x).T[m], 'o', color='m', label='observed value')\n",
    "\n",
    "    x = gen.forward(as_tensor(res['initial_states'][j][n:n+1]), \n",
    "                    T_obs=[n_chunks_recursive-1])[-1]\n",
    "    plt.plot(np.arange(K*(J+1))+1, sortL96fromChannels(x).flatten(), 'g', label='from true initial state')\n",
    "    plt.plot(m+1, sortL96fromChannels(x).T[m], 'x', color='g', label='observed value')\n",
    "\n",
    "    plt.plot(m+1, sortL96fromChannels(res['targets_obs'][len(T_obs)-1, n:n+1]).flatten()[m], 'kx', \n",
    "             label='observed state')\n",
    "\n",
    "    plt.xlabel('position k')\n",
    "    plt.ylabel('value X_k')\n",
    "    plt.legend()\n",
    "    plt.title('rollout from retrieved initial state')\n",
    "\n",
    "\n",
    "    plt.subplot(3,1,1)\n",
    "    x = x_sols[j_chunks][n:n+1].flatten()\n",
    "    plt.plot(np.arange(K*(J+1))+1,x, 'm', label='est. initial state')\n",
    "    plt.plot(m+1, x[m], 'o', color='m', label='observed value')\n",
    "\n",
    "    x = sortL96fromChannels(res['initial_states'][j][n:n+1]).flatten()\n",
    "    plt.plot(np.arange(K*(J+1))+1, x, 'g', label='true initial state')\n",
    "    plt.plot(m+1, x[m], 'x', color='g', label='observed value')\n",
    "\n",
    "    plt.xlabel('position k')\n",
    "    plt.ylabel('value X_k')\n",
    "    plt.legend()\n",
    "    plt.title('initial state - true and estimate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D-Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "from L96_emulator.run_DA import setup_4DVar\n",
    "\n",
    "#exp_id = '10'\n",
    "#exp_ids = ['14', '15', '16', '17', '18', '19', '20', '21']\n",
    "#exp_ids = ['22', '23', '24', '25', '26', '27', '28', '29']\n",
    "exp_ids = ['30', '31', '32', '33',  '35', '36', '37']\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "clrs, lgnd = ['w', 'b', 'c', 'g', 'y', 'r', 'm', 'k'], []\n",
    "plt.subplot(1,3,1)\n",
    "for clr in clrs:\n",
    "    plt.plot(-100, -1, 'o-', color=clr, linewidth=2.5)\n",
    "    \n",
    "rmses_total = np.zeros(len(exp_ids))\n",
    "win_lens = np.zeros(len(exp_ids))\n",
    "\n",
    "for eid, exp_id in enumerate(exp_ids):\n",
    "\n",
    "    exp_names = os.listdir('experiments_DA/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "    args = setup_4DVar(conf_exp=f'experiments_DA/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "\n",
    "    save_dir = 'results/data_assimilation/' + args['exp_id'] + '/'\n",
    "    fn = save_dir + 'out.npy'\n",
    "\n",
    "    out = np.load(res_dir + fn, allow_pickle=True)[()]\n",
    "\n",
    "    J = args['J']\n",
    "    n_steps = args['n_steps']\n",
    "    T_win = args['T_win'] \n",
    "    T_shift = args['T_shift'] if args['T_shift'] >= 0 else T_win\n",
    "    dt = args['dt']\n",
    "\n",
    "    data = out['out']\n",
    "    y, m = out['y'], out['m']\n",
    "    x_sols = out['x_sols']\n",
    "    losses, times = out['losses'], out['times']\n",
    "\n",
    "    assert T_win == out['T_win']\n",
    "\n",
    "    mses = np.zeros(((data.shape[0] - T_win) // T_shift + 1, data.shape[1]))\n",
    "    for i in range(len(mses)):\n",
    "        mse = np.nanmean((x_sols[i:i+1] - data)**2, axis=(-2, -1))\n",
    "        mses[i] = mse[i *T_shift]\n",
    "\n",
    "    xx = np.arange(0, data.shape[0] - T_win, T_shift)\n",
    "    \n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(xx, mses, 'o-', color=clrs[eid], linewidth=2.5)\n",
    "    plt.xlim(0, len(data))\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.plot(xx, np.nanmean(mses, axis=1), 'o-', color=clrs[eid], linewidth=2.5)\n",
    "    plt.xlim(0, len(data))\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.plot(xx, mses, 'o-', color=clrs[eid], linewidth=2.5)\n",
    "    plt.axis([0, len(data)-1, 0, 2])\n",
    "    print(np.nanmean(mses[1:]))\n",
    "    lgnd.append('window length='+str(T_win))\n",
    "    \n",
    "    rmses_total[eid] = np.sqrt(np.nanmean(mses))\n",
    "    win_lens[eid] = T_win\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('individial trials')\n",
    "plt.ylabel('initial state MSE')\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('averages over trials')\n",
    "plt.legend(lgnd[:3])\n",
    "plt.xlabel('time t')\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('inidividual trials, zoom-in on small MSEs')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(win_lens*1.5/24, rmses_total, '-', color='k', linewidth=2.5)\n",
    "plt.xlabel('integration window length [d]')\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.likelihood import ObsOp_identity, ObsOp_subsampleGaussian, ObsOp_rotsampleGaussian\n",
    "from L96_emulator.data_assimilation import GenModel, get_model, as_tensor\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch\n",
    "\n",
    "K,J = args['K'], args['J']\n",
    "\n",
    "model_pars = {\n",
    "    'exp_id' : args['model_exp_id'],\n",
    "    'model_forwarder' : 'rk4_default',\n",
    "    'K_net' : args['K'],\n",
    "    'J_net' : args['J'],\n",
    "    'dt_net' : args['dt']\n",
    "}\n",
    "\n",
    "model, model_forwarder, _ = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "\n",
    "obs_operator = args['obs_operator']\n",
    "obs_pars = {}\n",
    "if obs_operator=='ObsOp_subsampleGaussian':\n",
    "    obs_pars['obs_operator'] = ObsOp_subsampleGaussian\n",
    "    obs_pars['obs_operator_args'] = {'r' : args['obs_operator_r'], 'sigma2' : args['obs_operator_sig2']}\n",
    "elif obs_operator=='ObsOp_identity':\n",
    "    obs_pars['obs_operator'] = ObsOp_identity\n",
    "    obs_pars['obs_operator_args'] = {}\n",
    "elif obs_operator=='ObsOp_rotsampleGaussian':\n",
    "    obs_pars['obs_operator'] = ObsOp_rotsampleGaussian\n",
    "    obs_pars['obs_operator_args'] = {'frq' : args['obs_operator_frq'], \n",
    "                                     'sigma2' : args['obs_operator_sig2']}\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "model_observer = obs_pars['obs_operator'](**obs_pars['obs_operator_args'])\n",
    "\n",
    "prior = torch.distributions.normal.Normal(loc=torch.zeros((1,J+1,K)), \n",
    "                                          scale=1.*torch.ones((1,J+1,K)))\n",
    "\n",
    "# ### define generative model for observed data\n",
    "gen = GenModel(model_forwarder, model_observer, prior, T=T_win, x_init=None)\n",
    "\n",
    "forecast_win = int(120/1.5) # 5d forecast\n",
    "eval_every = int(6/1.5) # every 6h\n",
    "exp_id = '21'\n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "args = setup_4DVar(conf_exp=f'experiments_DA/{conf_exp}.yml')\n",
    "args.pop('conf_exp')\n",
    "\n",
    "#assert args['T_win'] == 64 # we want 4d integration window here\n",
    "\n",
    "save_dir = 'results/data_assimilation/' + args['exp_id'] + '/'\n",
    "fn = save_dir + 'out.npy'\n",
    "\n",
    "out = np.load(res_dir + fn, allow_pickle=True)[()]\n",
    "\n",
    "J = args['J']\n",
    "n_steps = args['n_steps']\n",
    "T_win = args['T_win'] \n",
    "T_shift = args['T_shift'] if args['T_shift'] >= 0 else T_win\n",
    "dt = args['dt']\n",
    "\n",
    "data = out['out']\n",
    "y, m = out['y'], out['m']\n",
    "x_sols = out['x_sols']\n",
    "losses, times = out['losses'], out['times']\n",
    "\n",
    "assert T_win == out['T_win']\n",
    "\n",
    "mses = np.zeros(((data.shape[0] - forecast_win - T_win) // T_shift + 1, forecast_win//eval_every+1, y.shape[1]))\n",
    "for i in range(len(mses)):\n",
    "    forecasts = gen._forward(x=as_tensor(x_sols[i]), T_obs=T_win + np.arange(0,forecast_win+1,eval_every))\n",
    "    n = i * T_shift + T_win\n",
    "    for j in range(mses.shape[1]): # loop over integration windows\n",
    "        forecast = forecasts[j].detach().cpu().numpy()\n",
    "        y_obs = data[n+j*eval_every] # sortL96intoChannels(y[n+j*eval_every],J=J)\n",
    "        mses[i,j] = np.nanmean((forecast - y_obs)**2, axis=(-2, -1))\n",
    "xx = 1.5/24 * np.arange(0, forecast_win+1, eval_every)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(xx,np.nanmean(np.sqrt(mses),axis=(0,-1)), '-', color='k', linewidth=2.5)\n",
    "plt.xlabel('forecast time [d]')\n",
    "plt.ylabel('RMSE')\n",
    "plt.yticks([0.5, 1.0, 1.5])\n",
    "plt.xticks(0.5*np.arange(10.1))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.data_assimilation import ObsOp_identity, ObsOp_subsampleGaussian, GenModel, get_model, as_tensor\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch\n",
    "\n",
    "K,J = args['K'], args['J']\n",
    "\n",
    "model_pars = {\n",
    "    'exp_id' : args['model_exp_id'],\n",
    "    'model_forwarder' : 'rk4_default',\n",
    "    'K_net' : args['K'],\n",
    "    'J_net' : args['J'],\n",
    "    'dt_net' : args['dt']\n",
    "}\n",
    "\n",
    "model, model_forwarder, _ = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "\n",
    "ObsOp = ObsOp_subsampleGaussian if args['obs_operator']=='ObsOp_subsampleGaussian' else ObsOp_identity\n",
    "\n",
    "# ### instantiate observation operator\n",
    "model_observer = ObsOp(**{'r' : args['obs_operator_r'], 'sigma2' : args['obs_operator_sig2']})\n",
    "\n",
    "prior = torch.distributions.normal.Normal(loc=torch.zeros((1,J+1,K)), \n",
    "                                          scale=1.*torch.ones((1,J+1,K)))\n",
    "\n",
    "# ### define generative model for observed data\n",
    "gen = GenModel(model_forwarder, model_observer, prior, T=T_win, x_init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i,j = 16, 9\n",
    "T = 40\n",
    "\n",
    "\n",
    "idx_off = (i)*T_win-1 if i > 0 else 0\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x_init = x_sols[i,j] #\n",
    "traj = sortL96fromChannels(torch.cat(gen._forward(as_tensor(x_init), T_obs=np.arange(T))).detach().cpu().numpy())\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(traj, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('rollout time step t')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(traj)\n",
    "plt.ylabel('state value x_k')\n",
    "plt.xlabel('rollout time step t')\n",
    "plt.suptitle('emulator rollout from estimated initial state')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "rollout_mses = ((traj-sortL96fromChannels(data[np.arange(T)+idx_off, j]))**2).mean(axis=1)\n",
    "\n",
    "rollout_mses_masked = (m[np.arange(T)+idx_off, j, 0]*(traj-y[np.arange(T)+idx_off, j])**2)\n",
    "rollout_mses_masked = rollout_mses_masked.sum(axis=(-1)) / m[np.arange(T)+idx_off, j].sum(axis=(1,2))\n",
    "\n",
    "traj = sortL96fromChannels(data[np.arange(T)+idx_off, j])\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(traj, aspect='auto')\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('rollout time step t')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(traj)\n",
    "plt.ylabel('state value x_k')\n",
    "plt.xlabel('rollout time step t')\n",
    "plt.suptitle('simulator rollout from true initial state')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x_init = data[idx_off, j] #\n",
    "traj = sortL96fromChannels(torch.cat(gen._forward(as_tensor(x_init), T_obs=np.arange(T))).detach().cpu().numpy())\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(traj, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('rollout time step t')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(traj)\n",
    "plt.ylabel('state value x_k')\n",
    "plt.xlabel('rollout time step t')\n",
    "plt.suptitle('emulator rollout from estimated initial state')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "x_init = x_sols[i-1,j] #\n",
    "x_init = sortL96fromChannels(torch.cat(gen._forward(as_tensor(x_init), T_obs=[T-1])).detach().cpu().numpy())\n",
    "traj = sortL96fromChannels(torch.cat(gen._forward(as_tensor(x_init), T_obs=np.arange(T))).detach().cpu().numpy())\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(traj, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('rollout time step t')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(traj)\n",
    "plt.ylabel('state value x_k')\n",
    "plt.xlabel('rollout time step t')\n",
    "plt.suptitle('emulator rollout from previous estimated initial state (aka background)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rollout_mses, label='MSE to ground-truth state')\n",
    "plt.plot(rollout_mses_masked, label='MSE to observation (masked & noisy)')\n",
    "plt.xlabel('rollout time step t')\n",
    "plt.legend()\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# share notebook results via html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html data_assimilation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
