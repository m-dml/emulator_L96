{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Solve a fully-observed inverse problem\n",
    "\n",
    "Given $x_T$, estimate $x_0$ by matching $G^{(T)}(x_0)$ to $x_T$. Use autodiff on $G$ to calculate gradients of an error metric w.r.t. $x_0$. Compare the resulting rollout to the original `true' simulation.\n",
    "\n",
    "Compare three approaches:\n",
    "- $argmin_{x_0} || x_T - G^{(T)}(x_i) ||$, i.e. $T$ steps in one go\n",
    "- $argmin_{x_i} || x_{i+T_i} - G^{(T_i)}(x_i) ||$, i.e. $T_i$ steps at a time, with $\\sum_i T_i = T$. In the extreme case of $T_i=1$, this becomes very similar to implicit numerical methods. Can invertible neural networks help beyond providing better initializations for $x_i$ ? \n",
    "- solving backwards: more of the extreme case of $\\forall i: T_i=1$, however: Only for some forward numerical solvers can we just reverse time [1] and expect to return to initial conditions. Leap-frog works, but e.g. forward-Euler time-reversed is backward-Euler. \n",
    "\n",
    "Generally, how do these approaches differ around \\& beyond the horizon of predictability? Which solutions do they pick, and how easy is it to get uncertainties from them?\n",
    "\n",
    "[1] https://scicomp.stackexchange.com/questions/32736/forward-and-backward-integration-cause-of-errors?noredirect=1&lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, J1, J1_init, f1_juliadef, f2_juliadef\n",
    "from L96_emulator.util import predictor_corrector, rk4_default\n",
    "from L96_emulator.run import sel_dataset_class\n",
    "\n",
    "try: \n",
    "    K, J, T, dt = args['K'], args['J'], args['T'], args['dt']\n",
    "    spin_up_time, train_frac = args['spin_up_time'], args['train_frac']\n",
    "    normalize_data = bool(args['normalize_data'])\n",
    "except:\n",
    "    K, J, T, dt = 36, 10, 605, 0.01\n",
    "    spin_up_time, train_frac = 5., 0.8\n",
    "    normalize_data = False\n",
    "\n",
    "F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "resimulate, save_sim = True, True\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,50)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, int(np.floor(T/dt)+1))\n",
    "    \n",
    "    out = rk4_default(fun=fun, y0=X_init.copy(), times=times)\n",
    "\n",
    "    # filename for data storage\n",
    "    if save_sim: \n",
    "        np.save(data_dir + fn_data, out.astype(dtype=dtype_np))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(out.shape[0]*train_frac)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick a (trained) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 20\n",
    "\n",
    "exp_names = os.listdir('experiments/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "print('conf_exp', conf_exp)\n",
    "\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose numerical solver scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['model_forwarder'] = 'rk4_default'\n",
    "args['dt_net'] = dt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load & instantiate the emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "\n",
    "model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)\n",
    "n_starts = np.array([5000, 10000, 15000])\n",
    "i = 0\n",
    "for i in range(len(n_starts)):\n",
    "    inputs = out[n_starts[i]]\n",
    "    inputs_torch = torch.as_tensor(sortL96intoChannels(np.atleast_2d(out[n_starts[i]]),J=J),dtype=dtype,device=device)\n",
    "\n",
    "    MSE = ((fun(0., inputs) - sortL96fromChannels(model.forward(inputs_torch).detach().cpu().numpy()))**2).mean()\n",
    "    print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simulate an example rollout from the emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "model_simulate = get_rollout_fun(dg_train, model_forwarder, prediction_task)\n",
    "\n",
    "T_start, T_dur = 10*spin_up_time, 10\n",
    "n_start, n_dur = int(T_start/dt), int(T_dur/dt)\n",
    "\n",
    "out_model = model_simulate(y0=dg_train[n_start].copy(), \n",
    "                           dy0=dg_train[n_start]-dg_train[n_start-dg_train.offset],\n",
    "                           n_steps=n_dur)\n",
    "out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "solver_comparison = True \n",
    "if solver_comparison:\n",
    "    try: \n",
    "        print(F, h, b, c)\n",
    "    except: \n",
    "        F, h, b, c = 10, 1, 10, 10\n",
    "    1\n",
    "    times_ = np.linspace(0, T_dur, 2*n_dur+1) # + n_start\n",
    "    out2 = rk4_default(fun=fun, y0=out[n_start], times=times_)[::2]\n",
    "else:\n",
    "    out2 = None\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out2, n_start=n_start, n_steps=n_dur, K=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving a fully-observed inverse problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_starts = np.arange(int(spin_up_time/dt), int(train_frac*out.shape[0]), 2* int(spin_up_time/dt))\n",
    "T_rollout, N = 40, len(n_starts)\n",
    "n_chunks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "if J > 0:\n",
    "    def negfun(t, x):\n",
    "        return - f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def negfun(t, x):\n",
    "        return - f1(x, F, dX_dt, K)\n",
    "\n",
    "n_steps, lr, weight_decay = 200, 1.0, 0.0\n",
    "\n",
    "loss_vals_backsolve = np.zeros(n_steps)\n",
    "time_vals_backsolve = time.time() * np.ones(n_steps)\n",
    "\n",
    "target = out[n_starts+T_rollout]\n",
    "state_mses_backsolve = np.zeros(n_chunks)\n",
    "\n",
    "x_init = np.zeros((len(n_starts), K*(J+1)))\n",
    "\n",
    "i_ = 0\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for c_, dt_fac in enumerate([1, 10, 100, 1000]):\n",
    "    plt.subplot(1,4,c_+1)\n",
    "    for j in range(n_chunks):\n",
    "\n",
    "        T_i = (j+1)*T_rollout//n_chunks\n",
    "        times = np.linspace(0, dt*T_i, dt_fac*T_i+1)\n",
    "        print('backward solving')\n",
    "        for i__ in range(len(n_starts)):\n",
    "            out2 = rk4_default(fun=negfun, y0=out[n_starts[i__]+T_rollout].copy(), times=times)\n",
    "            x_init[i__] = out2[-1].copy()\n",
    "        state_mses_backsolve[j] = ((x_init - target)**2).mean()\n",
    "    plt.plot(T_rollout//n_chunks*np.arange(1,n_chunks+1), state_mses_backsolve)\n",
    "    plt.xlabel('T_rollout')\n",
    "    plt.ylabel('MSE of iniitial state estimate')\n",
    "    plt.title(f'dt={dt/dt_fac}')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, split rollout time into chunks, solve sequentially from end to beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "n_steps, lr, weight_decay = 200, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_chunks = time.time() * np.ones(n_steps)\n",
    "loss_vals_LBFGS_chunks_rollout = np.zeros_like(loss_vals_LBFGS_chunks)\n",
    "T_rollout_i = (T_rollout//n_chunks) * np.ones(n_chunks, dtype=np.int)\n",
    "\n",
    "x_inits = np.zeros((n_chunks, N, K*(J+1)))\n",
    "x_init = sortL96intoChannels(np.atleast_2d(out[n_starts+T_rollout].copy()),J=J)\n",
    "targets = np.zeros((n_chunks, N, K*(J+1)))\n",
    "targets[0] = out[n_starts+T_rollout]\n",
    "\n",
    "x_sols_LBFGS_chunks = np.zeros_like(x_inits)\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                                        N=N, T=T_rollout_i[j], \n",
    "                                        x_init=x_init)\n",
    "    x_inits[j] = sortL96fromChannels(roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy())\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=100, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    target = sortL96intoChannels(torch.as_tensor(targets[j], dtype=dtype, device=device),J=J)\n",
    "    target_rollout = sortL96intoChannels(torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device),J=J)\n",
    "    roller_outer_LBFGS_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward() - target)**2).mean()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_chunks.forward() - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_chunks[i_] = time.time() - time_vals_LBFGS_chunks[i_]\n",
    "        print((time_vals_LBFGS_chunks[i_], loss_vals_LBFGS_chunks[i_]))\n",
    "\n",
    "        roller_outer_LBFGS_chunks.T = (j+1)*T_rollout//n_chunks\n",
    "        loss = ((roller_outer_LBFGS_chunks.forward() - target_rollout)**2).mean()\n",
    "        loss_vals_LBFGS_chunks_rollout[i_] = loss.detach().cpu().numpy().copy()\n",
    "        roller_outer_LBFGS_chunks.T = T_rollout_i[j]\n",
    "\n",
    "        i_ += 1\n",
    "\n",
    "    x_init = roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()\n",
    "    x_sols_LBFGS_chunks[j] = sortL96fromChannels(roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy())\n",
    "    if j < n_chunks - 1:\n",
    "        targets[j+1] = sortL96fromChannels(roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy())\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks, label='initialization')\n",
    "plt.title('rollout chunk state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_chunks_rollout, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go\n",
    "- warning, this can be excruciatingly slow and hard to converge !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "n_steps, lr, weight_decay = 1000, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_full_persistence = np.zeros(n_steps)\n",
    "time_vals_LBFGS_full_persistence = time.time() * np.ones(n_steps)\n",
    "x_init = sortL96intoChannels(out[n_starts+T_rollout].copy(), J=J)\n",
    "target = sortL96intoChannels(torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device), J=J)\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_LBFGS_full = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                                        N=N, T=(j+1)*T_rollout//n_chunks, x_init=x_init)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_full.parameters(),\n",
    "                                  lr=lr,\n",
    "                                  max_iter=20,\n",
    "                                  max_eval=None,\n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09,\n",
    "                                  history_size=50,\n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    roller_outer_LBFGS_full.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_full.forward() - target)**2).mean()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_full.forward() - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_full_persistence[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_full_persistence[i_] = time.time() - time_vals_LBFGS_full_persistence[i_]\n",
    "        print((time_vals_LBFGS_full_persistence[i_], loss_vals_LBFGS_full_persistence[i_]))\n",
    "        i_ += 1\n",
    "\n",
    "        \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_full_persistence, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go, initialize from chunked approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "n_steps, lr, weight_decay = 200, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_full_chunks = np.zeros(n_steps)\n",
    "time_vals_LBFGS_full_chunks = time.time() * np.ones(n_steps)\n",
    "target = sortL96intoChannels(torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device), J=J)\n",
    "\n",
    "x_sols_LBFGS_full_chunks = np.zeros(x_sols_LBFGS_chunks.shape)\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    x_init = sortL96intoChannels(x_sols_LBFGS_chunks[j], J=J)\n",
    "    roller_outer_LBFGS_full_chunks = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                                        N=N, T=(j+1)*T_rollout//n_chunks, x_init=x_init)\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_full_chunks.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=100, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "    roller_outer_LBFGS_full_chunks.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_full_chunks.forward() - target)**2).mean()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_full_chunks.forward() - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_full_chunks[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_full_chunks[i_] = time.time() - time_vals_LBFGS_full_chunks[i_]\n",
    "        print((time_vals_LBFGS_full_chunks[i_], loss_vals_LBFGS_full_chunks[i_]))\n",
    "        i_ += 1\n",
    "\n",
    "    x_sols_LBFGS_full_chunks[j] = sortL96fromChannels(roller_outer_LBFGS_full_chunks.X.detach().cpu().numpy().copy())\n",
    "        \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_full_chunks, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L-BFGS, solve across full rollout time in one go, initiate from backward solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return - f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return - f1(x, F, dX_dt, K)\n",
    "\n",
    "n_steps, lr, weight_decay = 200, 1.0, 0.0\n",
    "\n",
    "loss_vals_LBFGS_full_backsolve = np.zeros(n_steps)\n",
    "time_vals_LBFGS_full_backsolve = time.time() * np.ones(n_steps)\n",
    "target = sortL96intoChannels(torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device), J=J)\n",
    "\n",
    "x_init = np.zeros((len(n_starts), K*(J+1)))\n",
    "x_sols_LBFGS_full_backsolve = np.zeros_like(x_inits)\n",
    "\n",
    "i_ = 0\n",
    "for j in range(n_chunks):\n",
    "    \n",
    "    T_i = (j+1)*T_rollout//n_chunks\n",
    "    times = dt * np.linspace(0, T_i, 10 * T_i+1) # note the 10x increase in temporal resolution!\n",
    "    print('backward solving')\n",
    "    for i__ in range(len(n_starts)):\n",
    "        out2 = rk4_default(fun=fun, y0=out[n_starts[i__]+T_rollout].copy(), times=times)\n",
    "        x_init[i__] = out2[-1].copy()\n",
    "\n",
    "    roller_outer_LBFGS_full_backsolve = Rollout(model_forwarder, prediction_task='state', K=K, J=J, \n",
    "                                        N=N, T=(j+1)*T_rollout//n_chunks, \n",
    "                                        x_init=sortL96intoChannels(x_init,J=J))\n",
    "    optimizer = torch.optim.LBFGS(params=roller_outer_LBFGS_full_backsolve.parameters(), \n",
    "                                  lr=lr, \n",
    "                                  max_iter=100, \n",
    "                                  max_eval=None, \n",
    "                                  tolerance_grad=1e-07, \n",
    "                                  tolerance_change=1e-09, \n",
    "                                  history_size=50, \n",
    "                                  line_search_fn='strong_wolfe')\n",
    "\n",
    "    roller_outer_LBFGS_full_backsolve.train()\n",
    "    for i in range(n_steps//n_chunks):\n",
    "\n",
    "        loss = ((roller_outer_LBFGS_full_backsolve.forward() - target)**2).mean()\n",
    "        def closure():\n",
    "            loss = ((roller_outer_LBFGS_full_backsolve.forward() - target)**2).mean()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            return loss            \n",
    "        optimizer.step(closure)        \n",
    "        loss_vals_LBFGS_full_backsolve[i_] = loss.detach().cpu().numpy()\n",
    "        time_vals_LBFGS_full_backsolve[i_] = time.time() - time_vals_LBFGS_full_backsolve[i_]\n",
    "        print((time_vals_LBFGS_full_backsolve[i_], loss_vals_LBFGS_full_backsolve[i_]))\n",
    "        i_ += 1\n",
    "        \n",
    "    x_sols_LBFGS_full_backsolve[j] = sortL96fromChannels(roller_outer_LBFGS_full_backsolve.X.detach().cpu().numpy().copy())\n",
    "            \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_LBFGS_full_backsolve, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(res_dir + 'results/data_assimilation/fullyobs_initstate_tests',\n",
    "        arr={\n",
    "            'loss_vals_LBFGS_full_backsolve' : loss_vals_LBFGS_full_backsolve, \n",
    "            'loss_vals_LBFGS_full_persistence' : loss_vals_LBFGS_full_persistence,\n",
    "            'loss_vals_LBFGS_full_chunks' : loss_vals_LBFGS_full_chunks,\n",
    "            'loss_vals_LBFGS_chunks_rollout' : loss_vals_LBFGS_chunks_rollout,\n",
    "            'time_vals_LBFGS_full_backsolve' : time_vals_LBFGS_full_backsolve,\n",
    "            'time_vals_LBFGS_full_persistence' : time_vals_LBFGS_full_persistence,\n",
    "            'time_vals_LBFGS_full_chunks' : time_vals_LBFGS_full_chunks,\n",
    "            'time_vals_LBFGS_chunks' : time_vals_LBFGS_chunks\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appr_names = ['full optim, init from backsolve', 'full optim, init from chunks',\n",
    "              'full optim, init from persistence',\n",
    "              'optim over single chunk (full rollout error)']\n",
    "all_losses = [loss_vals_LBFGS_full_backsolve, loss_vals_LBFGS_full_chunks, loss_vals_LBFGS_full_persistence, loss_vals_LBFGS_chunks_rollout]\n",
    "all_times =  [time_vals_LBFGS_full_backsolve, time_vals_LBFGS_full_chunks, time_vals_LBFGS_full_persistence, time_vals_LBFGS_chunks]\n",
    "all_losses, all_times\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "for i,loss in enumerate(all_losses):\n",
    "    xx = np.arange(len(loss))+1 if len(loss) == 1000 else np.arange(0, 10*len(loss), 10)+1\n",
    "    plt.semilogy(xx, loss, label=appr_names[i])        \n",
    "\n",
    "loss = loss_vals_LBFGS_chunks\n",
    "xx = np.arange(len(loss))+1 if len(loss) == 1000 else np.arange(0, 10*len(loss), 10)+1\n",
    "plt.semilogy(xx, loss, 'k--', alpha=0.3, label='optim over single chunk (current chunk error)')        \n",
    "\n",
    "for i in range(n_chunks):\n",
    "    plt.semilogy(100*i + 100*np.array([0.05, 0.95]), 1e-10*np.ones(2), 'k')\n",
    "    plt.text(10*i*(T_rollout//n_chunks), 5e-11, f'T_rollout={(i+1)*T_rollout//n_chunks}')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('# gradient step')\n",
    "plt.ylabel('rollout MSE')\n",
    "plt.suptitle('optimization error for different initialization methods')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with plain gradient descent (SGD with single data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from L96_emulator.eval import Rollout\n",
    "\n",
    "\n",
    "n_starts = np.array([5000, 10000, 15000])\n",
    "T_rollout, N = 100, len(n_starts)\n",
    "n_chunks = 20\n",
    "\n",
    "target = torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device)\n",
    "\n",
    "x_init = out[n_starts+T_rollout].copy()\n",
    "roller_outer_SGD = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_init)\n",
    "x_init = roller_outer_SGD.X.detach().cpu().numpy().copy()\n",
    "\n",
    "n_steps, lr, weight_decay = 500, 0.01, 0.0\n",
    "roller_outer_SGD.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(roller_outer_SGD.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "#optimizer = torch.optim.LBFGS(params=roller_outer.parameters(), \n",
    "#                              lr=lr, \n",
    "#                              max_iter=20, \n",
    "#                              max_eval=None, \n",
    "#                              tolerance_grad=1e-07, \n",
    "#                              tolerance_change=1e-09, \n",
    "#                              history_size=100, \n",
    "#                              line_search_fn=None)\n",
    "loss_vals_SGD = np.zeros(n_steps)\n",
    "time_vals_SGD = time.time() * np.ones(n_steps)\n",
    "for i in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = ((roller_outer_SGD.forward(T=T_rollout) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_vals_SGD[i] = loss.detach().cpu().numpy()\n",
    "        time_vals_SGD[i] = time.time() - time_vals_SGD[i]\n",
    "        print((time_vals_SGD[i], loss_vals_SGD[i]))\n",
    "        \n",
    "plt.figure(figsize=(8,2))\n",
    "plt.semilogy(loss_vals_SGD, label='initialization')\n",
    "plt.title('rollout final state loss across gradient descent steps')\n",
    "plt.ylabel('MSE)')\n",
    "plt.xlabel('gradient step')\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MSEs_chunks = np.zeros(n_chunks)\n",
    "MSEs_direct__init_chunks = np.zeros(n_chunks)\n",
    "MSEs_direct__init_prev = np.zeros(n_chunks)\n",
    "\n",
    "target = torch.as_tensor(out[n_starts+T_rollout], dtype=dtype, device=device)\n",
    "for j in range(n_chunks):\n",
    "\n",
    "    roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_sols[j])\n",
    "    MSEs_chunks[j] = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean().detach().cpu().numpy()\n",
    "\n",
    "    #roller_outer_LBFGS_chunks = Rollout(model_forward, prediction_task='state', K=K, J=J, N=N, x_init=x_sols[j])\n",
    "    #MSEs_chunks[j] = ((roller_outer_LBFGS_chunks.forward(T=(j+1)*T_rollout//n_chunks) - target)**2).mean().detach().cpu().numpy()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(16,16))\n",
    "for i in range(N):\n",
    "    plt.subplot(2,N,i+1)\n",
    "    plt.plot(roller_outer_ADAM.X.detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.X.detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.plot(roller_outer_LBFGS_chunks.X.detach().cpu().numpy().copy()[i], label='in 10 chunks, L-BFGS')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,N,N+i+1)\n",
    "    plt.plot(roller_outer_ADAM.forward(T=T_rollout).detach().cpu().numpy().copy()[i], label='one go')\n",
    "    plt.plot(roller_outer_test.forward(T=T_rollout).detach().cpu().numpy().copy()[i], '--', label='in 10 chunks')\n",
    "    plt.plot(roller_outer_LBFGS_chunks.forward(T=T_rollout).detach().cpu().numpy().copy()[i], '--', label='in 10 chunks, L-BFGS')\n",
    "    plt.legend()\n",
    "    \n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# share notebook results via html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html data_assimilation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
