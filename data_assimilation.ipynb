{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Solve a fully-observed inverse problem\n",
    "\n",
    "Given $x_T$, estimate $x_0$ by matching $G^{(T)}(x_0)$ to $x_T$. Use autodiff on $G$ to calculate gradients of an error metric w.r.t. $x_0$. Compare the resulting rollout to the original `true' simulation.\n",
    "\n",
    "Compare three approaches:\n",
    "- $argmin_{x_0} || x_T - G^{(T)}(x_i) ||$, i.e. $T$ steps in one go\n",
    "- $argmin_{x_i} || x_{i+T_i} - G^{(T_i)}(x_i) ||$, i.e. $T_i$ steps at a time, with $\\sum_i T_i = T$. In the extreme case of $T_i=1$, this becomes very similar to implicit numerical methods. Can invertible neural networks help beyond providing better initializations for $x_i$ ? \n",
    "- solving backwards: more of the extreme case of $\\forall i: T_i=1$, however: Only for some forward numerical solvers can we just reverse time [1] and expect to return to initial conditions. Leap-frog works, but e.g. forward-Euler time-reversed is backward-Euler. \n",
    "\n",
    "Generally, how do these approaches differ around \\& beyond the horizon of predictability? Which solutions do they pick, and how easy is it to get uncertainties from them?\n",
    "\n",
    "[1] https://scicomp.stackexchange.com/questions/32736/forward-and-backward-integration-cause-of-errors?noredirect=1&lq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick a (trained) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 24\n",
    "J = 0\n",
    "if exp_id is None: \n",
    "    # loading 'perfect' (up to machine-precision-level quirks) L96 model in pytorch    \n",
    "    if J > 0:\n",
    "        conf_exp = '00_analyticalMinimalConvNet'\n",
    "    else:\n",
    "        conf_exp = '00_analyticalMinimalConvNet_oneLevel'\n",
    "else:\n",
    "    exp_names = os.listdir('experiments/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "    \n",
    "print('conf_exp', conf_exp)\n",
    "\n",
    "\n",
    "model_forwarder_str = 'rk4_default'\n",
    "\n",
    "optimizer_str = 'LBFGS' #'LBFGS', 'SGD'\n",
    "obs_operator_str = 'ObsOp_identity' #'ObsOp_subsampleGaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not obs_operator_str is None:\n",
    "    fn = f'fullyobs_initstate_tests_exp{exp_id}_{model_forwarder_str}_{optimizer_str}_{obs_operator_str}.npy'\n",
    "else:\n",
    "    fn = f'fullyobs_initstate_tests_exp{exp_id}_{model_forwarder_str}_{optimizer_str}.npy'\n",
    "\n",
    "res = np.load(res_dir + 'results/data_assimilation/'+ fn, allow_pickle=True)[()]\n",
    "\n",
    "targets=res['targets']\n",
    "initial_states=res['initial_states']\n",
    "\n",
    "J = res['J']\n",
    "n_steps = res['n_steps']\n",
    "n_chunks = res['n_chunks']\n",
    "n_chunks_recursive = res['n_chunks_recursive']\n",
    "T_rollout = res['T_rollout']\n",
    "dt = res['dt']\n",
    "back_solve_dt_fac=res['back_solve_dt_fac']\n",
    "n_starts = res['n_starts']\n",
    "\n",
    "optimiziation_schemes = res['optimiziation_schemes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "#\"\"\"\n",
    "appr_sel = {'LBFGS_chunks' : True,\n",
    "            'LBFGS_full_chunks' : True,\n",
    "            'LBFGS_recurse_chunks' : True,\n",
    "            'LBFGS_full_persistence' : True,\n",
    "            'backsolve' : True,\n",
    "            'LBFGS_full_backsolve' : True,\n",
    "           } \n",
    "#\"\"\"\n",
    "appr_sel = optimiziation_schemes.keys()\n",
    "\n",
    "appr_names = {\n",
    "    'LBFGS_chunks' : 'optim over single chunk (current chunk error)',\n",
    "    'LBFGS_full_backsolve' : 'full optim, init from backsolve', \n",
    "    'LBFGS_full_chunks' : 'full optim, init from chunks',\n",
    "    'LBFGS_full_persistence' : 'full optim, init from persistence',\n",
    "    'LBFGS_recurse_chunks' : 'full optim, recursive from backsolve',\n",
    "    'backsolve' : 'forward solve in reverse'\n",
    "}\n",
    "\n",
    "plt_styles = {\n",
    "    'LBFGS_chunks' : 'r--',\n",
    "    'LBFGS_full_backsolve' : 'b-', \n",
    "    'LBFGS_full_chunks' : 'm-',\n",
    "    'LBFGS_full_persistence' : 'g-',\n",
    "    'LBFGS_recurse_chunks' : 'k-',\n",
    "    'backsolve' : 'c--'        \n",
    "}\n",
    "\n",
    "\n",
    "# compute state differences\n",
    "\n",
    "state_diff = {}\n",
    "state_diff['LBFGS_chunks'] = [((sortL96fromChannels(res['targets']) - res['x_sols_LBFGS_chunks'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "state_diff['LBFGS_chunks'] += [(np.diff(res['x_sols_LBFGS_chunks'],axis=0)**2).mean(axis=2)]\n",
    "state_diff['LBFGS_chunks'] = np.concatenate(state_diff['LBFGS_chunks'])\n",
    "\n",
    "x_init = res['x_sols_LBFGS_chunks'][res['recursions_per_chunks']-1::res['recursions_per_chunks']]\n",
    "state_diff['LBFGS_full_chunks'] = ((x_init-res['x_sols_LBFGS_full_chunks'])**2).mean(axis=-1)\n",
    "\n",
    "state_diff['backsolve'] = [((sortL96fromChannels(res['targets']) - res['x_sols_backsolve'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "state_diff['backsolve'] += [(np.diff(res['x_sols_backsolve'],axis=0)**2).mean(axis=2)]\n",
    "state_diff['backsolve'] = np.concatenate(state_diff['backsolve'])\n",
    "\n",
    "x_init = res['x_sols_backsolve'][res['recursions_per_chunks']-1::res['recursions_per_chunks']]\n",
    "state_diff['LBFGS_full_backsolve'] = ((x_init-res['x_sols_LBFGS_full_backsolve'])**2).mean(axis=-1)\n",
    "\n",
    "x_init = sortL96fromChannels(res['targets'])\n",
    "state_diff['LBFGS_full_persistence'] = ((x_init-res['x_sols_LBFGS_full_persistence'])**2).mean(axis=-1)\n",
    "\n",
    "state_diff['LBFGS_recurse_chunks'] = [((sortL96fromChannels(res['targets']) - res['x_sols_LBFGS_recurse_chunks'][0])**2).mean(axis=1).reshape(1,-1)]\n",
    "state_diff['LBFGS_recurse_chunks'] += [(np.diff(res['x_sols_LBFGS_recurse_chunks'],axis=0)**2).mean(axis=2)]\n",
    "state_diff['LBFGS_recurse_chunks'] = np.concatenate(state_diff['LBFGS_recurse_chunks'])\n",
    "\n",
    "state_MSE_peristence = ((res['targets'].reshape(1,-1,res['J']+1, res['K']) - res['initial_states'])**2).mean(axis=(-2,-1))\n",
    "\n",
    "all_appr_names, all_losses, all_times, all_mses, all_stdfs, all_plt_styles = [], [], [], [], [], []\n",
    "for scheme_str in list(appr_sel):\n",
    "    if optimiziation_schemes[scheme_str]:\n",
    "\n",
    "        all_appr_names.append(appr_names[scheme_str])\n",
    "        all_losses.append(res['loss_vals_'+scheme_str])\n",
    "        all_times.append(res['time_vals_'+scheme_str])\n",
    "        all_mses.append(res['state_mses_'+scheme_str])\n",
    "        all_stdfs.append(state_diff[scheme_str])\n",
    "        all_plt_styles.append(plt_styles[scheme_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_avg = True\n",
    "if plot_avg:\n",
    "    all_losses = [l.mean(axis=1).reshape(-1,1) for l in all_losses]\n",
    "    all_times = [l.mean(axis=1).reshape(-1,1) for l in all_times]\n",
    "    all_mses = [l.mean(axis=1).reshape(-1,1) for l in all_mses]\n",
    "\n",
    "neg_log_loss_offset = (res['K']*(res['J']+1) * np.log(2.*np.pi))/2.\n",
    "\n",
    "plt.figure(figsize=(16,20))\n",
    "plt.subplot(4,1,1)\n",
    "for i,loss in enumerate(all_losses):\n",
    "    xx = np.linspace(0, T_rollout, loss.shape[0])\n",
    "    plt.semilogy(xx, loss[:,0] - neg_log_loss_offset, all_plt_styles[i], label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,loss in enumerate(all_losses):\n",
    "        xx = np.linspace(0, T_rollout, loss.shape[0])\n",
    "        plt.semilogy(xx, loss[:,1:] - neg_log_loss_offset, all_plt_styles[i])        \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('rollout step')\n",
    "plt.ylabel('neg. log-likelihood (up to constant)')\n",
    "plt.title('loss during optimization for different initialization methods')\n",
    "\n",
    "\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "for i,mse in enumerate(all_mses):\n",
    "    xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, mse[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,mse in enumerate(all_mses):\n",
    "        xx = np.linspace(0, T_rollout, mse.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, mse[:,1:], all_plt_styles[i], marker='.')        \n",
    "\n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, state_MSE_peristence[:,0], ':', color='orange', marker='x', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, state_MSE_peristence[:,1:], ':', color='orange', marker='x')        \n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('initial state MSE')\n",
    "plt.title('initial state error for different initialization methods')\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "for i,stdf in enumerate(all_stdfs):\n",
    "    xx = np.linspace(0, T_rollout, stdf.shape[0]+1)[1:]\n",
    "    plt.semilogy(xx, stdf[:,0], all_plt_styles[i], marker='.',label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i,stdf in enumerate(all_stdfs):\n",
    "        xx = np.linspace(0, T_rollout, stdf.shape[0]+1)[1:]\n",
    "        plt.semilogy(xx, stdf[:,1:], all_plt_styles[i], marker='.')        \n",
    "xx = np.linspace(0, T_rollout, state_MSE_peristence.shape[0]+1)[1:]\n",
    "plt.semilogy(xx, state_MSE_peristence[:,0], ':', color='orange', marker='x', label='persistence')        \n",
    "if not plot_avg:\n",
    "    plt.semilogy(xx, state_MSE_peristence[:,1:], ':', color='orange', marker='x')        \n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('mean-squared distance to initialization')\n",
    "plt.title('difference to initial state estimate initialization')\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "for i, tms in enumerate(all_times):\n",
    "    tms[np.where(tms > 1e6)[0]] = np.nan\n",
    "    xx = np.linspace(1, T_rollout, tms.shape[0])\n",
    "    plt.semilogy(xx, tms[:,0], all_plt_styles[i], label=all_appr_names[i])        \n",
    "if not plot_avg:\n",
    "    for i, tms in enumerate(all_times):\n",
    "        tms[np.where(tms > 1e6)[0]] = np.nan\n",
    "        xx = np.linspace(1, T_rollout, tms.shape[0])\n",
    "        plt.semilogy(xx, tms[:,1:], all_plt_styles[i])        \n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('rollout steps')\n",
    "plt.ylabel('computation time [s]')\n",
    "plt.title('full computation time for different initialization methods')\n",
    "\n",
    "plt.suptitle('exp_id : ' + conf_exp)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data\n",
    "\n",
    "out, datagen_dict = get_data(K=res['K'], J=res['J'], T=res['T'], dt=res['dt'], N_trials=1, \n",
    "                             F=res['F'], h=res['h'], b=res['b'], c=res['c'], \n",
    "                             resimulate=True, solver=rk4_default,\n",
    "                             save_sim=False, data_dir=data_dir)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# share notebook results via html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html data_assimilation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
