{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrization learning with emulators\n",
    "\n",
    "- first attempt, for now assuming a 'perfect' emulator (how to get that starting out from a suboptimal parametrization is an open research question)\n",
    "\n",
    "- heavily leaning on the setup of Rasp (2020), where Rasp generated training data to refine a parametrization 'within the loop' of a simulator. To do that, Rasp nudges a high-res model (two-level L96) to the parametrized low-res model (one-level L96), using the high-res model outputs as training targets for the paramtrization. The nudging is annoying an allegedly fiddly, and the usfulness of the training data depends on the validty of the nudging.\n",
    "- we here use a non-nudged free high-res simulation to generate training data for the paramtetrization, which we optimize directly with gradients through the differentiable emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "from L96_emulator.util import dtype, dtype_np, device, as_tensor\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup from Rasp (2020)\n",
    "https://arxiv.org/abs/1907.01351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "from L96_emulator.networks import named_network\n",
    "\n",
    "K, J, J_net = 36, 10, 0\n",
    "dt = 0.001\n",
    "\n",
    "model_name = 'MinimalConvNetL96'\n",
    "model_kwargs = {\n",
    "        'K_net' : K, \n",
    "        'J_net' : J_net, \n",
    "        'init_net' : 'analytical', \n",
    "        'dt_net' : dt, \n",
    "\n",
    "        'l96_F' : 10., \n",
    "        'l96_h' : 1., \n",
    "        'l96_b' : 10., \n",
    "        'l96_c' : 10., \n",
    "        'model_forwarder' : 'rk4_default',\n",
    "        'padding_mode' : 'circular'\n",
    "}\n",
    "\n",
    "model, model_forwarder = named_network(model_name, \n",
    "                                       n_input_channels=J_net+1, \n",
    "                                       n_output_channels=J_net+1, \n",
    "                                       seq_length=1, **model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define simple linear parametrization\n",
    "- grab two linear parametrizations from Rasp's paper (cf. figure 3): \n",
    "    - the 'ideal' one ($a=-0.31$, $b=-0.2$) trained on the real two-level L96 model (as above) and \n",
    "    - the 'bad' one ($a=-3/4$, $b=-0.4$) trained on different two-level L96 parameters $F=7, h=2, b=c=5$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.parametrization import Parametrization_lin, Parametrization_nn\n",
    "\n",
    "param_lin_good = Parametrization_lin(a=as_tensor(np.array([-0.31])), b=as_tensor(np.array([-0.2])))\n",
    "param_lin_bad = Parametrization_lin(a=as_tensor(np.array([-0.75])), b=as_tensor(np.array([-0.4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define ground-truth and parameterized models in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.parametrization import Parametrized_twoLevel_L96\n",
    "from L96_emulator.networks import Model_forwarder_rk4default\n",
    "from L96sim.L96_base import f1, f2, pf2\n",
    "import torch\n",
    "\n",
    "model_parametrized_bad = Parametrized_twoLevel_L96(emulator=model, \n",
    "                                               parametrization=param_lin_bad)\n",
    "model_forwarder_parametrized_bad = Model_forwarder_rk4default(model=model_parametrized_bad, dt=dt)\n",
    "\n",
    "model_parametrized_good = Parametrized_twoLevel_L96(emulator=model, \n",
    "                                               parametrization=param_lin_good)\n",
    "model_forwarder_parametrized_good = Model_forwarder_rk4default(model=model_parametrized_good, dt=dt)\n",
    "\n",
    "\n",
    "# ground-truth two-level L96 model (based on Numba implementation):\n",
    "\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, model_kwargs['l96_F'], model_kwargs['l96_h'], model_kwargs['l96_b'], model_kwargs['l96_c'], dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, model_kwargs['l96_F'], dX_dt, K)\n",
    "\n",
    "class Torch_solver(torch.nn.Module):\n",
    "    # numerical solver (from numpy/numba/Julia)\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "    def forward(self, x):\n",
    "        x = sortL96fromChannels(x.detach().cpu().numpy()).flatten()\n",
    "        return sortL96intoChannels(np.atleast_2d(self.fun(0., x)), J=J)\n",
    "\n",
    "model_forwarder_np = Model_forwarder_rk4default(Torch_solver(fun), dt=dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create some training data from the true two-level L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_init = model_kwargs['l96_F'] * (0.5 + np.random.randn(1,K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,50)\n",
    "\n",
    "def model_simulate(y0, dy0, n_steps):\n",
    "    x = np.empty((n_steps+1, *y0.shape[1:]), dtype=dtype_np)\n",
    "    x[0] = y0.copy()\n",
    "    xx = as_tensor(x[0]).reshape(1,1,-1)\n",
    "    for i in range(1,n_steps+1):\n",
    "        xx = model_forwarder_np(xx.reshape(1,J+1,-1))\n",
    "        x[i] = xx.detach().cpu().numpy().copy()\n",
    "    return x\n",
    "\n",
    "T_dur = 10000\n",
    "data_full = model_simulate(y0=sortL96intoChannels(X_init,J=J), dy0=None, n_steps=T_dur)\n",
    "\n",
    "# two-level simulates for fast and slow variables, we only take the slow ones for training !\n",
    "data = data_full[:,0,:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create another parametrized model to optimize\n",
    "- initialized with the 'bad' paramterization from above\n",
    "- we'll optimize the parametrization directly throught he (for now analytically perfect...) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lin_train = Parametrization_lin(a=as_tensor(np.array([-0.75])), b=as_tensor(np.array([-0.4])))\n",
    "\n",
    "model_parametrized_train = Parametrized_twoLevel_L96(emulator=model, \n",
    "                                                     parametrization=param_lin_train)\n",
    "for p in model_parametrized_train.emulator.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model_forwarder_parametrized_train = Model_forwarder_rk4default(model=model_parametrized_train, dt=dt)\n",
    "\n",
    "print('torch.nn.Parameters of parametrization require grad: ')\n",
    "for p in model_forwarder_parametrized_train.model.param.parameters():\n",
    "    print(p.requires_grad)\n",
    "    \n",
    "print('torch.nn.Parameters of emulator require grad: ')\n",
    "for p in model_forwarder_parametrized_train.model.emulator.parameters():\n",
    "    print(p.requires_grad)\n",
    "    \n",
    "print('initialized a', model_parametrized_train.param.a)\n",
    "print('initialized b', model_parametrized_train.param.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class, loss_function\n",
    "from L96_emulator.train import train_model\n",
    "\n",
    "lead_time = 1\n",
    "normalize_data = False\n",
    "prediction_task = 'state'\n",
    "N_trials = 1\n",
    "\n",
    "batch_size = 32\n",
    "train_frac = 0.8\n",
    "validation_frac = 0.1\n",
    "spin_up_time = 0.\n",
    "\n",
    "DatasetClass = sel_dataset_class(prediction_task, N_trials, local=False)\n",
    "test_frac = 1. - (train_frac + validation_frac)\n",
    "assert test_frac > 0.\n",
    "spin_up = int(spin_up_time/dt)\n",
    "\n",
    "dg_train = DatasetClass(data=data, J=J_net, offset=lead_time, normalize=bool(normalize_data), \n",
    "                   start=spin_up, \n",
    "                   end=int(np.floor(T_dur*train_frac)))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dg_train, batch_size=batch_size, drop_last=True, num_workers=0\n",
    ")\n",
    "dg_val   = DatasetClass(data=data, J=J_net, offset=lead_time, normalize=bool(normalize_data), \n",
    "                   start=int(np.ceil(T_dur*train_frac)),\n",
    "                   end=int(np.ceil(T_dur*(train_frac+validation_frac))))\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_val, batch_size=batch_size, drop_last=True, num_workers=0\n",
    ")\n",
    "\n",
    "loss_fun = loss_function(loss_fun='mse', extra_args={})\n",
    "training_outputs = train_model(\n",
    "    model=model_forwarder_parametrized_train,\n",
    "    train_loader=train_loader, \n",
    "    validation_loader=validation_loader, \n",
    "    device=device, \n",
    "    model_forward=model_forwarder_parametrized_train, \n",
    "    loss_fun=loss_fun,\n",
    "    max_epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check learned parametrization parameter a, b\n",
    "\n",
    "- initialized at 'bad' values $a=-0.75$, $b=-0.4$\n",
    "- should approach the 'ideal values $a = -0.37$, $b=-0.2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('learned a', model_parametrized_train.param.a)\n",
    "print('learned b', model_parametrized_train.param.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check quality of simulation\n",
    "- starting at the final state of the two-level L96 training data, we visually inspect how the solutions of the slow variables look for different (parameterized) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "\n",
    "X_init = data_full[-1].reshape(1,-1)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "model_forwarders = [Model_forwarder_rk4default(model, dt=dt),\n",
    "                    model_forwarder_parametrized_bad, \n",
    "                    model_forwarder_parametrized_train,\n",
    "                    model_forwarder_parametrized_good,\n",
    "                    model_forwarder_np]\n",
    "X_inits = [X_init[:,:K].copy(), X_init[:,:K].copy(), X_init[:,:K].copy(), X_init[:,:K].copy(), X_init.copy()]\n",
    "Js = [J_net, J_net, J_net, J_net, J]\n",
    "panel_titles=['one-level L96', 'bad linear parametrization', 'learned parametrization', 'good linear parametrization', 'full two-level L96']\n",
    "for i_model in range(len(model_forwarders)): \n",
    "    \n",
    "    model_forwarder_i, X_init_i, J_i = model_forwarders[i_model], X_inits[i_model], Js[i_model]\n",
    "\n",
    "    def model_simulate(y0, dy0, n_steps):\n",
    "        x = np.empty((n_steps+1, *y0.shape[1:]))\n",
    "        x[0] = y0.copy()\n",
    "        xx = as_tensor(x[0]).reshape(1,1,-1)\n",
    "        for i in range(1,n_steps+1):\n",
    "            xx = model_forwarder_i(xx.reshape(1,J_i+1,-1))\n",
    "            x[i] = xx.detach().cpu().numpy().copy()\n",
    "        return x\n",
    "\n",
    "    T_dur = 5000\n",
    "    out_model = model_simulate(y0=sortL96intoChannels(X_init_i,J=J_i), dy0=None, n_steps=T_dur)\n",
    "\n",
    "    plt.subplot(1,len(model_forwarders),i_model+1)\n",
    "    plt.imshow(sortL96fromChannels(out_model[:,:1,:]).T, aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(panel_titles[i_model])\n",
    "    \n",
    "    if i_model == 0:\n",
    "        plt.ylabel('location k')\n",
    "    if i_model == 2:\n",
    "        plt.xlabel('time [steps]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html parametrization.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
