{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load specific experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 20\n",
    "\n",
    "exp_names = os.listdir('experiments/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, J1, J1_init, f1_juliadef, f2_juliadef\n",
    "from L96_emulator.util import predictor_corrector\n",
    "\n",
    "try: \n",
    "    K, J, T, dt = args['K'], args['J'], args['T'], args['dt']\n",
    "    spin_up_time = args['spin_up_time']\n",
    "except:\n",
    "    F, h, b, c = 10, 1, 10, 10\n",
    "    K, J, T, dt = 36, 10, 605, 0.001\n",
    "    spin_up_time = 5.\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "\n",
    "resimulate = False\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0).astype(dtype=dtype_np) / np.maximum(J,10)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, int(np.floor(T/dt))+1)\n",
    "\n",
    "    if J > 0:\n",
    "        def fun(t, x):\n",
    "            return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "    else:\n",
    "        def fun(t, x):\n",
    "            return f1(x, F, dX_dt, K)\n",
    "\n",
    "    out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "    # filename for data storage\n",
    "    np.save(data_dir + fn_data, out.astype(dtype=dtype))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: create short comparison solution with different step size for numerical solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import solve_from_init\n",
    "\n",
    "solver_comparison = True \n",
    "\n",
    "if solver_comparison:\n",
    "    try: \n",
    "        print(F, h, b, c)\n",
    "    except: \n",
    "        F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "    T_burnin, T_comparison = int(spin_up_time/dt), 5000\n",
    "    out2 = solve_from_init(K, J, \n",
    "                           T_burnin=T_burnin, T_=T_comparison, dt=dt, \n",
    "                           F=F, h=h, b=b, c=c, \n",
    "                           data=out, dilation=2, norm_mean=0., norm_std=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn local emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%run -i 'main_train.py -c experiments/template.yml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "\n",
    "model, model_forward, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "model.layers_ks1, model.layers3x3\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import AnalyticModel_twoLevel\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch\n",
    "\n",
    "model = AnalyticModel_twoLevel(K=K, J=J, F=F, b=b, c=c, h=h, loc=1e3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels, init_torch_device\n",
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "from L96_emulator.eval import solve_from_init\n",
    "\n",
    "DatasetClass = sel_dataset_class(prediction_task=args['prediction_task'])\n",
    "dg_train = DatasetClass(data=out, J=J, offset=args['lead_time'], normalize=True, \n",
    "                   start=int(args['spin_up_time']/args['dt']), \n",
    "                   end=int(np.floor(out.shape[0]*args['train_frac'])))\n",
    "\n",
    "\n",
    "def model_forward(x):\n",
    "    alpha = 0.5\n",
    "    x = sortL96fromChannels(x.detach().cpu().numpy() * dg_train.std + dg_train.mean) \n",
    "    f0 = model.forward(x.T).T\n",
    "    f1 = model.forward((x + dt*f0).T).T\n",
    "    #f0 = fun(0., x.T).T\n",
    "    #f1 = fun(0., (x + dt*f0).T ).T    \n",
    "    out = (sortL96intoChannels(x + dt * (alpha*f0 + (1-alpha)*f1), J=J) - dg_train.mean) / dg_train.std \n",
    "    \n",
    "    return torch.as_tensor(out, device=device, dtype=dtype)\n",
    "\n",
    "model_simulate = get_rollout_fun(dg_train, model_forward, args['prediction_task'])\n",
    "\n",
    "T_start, T_dur = 100*int(spin_up_time/dt), 200\n",
    "out_model = model_simulate(y0=dg_train[T_start].copy(), \n",
    "                           dy0=dg_train[T_start]-dg_train[T_start-dg_train.offset],\n",
    "                           T=T_dur)\n",
    "out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "solver_comparison = True \n",
    "if solver_comparison:\n",
    "    try: \n",
    "        print(F, h, b, c)\n",
    "    except: \n",
    "        F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "    out2 = solve_from_init(K, J, \n",
    "                           T_burnin=T_start, T_=T_dur, dt=dt, \n",
    "                           F=F, h=h, b=b, c=c, \n",
    "                           data=out, dilation=2, norm_mean=0., norm_std=1.)\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out2, T_start=T_start, T_dur=T_dur, K=K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.train import calc_val_loss, loss_function\n",
    "from L96_emulator.util import init_torch_device\n",
    "\n",
    "device = init_torch_device()\n",
    "\n",
    "batch_size, train_frac, validation_frac = 32, 0.8, 0.1\n",
    "dg_val   = DatasetClass(data=out, J=J, offset=1, normalize=True, \n",
    "                   start=int(np.ceil(out.shape[0]*train_frac)), \n",
    "                   end=int(np.floor(out.shape[0]*(train_frac+validation_frac))))\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    dg_val, batch_size=batch_size, drop_last=False, num_workers=0 \n",
    ")\n",
    "\n",
    "loss_fun = loss_function(loss_fun='mse', extra_args={})\n",
    "\n",
    "\n",
    "model = AnalyticModel_twoLevel(K=K, J=J, F=F, b=b, c=c, h=h, loc=1e0, scale=1e6)\n",
    "def model_forward(x):\n",
    "    alpha = 0.5\n",
    "    x = sortL96fromChannels(x.detach().cpu().numpy() * dg_train.std + dg_train.mean) \n",
    "    f0 = model.forward(x.T).T\n",
    "    f1 = model.forward((x + dt*f0).T).T\n",
    "    #f0 = fun(0., x.T).T\n",
    "    #f1 = fun(0., (x + dt*f0).T ).T    \n",
    "    out = (sortL96intoChannels(x + dt * (alpha*f0 + (1-alpha)*f1), J=J) - dg_train.mean) / dg_train.std \n",
    "    \n",
    "    return torch.as_tensor(out, device=device, dtype=dtype)\n",
    "\n",
    "calc_val_loss(validation_loader, model_forward, device, loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.networks import AnalyticModel_oneLevel, AnalyticModel_twoLevel\n",
    "         \n",
    "J = 10\n",
    "F, h, b, c = 10, 1, 10, 10\n",
    "K, T, dt = 36, 605, 0.001\n",
    "spin_up_time = 5.\n",
    "\n",
    "\n",
    "print('simulating data')\n",
    "X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0) / np.maximum(J,10)\n",
    "dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "times = np.linspace(0, T, int(np.floor(T/dt))+1)\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)    \n",
    "    \n",
    "\n",
    "for skip_conn in [False, True]:\n",
    "    if J > 0:\n",
    "        def get_model(loc=1.):\n",
    "            return AnalyticModel_twoLevel(K, J, F=F, b=b, c=c, h=h, loc=loc, skip_conn=skip_conn)\n",
    "    else:\n",
    "        def get_model(loc=1.):\n",
    "            return AnalyticModel_oneLevel(K=K, F=F, loc=loc, skip_conn=skip_conn)\n",
    "\n",
    "    t = 500000\n",
    "    model = get_model()\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    dX_dt = np.empty(K*(J+1))\n",
    "    plt.plot(fun(t, out[t]) - model.forward(out[t]).flatten())\n",
    "    plt.title('difference between network and rhs of diff.eq.')\n",
    "    plt.xlabel('location k')\n",
    "    plt.subplot(1,2,2)\n",
    "    locs = 10.**np.arange(-5, 10)    \n",
    "    errs = np.zeros(len(locs))\n",
    "    for i,loc in enumerate(locs):\n",
    "        model = get_model(loc=loc)\n",
    "\n",
    "        errs[i] = np.mean((fun(t, out[t]) - model.forward(out[t]))**2)\n",
    "    plt.loglog(locs, errs)\n",
    "    plt.title('identity through nonlinearity: location for shifting data')\n",
    "    plt.xlabel('loc parameter')\n",
    "    plt.ylabel('MSE between network and rhs of diff.eq.')\n",
    "    plt.show()\n",
    "    print('one-step error:', np.min(errs))\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    y1 = out[t] + dt*model.forward(out[t])\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(dt/2*(model.forward(y1) + model.forward(out[t])), label='est')\n",
    "    plt.plot(out[t+1]-out[t], '--', label='finite-diff')\n",
    "    plt.legend()\n",
    "    plt.title(f'est. vs numerical temporal difference, example step t={t}')\n",
    "    plt.xlabel('location k')\n",
    "    plt.ylabel('state X_k')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(out[t+1]-out[t] - dt/2*(model.forward(y1) + model.forward(out[t])))\n",
    "    plt.title(f'error in temporal difference, example step t={t}')\n",
    "    plt.xlabel('location k')\n",
    "    plt.ylabel('state difference X_k - \\hat{X_k}')\n",
    "    plt.show()\n",
    "    print('example MSE :', np.mean( (out[t] + dt/2.*(model.forward(y1) + model.forward(out[t])) - out[t+1])**2 ))\n",
    "\n",
    "\n",
    "    MSE, t_range = 0., np.arange(5000, out.shape[0]-1, 100)\n",
    "    for t in t_range:\n",
    "        MSE += np.mean( (out[t] + dt/2.*(model.forward(y1) + model.forward(out[t])) - out[t+1])**2 )\n",
    "    MSE /= len(t_range)\n",
    "    print('subsampled MSE :', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data_std = np.linalg.norm(out[np.arange(T_dur+1)+T_start], axis=1)\n",
    "def prediction_horizon(data, est, threshold=0.1, data_std=1.):\n",
    "\n",
    "    assert data.ndim == 2\n",
    "    assert est.ndim == 2\n",
    "    L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "\n",
    "    return np.argmax(L2/data_std>threshold)\n",
    "\n",
    "def prediction_error(data, est, dt, T_horizon=None, data_std=1.):\n",
    "\n",
    "    T_horizon = int(np.ceil(0.5/dt)) if T_horizon is None else T_horizon\n",
    "    assert T_horizon == T_horizon//1, 'has to be integer'\n",
    "    T_horizon = int(T_horizon)\n",
    "    \n",
    "    assert data.ndim == 2\n",
    "    assert est.ndim == 2\n",
    "    L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "\n",
    "    return np.sum(L2[:T_horizon]/data_std) / (T_horizon*dt)\n",
    "\n",
    "pred_horizons = (prediction_horizon(out[np.arange(T_dur+1)+T_start], out_model, threshold=0.1, data_std=data_std), \n",
    "                 prediction_horizon(out[np.arange(T_dur+1)+T_start], out2[:T_dur+1], threshold=0.1, data_std=data_std))\n",
    "pred_errors = (prediction_error(out[np.arange(T_dur+1)+T_start], out_model, dt=args['dt'], data_std=data_std), \n",
    "              prediction_error(out[np.arange(T_dur+1)+T_start], out2[:T_dur+1], dt=args['dt'], data_std=data_std))\n",
    "\n",
    "data, est = out[np.arange(T_dur+1)+T_start], out_model\n",
    "L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "plt.plot(L2/data_std)\n",
    "\n",
    "pred_horizons, pred_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adams_bashfort(model_simulate, dg_train, T_start, T_dur):\n",
    "\n",
    "    y = dg_train[T_start].copy()\n",
    "    dy_prev = model_simulate(y0=y, dy0=None, T=1)[-1] - y    \n",
    "\n",
    "    out = np.empty((T_dur+1, *y.shape[1:]))\n",
    "    out[0] = y.copy()\n",
    "    y += dy_prev\n",
    "    out[1] = y.copy()\n",
    "\n",
    "    for t in range(2,T_dur+1):\n",
    "        dy_new = model_simulate(y0=y, dy0=None, T=1)[-1] - y\n",
    "        y += 0.5 * (3 * dy_new - 1. * dy_prev)\n",
    "        dy_prev = dy_new\n",
    "        out[t] = y\n",
    "\n",
    "    return out\n",
    "\n",
    "out_ab = adams_bashfort(model_simulate, dg_train, T_start, T_dur=T_dur)\n",
    "out_ab = sortL96fromChannels(out_ab * dg_train.std + dg_train.mean)\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out_ab, T_start=T_start, T_dur=T_dur)\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(['direct model reconstruction', 'Adams-Bashfort explicit step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug corner - might not execture anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### older fits - comparison of validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "conf_exps = [\n",
    "            f'0{i}_resnet_1x1convs_predictState' for i in range(1,6) # initially just gave the network fits version numbers V0 - V9\n",
    "          ]\n",
    "\n",
    "def find_weights(fn):\n",
    "    return fn[-3:] == '.pt'\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "for i, conf_exp in enumerate(conf_exps):\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "    exp_id, dim = args['exp_id'], args['J']+1\n",
    "\n",
    "    save_dir = res_dir + 'models/' + exp_id + '/'\n",
    "\n",
    "\n",
    "    #plt.subplot(1,2,1)\n",
    "    try:\n",
    "        training_outputs = np.load(save_dir + '_training_outputs' + '.npy', allow_pickle=True)[()]\n",
    "        training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "        plt.semilogy(validation_loss, label=exp_id + f' ({dim}D)')\n",
    "    except:\n",
    "        plt.semilogy(1., label=exp_id + f' ({dim}D)')            \n",
    "    plt.title('training')\n",
    "\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(16,4))\n",
    "cellText = np.hstack((np.array(conf_exps).reshape(-1,1), np.around(RMSEall,2)))\n",
    "collabel=('experiment', f'RMSE {lead_time}h, z 500', f'RMSE {lead_time}h, t 850')\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.table(cellText=cellText,colLabels=collabel,loc='center')\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_burnin = 10000\n",
    "out_model = model_simulate(y0=dg_train[T_burnin].copy(), dy0 = None, T=T_)#.reshape(-1, K*(J+1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(J+1):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,20,0.0000001, 0.5])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(J+1):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,1300,0.0000001, 1000])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_burnin = 10000\n",
    "out_model = model_simulate(y0=dg_train[T_burnin].copy(), T=T_)#.reshape(-1, K*(J+1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(K):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,20,0.0000001, 0.5])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(K):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,1300,0.0000001, 1000])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 10000\n",
    "plt.plot((dg_train.mean_in + dg_train.std_in * dg_train[t][0,:,:]).reshape(K*(J+1)) - out[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 5000\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(dg_train[t+0].flatten(), label='t=0')\n",
    "plt.plot(dg_train[t+1].flatten(), label='t=1')\n",
    "plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten(), 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 10000\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(dg_train[t+1].flatten() - dg_train[t+0].flatten(), label='sim')\n",
    "plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten()  - dg_train[t+0].flatten(), label='model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in [0, 100, 1000, 10000]:\n",
    "    plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten()  - dg_train[t+1].flatten(), label='model')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.semilogy(np.std(np.diff(dg_train[np.arange(T_+1)+ T_burnin].reshape(-1,(J+1)*K), axis=0), axis=0))\n",
    "plt.xlabel('variable ID (slow: first K=36)')\n",
    "plt.ylabel('std')\n",
    "plt.title('variability of 1-step temporal differences')\n",
    "plt.axis([0, 397, 0.001, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "T_burnin = 10000\n",
    "T_ = 10000\n",
    "plt.imshow(dg_train[np.arange(T_+1)+ T_burnin].reshape(-1,(J+1)*K).T, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dg_train[np.arange(T_+1)+ T_burnin][:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "T_burnin = 10000\n",
    "T_ = 10000\n",
    "plt.imshow(dg_train[np.arange(T_+1)+ T_burnin][:,:,0].reshape(-1,J+1).T, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,9))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(dg_train[np.arange(100)+T_burnin].reshape(100,-1).T - dg_train[T_burnin].reshape(-1,1), aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation, differences to yo')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(out_model[:100].reshape(100,-1).T - dg_train[T_burnin].reshape(-1,1), aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('model-reconstructed simulation, differences to yo')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.dataset import DatasetRelPred\n",
    "dg_train = DatasetRelPred(data=out, J=J, offset=temporal_offset, normalize=True, \n",
    "                          start=T_burnin, \n",
    "                          end=int(np.floor(out.shape[0]*0.8)))\n",
    "dg_val   = DatasetRelPred(data=out, J=J, offset=temporal_offset, normalize=True, \n",
    "                          start=int(np.ceil(out.shape[0]*0.8)), \n",
    "                          end=int(np.floor(out.shape[0]*0.9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "s = np.zeros((474000, 11, 36))\n",
    "for batch in dg_train:\n",
    "    X,Y = batch\n",
    "    #print(X.shape, Y.shape)\n",
    "    \n",
    "    s[ct] = Y.copy()\n",
    "    ct += 1\n",
    "    if ct > 1000:\n",
    "        pass #break\n",
    "m = np.mean(s, axis=(0,2))\n",
    "s = np.std(s, axis=(0,2))\n",
    "print(m, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
