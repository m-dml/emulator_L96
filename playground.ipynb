{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import L96sim\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'\n",
    "dtype = np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load specific experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "conf_exp = '12_resnet_1x1convs_predictState' #'template'\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load / simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, J1, J1_init, f1_juliadef, f2_juliadef\n",
    "from L96_emulator.util import predictor_corrector\n",
    "\n",
    "try: \n",
    "    K, J, T, dt = args['K'], args['J'], args['T'], args['dt']\n",
    "    spin_up_time = args['spin_up_time']\n",
    "except:\n",
    "    F, h, b, c = 10, 1, 10, 10\n",
    "    K, J, T, dt = 36, 10, 605, 0.001\n",
    "    spin_up_time = 5.\n",
    "\n",
    "fn_data = f'out_K{K}_J{J}_T{T}_dt0_{str(dt)[2:]}'\n",
    "\n",
    "resimulate = False\n",
    "if resimulate:\n",
    "    print('simulating data')\n",
    "    X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0) / np.maximum(J,10)\n",
    "    dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "    times = np.linspace(0, T, np.floor(T/dt)+1)\n",
    "\n",
    "    if J > 0:\n",
    "        def fun(t, x):\n",
    "            return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "    else:\n",
    "        def fun(t, x):\n",
    "            return f1(x, F, dX_dt, K)\n",
    "\n",
    "    out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "    # filename for data storage\n",
    "    np.save(data_dir + fn_data, out.astype(dtype=dtype))\n",
    "else:\n",
    "    print('loading data')\n",
    "    out = np.load(data_dir + fn_data + '.npy')\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(out.T, aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optional: create short comparison solution with different step size for numerical solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import solve_from_init\n",
    "\n",
    "solver_comparison = True \n",
    "\n",
    "if solver_comparison:\n",
    "    try: \n",
    "        print(F, h, b, c)\n",
    "    except: \n",
    "        F, h, b, c = 10, 1, 10, 10\n",
    "\n",
    "    T_burnin, T_comparison = int(spin_up_time/dt), 5000\n",
    "    out2 = solve_from_init(K, J, \n",
    "                           T_burnin=T_burnin, T_=T_comparison, dt=dt, \n",
    "                           F=F, h=h, b=b, c=c, \n",
    "                           data=out, dilation=2, norm_mean=0., norm_std=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn local emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%run -i 'main_train.py -c experiments/template.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "F, h, b, c = 10, 1, 10, 10\n",
    "K, J, T, dt = 36, 0, 605, 0.001\n",
    "spin_up_time = 5.\n",
    "\n",
    "print('simulating data')\n",
    "X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0) / np.maximum(J,10)\n",
    "dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "times = np.linspace(0, T, np.floor(T/dt)+1)\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "class AnalyticModel_oneLevel():\n",
    "\n",
    "    def __init__(self, K, F=10., loc=1., scale=1e6):\n",
    "\n",
    "        self.K = K\n",
    "        self.nonlinearity = lambda x: x**2\n",
    "\n",
    "        # approximate identity through x = s * (x/s + l)^2 / (2*l) + s * l\n",
    "        self.scale, self.loc = scale, loc\n",
    "        self.loc_grad = 2. * self.loc\n",
    "\n",
    "        kplus1, kminus1, kminus2 = self.td_mat(K,1), self.td_mat(K,-1), self.td_mat(K,-2)\n",
    "\n",
    "        self.W1 = np.vstack(\n",
    "            (kminus1, \n",
    "             kplus1 - kminus2,\n",
    "             kminus1 + kplus1 - kminus2,\n",
    "             np.eye(K) / self.scale)\n",
    "        )\n",
    "        self.b1 = np.zeros(4*K)\n",
    "        self.b1[3*K:] = self.loc * np.ones(K)\n",
    "\n",
    "        self.W2 = np.hstack(\n",
    "            (- np.eye(K) / 2., \n",
    "             - np.eye(K) / 2., \n",
    "               np.eye(K) / 2., \n",
    "             - self.scale / self.loc_grad * np.eye(K))\n",
    "        )\n",
    "        self.b2 = F * np.ones(K) -  self.W2.dot(self.b1**2)\n",
    "\n",
    "    def td_mat(self, K, k):\n",
    "        if K <= 0:\n",
    "            return np.array([[1.]]) if k==0 else np.array([[]])\n",
    "        ak, koff = np.abs(k), K+k if k <= 0 else k-K \n",
    "        return np.diag(np.ones(K-ak), k=k) + np.diag(np.ones(ak), k=koff)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        return self.W2.dot(self.nonlinearity(self.W1.dot(x) + self.b1)) + self.b2\n",
    "        \n",
    "if J == 0:\n",
    "    t = 10000\n",
    "    model = AnalyticModel_oneLevel(K, F=F, loc=10**5)\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    dX_dt = np.empty(K*(J+1))\n",
    "    plt.plot(fun(t, out[t]) - model.forward(out[t]))\n",
    "    plt.title('difference between network and rhs of diff.eq.')\n",
    "    plt.xlabel('location k')\n",
    "    plt.subplot(1,2,2)\n",
    "    locs = 10.**np.arange(-5, 10)    \n",
    "    errs = np.zeros(len(locs))\n",
    "    for i,loc in enumerate(locs):\n",
    "        model = AnalyticModel_oneLevel(K, F=F, loc=loc, scale=100000.)\n",
    "        errs[i] = np.mean((fun(t, out[t]) - model.forward(out[t]))**2)\n",
    "    plt.loglog(locs, errs)\n",
    "    plt.title('identity through nonlinearity: location for shifting data')\n",
    "    plt.xlabel('loc parameter')\n",
    "    plt.ylabel('MSE between network and rhs of diff.eq.')\n",
    "    plt.show()\n",
    "    \n",
    "    model = AnalyticModel_oneLevel(K=K, F=F,  loc=locs[np.argmin(errs)])\n",
    "    y1 = out[t] + dt*model.forward(out[t])\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(dt/2*(model.forward(y1) + model.forward(out[t])), label='est')\n",
    "    plt.plot(out[t+1]-out[t], '--', label='finite-diff')\n",
    "    plt.legend()\n",
    "    plt.title(f'est. vs numerical temporal difference, example step t={t}')\n",
    "    plt.xlabel('location k')\n",
    "    plt.ylabel('state X_k')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(out[t+1]-out[t] - dt/2*(model.forward(y1) + model.forward(out[t])))\n",
    "    plt.title(f'error in temporal difference, example step t={t}')\n",
    "    plt.xlabel('location k')\n",
    "    plt.ylabel('state difference X_k - \\hat{X_k}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, h, b, c = 10, 1, 10, 10\n",
    "K, J, T, dt = 36, 10, 605, 0.001\n",
    "spin_up_time = 5.\n",
    "\n",
    "print('simulating data')\n",
    "X_init = F * (0.5 + np.random.randn(K*(J+1)) * 1.0) / np.maximum(J,10)\n",
    "dX_dt = np.empty(X_init.size, dtype=X_init.dtype)\n",
    "times = np.linspace(0, T, np.floor(T/dt)+1)\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "out = predictor_corrector(fun=fun, y0=X_init.copy(), times=times, alpha=0.5)\n",
    "\n",
    "class AnalyticModel_twoLevel(AnalyticModel_oneLevel):\n",
    "\n",
    "    def __init__(self, K, J, F=10., b=10., c=10., h=1., loc=1., scale=1e6):\n",
    "\n",
    "        super(AnalyticModel_twoLevel, self).__init__(K=K, F=F, loc=loc, scale=scale)\n",
    "\n",
    "        kminus1, kplus1, kplus2 = self.td_mat(J*K,-1), self.td_mat(J*K,1), self.td_mat(J*K,2)\n",
    "        # block matrix: W1 = [W1 for X,    0, \n",
    "        #                        0,     W1 for Y]\n",
    "        W1_Y = np.vstack((kplus1, \n",
    "                          kminus1 - kplus2,\n",
    "                          kplus1 + kminus1 - kplus2,\n",
    "                          np.eye(J*K) / self.scale))\n",
    "        self.W1 = np.vstack((np.hstack((self.W1, np.zeros((4*K, J*K)))), \n",
    "                             np.hstack((np.zeros((4*J*K, K)), W1_Y))))\n",
    "\n",
    "        self.b1 = np.hstack((self.b1, np.zeros(4*J*K)))\n",
    "        self.b1[-J*K:] = self.loc * np.ones(J*K)\n",
    "\n",
    "        eyes = c * h / J * np.eye(K).flatten()\n",
    "        eyes = [eyes for i in range(J)]\n",
    "\n",
    "        self.W2 = np.hstack((self.W2, np.zeros((K, 4*J*K))))\n",
    "        # dependency of X_k on <Y_{k,J}>\n",
    "        self.W2[:, -J*K:] = - self.scale / self.loc_grad * np.vstack(eyes).T.reshape(K, J*K)\n",
    "        # dependency of Y_{k,j} on X_k\n",
    "        W2_Y = np.zeros((J*K, 4*K))\n",
    "        W2_Y[:,-K:] = self.scale / self.loc_grad * np.vstack(eyes).T.reshape(K,J*K).T\n",
    "        # dependencies of Y_{k,j} on Y_{k,~j}\n",
    "        W2_Y = np.hstack((W2_Y,\n",
    "                          c * np.hstack(\n",
    "                                  (- b * np.eye(J*K) / 2.,\n",
    "                                   - b * np.eye(J*K) / 2.,\n",
    "                                     b * np.eye(J*K) / 2.,\n",
    "                                   - self.scale / self.loc_grad * np.eye(J*K))  \n",
    "                          )\n",
    "                         ))\n",
    "        # block matrix: W2 = [W2 for X, `mean of Y`, \n",
    "        #                     `add X`,    W2 for Y]        \n",
    "        self.W2 = np.vstack((self.W2, W2_Y))\n",
    "\n",
    "        self.b2 = np.zeros(K*(J+1))\n",
    "        self.b2[:K] = F\n",
    "        self.b2 = self.b2 -  self.W2.dot(self.b1**2)\n",
    "\n",
    "    def forward2(self, x):        \n",
    "        return self.W1.dot(x) #+ self.b1\n",
    "    \n",
    "t = 5000\n",
    "model = AnalyticModel_twoLevel(K=K, J=J, F=F, b=b, c=c, h=h)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(1,2,1)\n",
    "dX_dt = np.empty(K*(J+1))\n",
    "plt.plot(fun(t, out[t]) - model.forward(out[t]))\n",
    "plt.title('difference between network and rhs of diff.eq.')\n",
    "plt.xlabel('location k')\n",
    "plt.subplot(1,2,2)\n",
    "locs = 10.**np.arange(-5, 10)    \n",
    "errs = np.zeros(len(locs))\n",
    "for i,loc in enumerate(locs):\n",
    "    model = AnalyticModel_twoLevel(K, J, F=F, b=b, c=c, h=h, loc=loc, scale=100000.)\n",
    "    errs[i] = np.mean((fun(t, out[t]) - model.forward(out[t]))**2)\n",
    "plt.loglog(locs, errs)\n",
    "plt.title('identity through nonlinearity: location for shifting data')\n",
    "plt.xlabel('loc parameter')\n",
    "plt.ylabel('MSE between network and rhs of diff.eq.')\n",
    "plt.show()\n",
    "\n",
    "model = AnalyticModel_twoLevel(K=K, J=J, F=F, b=b, c=c, h=h, loc=locs[np.argmin(errs)])\n",
    "y1 = out[t] + dt*model.forward(out[t])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(dt/2*(model.forward(y1) + model.forward(out[t])), label='est')\n",
    "plt.plot(out[t+1]-out[t], '--', label='finite-diff')\n",
    "plt.legend()\n",
    "plt.title(f'est. vs numerical temporal difference, example step t={t}')\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('state X_k')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(out[t+1]-out[t] - dt/2*(model.forward(y1) + model.forward(out[t])))\n",
    "plt.title(f'error in temporal difference, example step t={t}')\n",
    "plt.xlabel('location k')\n",
    "plt.ylabel('state difference X_k - \\hat{X_k}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "\n",
    "model, model_forward, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "model.layers_ks1, model.layers3x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "\n",
    "dtype = torch.float32\n",
    "\n",
    "DatasetClass = sel_dataset_class(prediction_task=args['prediction_task'])\n",
    "dg_train = DatasetClass(data=out, J=J, offset=args['lead_time'], normalize=True, \n",
    "                   start=int(args['spin_up_time']/args['dt']), \n",
    "                   end=int(np.floor(out.shape[0]*args['train_frac'])))\n",
    "\n",
    "model_simulate = get_rollout_fun(dg_train, model_forward, args['prediction_task'])\n",
    "\n",
    "T_start, T_dur = int(spin_up_time/dt), 400\n",
    "out_model = model_simulate(y0=dg_train[T_start].copy(), \n",
    "                           dy0=dg_train[T_start]-dg_train[T_start-dg_train.offset],\n",
    "                           T=T_dur)\n",
    "out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out2, T_start=T_start, T_dur=T_dur, K=K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data_std = np.linalg.norm(out[np.arange(T_dur+1)+T_start], axis=1)\n",
    "def prediction_horizon(data, est, threshold=0.1, data_std=1.):\n",
    "\n",
    "    assert data.ndim == 2\n",
    "    assert est.ndim == 2\n",
    "    L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "\n",
    "    return np.argmax(L2/data_std>threshold)\n",
    "\n",
    "def prediction_error(data, est, dt, T_horizon=None, data_std=1.):\n",
    "\n",
    "    T_horizon = int(np.ceil(0.5/dt)) if T_horizon is None else T_horizon\n",
    "    assert T_horizon == T_horizon//1, 'has to be integer'\n",
    "    T_horizon = int(T_horizon)\n",
    "    \n",
    "    assert data.ndim == 2\n",
    "    assert est.ndim == 2\n",
    "    L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "\n",
    "    return np.sum(L2[:T_horizon]/data_std) / (T_horizon*dt)\n",
    "\n",
    "pred_horizons = (prediction_horizon(out[np.arange(T_dur+1)+T_start], out_model, threshold=0.1, data_std=data_std), \n",
    "                 prediction_horizon(out[np.arange(T_dur+1)+T_start], out2[:T_dur+1], threshold=0.1, data_std=data_std))\n",
    "pred_errors = (prediction_error(out[np.arange(T_dur+1)+T_start], out_model, dt=args['dt'], data_std=data_std), \n",
    "              prediction_error(out[np.arange(T_dur+1)+T_start], out2[:T_dur+1], dt=args['dt'], data_std=data_std))\n",
    "\n",
    "data, est = out[np.arange(T_dur+1)+T_start], out_model\n",
    "L2 = np.sqrt(np.sum( (data - est)**2, axis=1 ))\n",
    "plt.plot(L2/data_std)\n",
    "\n",
    "pred_horizons, pred_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adams_bashfort(model_simulate, dg_train, T_start, T_dur):\n",
    "\n",
    "    y = dg_train[T_start].copy()\n",
    "    dy_prev = model_simulate(y0=y, dy0=None, T=1)[-1] - y    \n",
    "\n",
    "    out = np.empty((T_dur+1, *y.shape[1:]))\n",
    "    out[0] = y.copy()\n",
    "    y += dy_prev\n",
    "    out[1] = y.copy()\n",
    "\n",
    "    for t in range(2,T_dur+1):\n",
    "        dy_new = model_simulate(y0=y, dy0=None, T=1)[-1] - y\n",
    "        y += 0.5 * (3 * dy_new - 1. * dy_prev)\n",
    "        dy_prev = dy_new\n",
    "        out[t] = y\n",
    "\n",
    "    return out\n",
    "\n",
    "out_ab = adams_bashfort(model_simulate, dg_train, T_start, T_dur=T_dur)\n",
    "out_ab = sortL96fromChannels(out_ab * dg_train.std + dg_train.mean)\n",
    "\n",
    "fig = plot_rollout(out, out_model, out_comparison=out_ab, T_start=T_start, T_dur=T_dur)\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(['direct model reconstruction', 'Adams-Bashfort explicit step'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug corner - might not execture anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### older fits - comparison of validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "conf_exps = [\n",
    "            f'0{i}_resnet_1x1convs_predictState' for i in range(1,6) # initially just gave the network fits version numbers V0 - V9\n",
    "          ]\n",
    "\n",
    "def find_weights(fn):\n",
    "    return fn[-3:] == '.pt'\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(16,9))\n",
    "for i, conf_exp in enumerate(conf_exps):\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "    exp_id, dim = args['exp_id'], args['J']+1\n",
    "\n",
    "    save_dir = res_dir + 'models/' + exp_id + '/'\n",
    "\n",
    "\n",
    "    #plt.subplot(1,2,1)\n",
    "    try:\n",
    "        training_outputs = np.load(save_dir + '_training_outputs' + '.npy', allow_pickle=True)[()]\n",
    "        training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "        plt.semilogy(validation_loss, label=exp_id + f' ({dim}D)')\n",
    "    except:\n",
    "        plt.semilogy(1., label=exp_id + f' ({dim}D)')            \n",
    "    plt.title('training')\n",
    "\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "plt.figure(figsize=(16,4))\n",
    "cellText = np.hstack((np.array(conf_exps).reshape(-1,1), np.around(RMSEall,2)))\n",
    "collabel=('experiment', f'RMSE {lead_time}h, z 500', f'RMSE {lead_time}h, t 850')\n",
    "plt.axis('tight')\n",
    "plt.axis('off')\n",
    "plt.table(cellText=cellText,colLabels=collabel,loc='center')\n",
    "plt.show()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### more plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_burnin = 10000\n",
    "out_model = model_simulate(y0=dg_train[T_burnin].copy(), dy0 = None, T=T_)#.reshape(-1, K*(J+1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(J+1):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,20,0.0000001, 0.5])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(J+1):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(2,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,1300,0.0000001, 1000])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T_burnin = 10000\n",
    "out_model = model_simulate(y0=dg_train[T_burnin].copy(), T=T_)#.reshape(-1, K*(J+1))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(K):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,20,0.0000001, 0.5])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(K):\n",
    "    plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,i], \n",
    "             'b--')\n",
    "plt.semilogy(np.arange(1, T_+1), (np.mean( (out_model[1:] - dg_train[np.arange(1,T_+1)+ T_burnin])**2, axis=(1,) ))[:,0], 'k', linewidth=2,\n",
    "         label='slow variables')\n",
    "plt.semilogy(-1, 1, 'b--', label=f'fast variables (J={str(J)})')\n",
    "plt.axis([0,1300,0.0000001, 1000])\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('error over iterations, per variable type')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 10000\n",
    "plt.plot((dg_train.mean_in + dg_train.std_in * dg_train[t][0,:,:]).reshape(K*(J+1)) - out[t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 5000\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(dg_train[t+0].flatten(), label='t=0')\n",
    "plt.plot(dg_train[t+1].flatten(), label='t=1')\n",
    "plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten(), 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = 10000\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(dg_train[t+1].flatten() - dg_train[t+0].flatten(), label='sim')\n",
    "plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten()  - dg_train[t+0].flatten(), label='model')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in [0, 100, 1000, 10000]:\n",
    "    plt.plot(model_simulate(y0=dg_train[t+0].copy(), T=1)[-1,:,:].flatten()  - dg_train[t+1].flatten(), label='model')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.semilogy(np.std(np.diff(dg_train[np.arange(T_+1)+ T_burnin].reshape(-1,(J+1)*K), axis=0), axis=0))\n",
    "plt.xlabel('variable ID (slow: first K=36)')\n",
    "plt.ylabel('std')\n",
    "plt.title('variability of 1-step temporal differences')\n",
    "plt.axis([0, 397, 0.001, 0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "T_burnin = 10000\n",
    "T_ = 10000\n",
    "plt.imshow(dg_train[np.arange(T_+1)+ T_burnin].reshape(-1,(J+1)*K).T, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dg_train[np.arange(T_+1)+ T_burnin][:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,12))\n",
    "T_burnin = 10000\n",
    "T_ = 10000\n",
    "plt.imshow(dg_train[np.arange(T_+1)+ T_burnin][:,:,0].reshape(-1,J+1).T, aspect='auto', vmin=vmin, vmax=vmax)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,9))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(dg_train[np.arange(100)+T_burnin].reshape(100,-1).T - dg_train[T_burnin].reshape(-1,1), aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('numerical simulation, differences to yo')\n",
    "plt.colorbar()\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(out_model[:100].reshape(100,-1).T - dg_train[T_burnin].reshape(-1,1), aspect='auto')\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('location')\n",
    "plt.title('model-reconstructed simulation, differences to yo')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.dataset import DatasetRelPred\n",
    "dg_train = DatasetRelPred(data=out, J=J, offset=temporal_offset, normalize=True, \n",
    "                          start=T_burnin, \n",
    "                          end=int(np.floor(out.shape[0]*0.8)))\n",
    "dg_val   = DatasetRelPred(data=out, J=J, offset=temporal_offset, normalize=True, \n",
    "                          start=int(np.ceil(out.shape[0]*0.8)), \n",
    "                          end=int(np.floor(out.shape[0]*0.9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ct = 0\n",
    "s = np.zeros((474000, 11, 36))\n",
    "for batch in dg_train:\n",
    "    X,Y = batch\n",
    "    #print(X.shape, Y.shape)\n",
    "    \n",
    "    s[ct] = Y.copy()\n",
    "    ct += 1\n",
    "    if ct > 1000:\n",
    "        pass #break\n",
    "m = np.mean(s, axis=(0,2))\n",
    "s = np.std(s, axis=(0,2))\n",
    "print(m, s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
