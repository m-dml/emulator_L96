{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emulator evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "from L96_emulator.run import setup, sel_dataset_class\n",
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels, load_model_from_exp_conf\n",
    "from L96_emulator.networks import named_network, Model_forwarder_predictorCorrector, Model_forwarder_rk4default\n",
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data, as_tensor\n",
    "from L96sim.L96_base import f1, f2, pf2\n",
    "\n",
    "# experiments to use: \n",
    "\n",
    "# for one-level L96, K=40, F=8\n",
    "\n",
    "# reference (analytical) emulators:\n",
    "# dt=0.05  : exp_id=34 for minimal, exp_id=35 for bilinear net\n",
    "# dt=0.0125: exp_id=37 for minimal, exp_id=36 for bilinear net\n",
    "# full domain training: \n",
    "# dt=0.05  : exp_id=26 for minimal, exp_id=27 for bilinear net\n",
    "# dt=0.0125: exp_id=28 for minimal, exp_id=29 for bilinear net\n",
    "# local training:\n",
    "# K_local = 10, batch-size = 32\n",
    "# dt=0.05  : exp_id=30 for minimal, exp_id=31 for bilinear net\n",
    "# dt=0.0125: exp_id=32 for minimal, exp_id=33 for bilinear net\n",
    "# K_local = 1, batch-size = 32\n",
    "# dt=0.05  : exp_id=38 for minimal, exp_id=39 for bilinear net\n",
    "# K_local = 1, batch-size = 1\n",
    "# dt=0.05  : exp_id=40 for minimal, exp_id=41 for bilinear net\n",
    "\n",
    "# for one-level L96, K=36, F=10\n",
    "\n",
    "# full domain training: \n",
    "# dt=0.01  : exp_id=42 for minimal, exp_id=43 for bilinear net\n",
    "\n",
    "\n",
    "\n",
    "exp_ids = [34, 26, 30, 38, 40, 35, 27, 31, 39, 41]\n",
    "exp_id_model_sorted = [np.arange(0,5), np.arange(5,10)]\n",
    "\n",
    "all_lgnd = []\n",
    "all_models, all_model_forwarders, all_training_outputs = [], [], []\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "for exp_id in exp_ids:\n",
    "\n",
    "    exp_names = os.listdir('experiments/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "    args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "    args['model_forwarder'] = 'rk4_default'\n",
    "\n",
    "    K,J = args['K'], args['J']\n",
    "    assert args['dt_net'] == args['dt']\n",
    "\n",
    "    if J > 0:\n",
    "        F, h, b, c = 10., 1., 10., 10.\n",
    "    else:\n",
    "        h, b, c = 1., 10., 10.\n",
    "        F = 10. if K==36 else 8.\n",
    "\n",
    "    #exp_str = 'blnNet' if args['model_name']=='BilinearConvNetL96' else 'sqrNet'\n",
    "    if args['init_net']=='analytical':\n",
    "        exp_str = 'analytic'\n",
    "    else:\n",
    "        exp_str = 'local'+str(args['K_local']) if args['loss_fun']=='local_mse' else 'fullDomain'\n",
    "        exp_str += '_bs'+str(args['batch_size'])\n",
    "        \n",
    "    all_lgnd.append(exp_str)\n",
    "\n",
    "    if args['padding_mode'] == 'valid':\n",
    "        print('switching from local training to global evaluation')\n",
    "        args['padding_mode'] = 'circular'\n",
    "    model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "    all_models.append(model)\n",
    "    all_model_forwarders.append(model_forwarder)\n",
    "    all_training_outputs.append(training_outputs)\n",
    "\n",
    "    if not training_outputs is None:\n",
    "        seq_length = args['seq_length']\n",
    "        plt.semilogy(training_outputs['validation_loss'], label=all_lgnd[-1])\n",
    "\n",
    "all_lgnd = np.array(all_lgnd)\n",
    "plt.title('training')\n",
    "plt.ylabel('validation error')\n",
    "plt.legend()\n",
    "fig.patch.set_facecolor('xkcd:white')\n",
    "plt.show()\n",
    "\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)\n",
    "dts = {Model_forwarder_predictorCorrector : args['dt']/10,\n",
    "       Model_forwarder_rk4default : args['dt']}\n",
    "\n",
    "spin_up_time, train_frac = args['spin_up_time'], args['train_frac']\n",
    "normalize_data = bool(args['normalize_data'])\n",
    "T, N_trials, dt = args['T'], args['N_trials'], args['dt']\n",
    "\n",
    "out, _ = get_data(K=K, J=J, T=T, dt=dt, N_trials=N_trials, F=F, h=h, b=b, c=c, \n",
    "                  resimulate=True, solver=rk4_default,\n",
    "                  save_sim=False, data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_start = np.arange(int(T/dt))[::10] # grab initial states for rollout from long-running simulations\n",
    "i_trial = np.random.choice(N_trials, size=T_start.shape)\n",
    "idx_show = np.arange(0,len(T_start)-1, len(T_start)//3)\n",
    "\n",
    "RMSEs = np.zeros((len(exp_ids), len(T_start)))\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on differential equation (tendencies) !')\n",
    "print('\\n')\n",
    "\n",
    "for m_i, model in enumerate(all_models):\n",
    "    for i in range(len(T_start)): # diff.eq. implementaion in numpy cannot necessarily handle parallel solving\n",
    "        inputs = out[i_trial[i], T_start[i]] if N_trials > 1 else out[T_start[i]]\n",
    "        inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "\n",
    "        out_np = fun(0., inputs)\n",
    "        out_model = model.forward(inputs_torch).detach().cpu().numpy()\n",
    "\n",
    "        RMSEs[m_i,i] = np.sqrt(((out_np - sortL96fromChannels(out_model))**2).mean())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(16,12))\n",
    "for i in range(len(exp_id_model_sorted)):\n",
    "    plt.subplot(2,2,1+2*i)\n",
    "    plt.semilogy(np.sort(RMSEs[exp_id_model_sorted[i]],axis=1).T)\n",
    "    plt.title('comparison of MSEs (sorted), learned and analyical')\n",
    "    plt.legend(all_lgnd[exp_id_model_sorted[i]])\n",
    "    plt.subplot(2,2,2+2*i)\n",
    "    plt.boxplot(RMSEs[exp_id_model_sorted[i]].T, labels=all_lgnd[exp_id_model_sorted[i]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import Model_forwarder_predictorCorrector, Model_forwarder_rk4default\n",
    "import torch \n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "MFWDs = [Model_forwarder_predictorCorrector, Model_forwarder_rk4default]\n",
    "RMSEs = np.zeros((len(MFWDs), len(exp_ids), len(T_start)))\n",
    "\n",
    "class Torch_solver(torch.nn.Module):\n",
    "    # numerical solver (from numpy/numba/Julia)\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "    def forward(self, x):\n",
    "        x = sortL96fromChannels(x.detach().cpu().numpy()).flatten()\n",
    "        return sortL96intoChannels(np.atleast_2d(self.fun(0., x)), J=J)\n",
    "\n",
    "    \n",
    "for mf_i, MFWD in enumerate(MFWDs):\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "    print('\\n')\n",
    "\n",
    "    model_forwarder_np = MFWD(Torch_solver(fun), dt=dts[MFWD])\n",
    "    \n",
    "    for m_i, model in enumerate(all_models):\n",
    "        \n",
    "        model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "        \n",
    "        for i in range(len(T_start)):\n",
    "            inputs = out[i_trial[i], T_start[i]] if N_trials > 1 else out[T_start[i]]\n",
    "            inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "\n",
    "            out_np = model_forwarder_np(inputs_torch)\n",
    "            out_model = model_forwarder(inputs_torch)\n",
    "\n",
    "            RMSEs[mf_i, m_i, i] = np.sqrt(((out_np - out_model)**2).mean().detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(16,12))\n",
    "    for i in range(len(exp_id_model_sorted)):\n",
    "        plt.subplot(2,2,1+2*i)\n",
    "        plt.semilogy(np.sort(RMSEs[mf_i][exp_id_model_sorted[i]],axis=1).T)\n",
    "        plt.legend(all_lgnd[exp_id_model_sorted[i]])\n",
    "        plt.subplot(2,2,2+2*i)\n",
    "        plt.boxplot(RMSEs[mf_i][exp_id_model_sorted[i]].T, labels=all_lgnd[exp_id_model_sorted[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "\n",
    "MTU = 5 # rollout time in time units, should be rough estimate of first Lyapunov exponent\n",
    "\n",
    "DatasetClass = sel_dataset_class(prediction_task='state', N_trials=N_trials, local=False)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=1, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(T/dt*train_frac)))\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "for i in idx_show:\n",
    "    print(f'integrating for starting point {i+1} / {len(T_start)}')\n",
    "    for MFWD in [Model_forwarder_predictorCorrector, Model_forwarder_rk4default]:\n",
    "\n",
    "        print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "        T_dur = int(MTU/dts[MFWD])\n",
    "\n",
    "        model_forwarder_np = MFWD(Torch_solver(fun), \n",
    "                                  dt=dts[MFWD])\n",
    "        model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "        model_forwarder_ubo = MFWD(model=model_ubo, dt=dts[MFWD])\n",
    "\n",
    "        model_simulate = get_rollout_fun(dg_train, model_forwarder, prediction_task)\n",
    "        ubo_simulate = get_rollout_fun(dg_train, model_forwarder_ubo, prediction_task)\n",
    "        np_simulate = get_rollout_fun(dg_train, model_forwarder_np, prediction_task)\n",
    "\n",
    "        out_np = np_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                             dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                             n_steps=T_dur)\n",
    "        out_np = sortL96fromChannels(out_np * dg_train.std + dg_train.mean)\n",
    "        out_model = model_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                                   dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur)\n",
    "        out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "        out_ubo = ubo_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                                   dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur)\n",
    "        out_ubo = sortL96fromChannels(out_ubo * dg_train.std + dg_train.mean)\n",
    "\n",
    "        fig = plot_rollout(out_np, out_model, out_comparison=out_ubo, n_start=0, n_steps=T_dur, K=K)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.legend(['trained model', '(only slow vars)', 'upper-bound model', '(only slow vars)'])\n",
    "        plt.suptitle('integration scheme: ' + str(MFWD))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1 # pick one starting point for a long simulation\n",
    "n_rep = 100\n",
    "T_dur = 10 if J>0 else 100\n",
    "\n",
    "\n",
    "bins_K = np.linspace(out[0,int(spin_up_time/dt):, :K].min(),\n",
    "                     out[0,int(spin_up_time/dt):, :K].max(),\n",
    "                     50)\n",
    "\n",
    "if J > 0:\n",
    "    bins_J = np.linspace(out[0,int(spin_up_time/dt):, K:].min(),\n",
    "                         out[0,int(spin_up_time/dt):, K:].max(),\n",
    "                         50)\n",
    "else: \n",
    "    bins_J = np.array([])\n",
    "\n",
    "def calc_state_pdf(out, T_start=0, T_end=-1, bins_K=None, bins_J=None, n_bins= 100):\n",
    "    # assuming out.shape = (T, K*(J+1))\n",
    "    out_K = out[T_start:T_end][:K]\n",
    "    bins_K = np.linspace(out_K.min(), out_K.max(), n_bins) if bins_K is None else bins_K\n",
    "    pdf_K, bins_K = np.histogram(out_K, bins=bins_K, density=True)\n",
    "\n",
    "    if J > 0:\n",
    "        out_J = out[T_start:T_end][K:]\n",
    "        bins_J = np.linspace(out_J.min(), out_J.max(), n_bins) if bins_J is None else bins_J\n",
    "        pdf_J, bins_J = np.histogram(out_J, bins=bins_J, density=True)\n",
    "    else: \n",
    "        pdf_J, bins_J = None, np.array([])\n",
    "    return pdf_K, pdf_J, bins_K, bins_J\n",
    "\n",
    "def iter_solve_and_stats(out, simulate_fun, T_dur, n_rep, bins_K, bins_J):\n",
    "\n",
    "    pdf_Ks, pdf_Js = [], []\n",
    "    for n in range(n_rep):\n",
    "        print(f'- {n+1} / {n_rep}')\n",
    "        out = simulate_fun(y0=out[-1:].copy(),\n",
    "                          dy0=None,\n",
    "                          n_steps=T_dur)\n",
    "        out = out * dg_train.std + dg_train.mean\n",
    "        assert not np.any(np.isnan(out))        \n",
    "        pdf_K_n, pdf_J_n, _, _ = calc_state_pdf(sortL96fromChannels(out), bins_K=bins_K, bins_J=bins_J)\n",
    "        pdf_Ks.append(pdf_K_n)\n",
    "        if J > 0:\n",
    "            pdf_Js.append(pdf_J_n)\n",
    "    return out, pdf_Ks, pdf_Js\n",
    "            \n",
    "print('simulating from simulator')\n",
    "out_np, pdf_K_np, pdf_J_np = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  np_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "print('simulating from emulator')\n",
    "out_model, pdf_K_model, pdf_J_model = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  model_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "print('simulating from reference emulator')\n",
    "out_ubo, pdf_K_ubo, pdf_J_ubo = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  ubo_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "# quick visual inspection\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(sortL96fromChannels(out_np).T, aspect='auto')\n",
    "plt.title('simulator')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sortL96fromChannels(out_model).T, aspect='auto')\n",
    "plt.title('learned emulator')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sortL96fromChannels(out_ubo).T, aspect='auto')\n",
    "plt.title('upper-bound emulator')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xx = np.repeat(bins_K[:-1],2)\n",
    "xx[::2] -= np.diff(bins_K).mean()\n",
    "\n",
    "Q = 4 # divide long simulation into 4 quarters, track distributions invididually\n",
    "o = n_rep//Q\n",
    "\n",
    "if J > 0:\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.subplot(1,2,1)\n",
    "else:\n",
    "    plt.figure(figsize=(12,12))\n",
    "\n",
    "\n",
    "for q in range(Q):\n",
    "    if J > 0:\n",
    "        plt.subplot(2,4,2*q+1)\n",
    "    else:\n",
    "        plt.subplot(2,2,q+1)\n",
    "    # histogram of all data is average of (normalized!) histograms\n",
    "    pdf_K_np_q = np.stack(pdf_K_np[q*o:(q+1)*o]).mean(axis=0)\n",
    "    pdf_K_model_q = np.stack(pdf_K_model[q*o:(q+1)*o]).mean(axis=0)\n",
    "    pdf_K_ubo_q = np.stack(pdf_K_ubo[q*o:(q+1)*o]).mean(axis=0)\n",
    "\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_np_q, 2), color='g', label='simulator')\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_ubo_q, 2), color='k', label='upper-bound model')\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_model_q, 2), color='b', label='model')\n",
    "    if J > 0:\n",
    "        plt.xlabel('value of slow variables')\n",
    "    else:\n",
    "        plt.xlabel('state value')        \n",
    "    plt.ylabel('relative frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    if J > 0:\n",
    "        plt.subplot(2,4,2*(q+1))\n",
    "        xx = np.repeat(bins_J[:-1],2)\n",
    "        xx[::2] -= np.diff(bins_J).mean()\n",
    "        pdf_J_np_q = np.stack(pdf_J_np[q*o:(q+1)*o]).mean(axis=0)\n",
    "        pdf_J_model_q = np.stack(pdf_J_model[q*o:(q+1)*o]).mean(axis=0)\n",
    "        pdf_J_ubo_q = np.stack(pdf_J_ubo[q*o:(q+1)*o]).mean(axis=0)\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_np_q, 2), color='g')\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_ubo_q, 2), color='k')\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_model_q, 2), color='b')\n",
    "        plt.xlabel('value of fast variables')\n",
    "        plt.ylabel('relative frequency')\n",
    "    plt.title(f'Quarter {q+1} / {Q}')\n",
    "    plt.suptitle('distribution of state values for long simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D-Var results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "from L96_emulator.run_DA import setup_4DVar\n",
    "from L96_emulator.likelihood import ObsOp_identity, ObsOp_subsampleGaussian, ObsOp_rotsampleGaussian\n",
    "from L96_emulator.data_assimilation import GenModel, get_model, as_tensor\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "import torch\n",
    "\n",
    "clrs, lgnd = ['w', 'b', 'c', 'g', 'y', 'r', 'm', 'k'], []\n",
    "\n",
    "def get_analysis_rmses_4DVar_exp(exp_ids, ifplot=False):\n",
    "\n",
    "    rmses_total = np.zeros(len(exp_ids))\n",
    "    win_lens = np.zeros(len(exp_ids))\n",
    "\n",
    "    if ifplot:\n",
    "        plt.figure(figsize=(16,6))\n",
    "        plt.subplot(1,3,1)\n",
    "        for clr in clrs:\n",
    "            plt.plot(-100, -1, 'o-', color=clr, linewidth=2.5)    \n",
    "\n",
    "    for eid, exp_id in enumerate(exp_ids):\n",
    "\n",
    "        exp_names = os.listdir('experiments_DA/')   \n",
    "        conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "        args = setup_4DVar(conf_exp=f'experiments_DA/{conf_exp}.yml')\n",
    "        args.pop('conf_exp')\n",
    "\n",
    "        save_dir = 'results/data_assimilation/' + args['exp_id'] + '/'\n",
    "        fn = save_dir + 'out.npy'\n",
    "\n",
    "        out = np.load(res_dir + fn, allow_pickle=True)[()]\n",
    "\n",
    "        J = args['J']\n",
    "        n_steps = args['n_steps']\n",
    "        T_win = args['T_win'] \n",
    "        T_shift = args['T_shift'] if args['T_shift'] >= 0 else T_win\n",
    "        dt = args['dt']\n",
    "\n",
    "        data = out['out']\n",
    "        y, m = out['y'], out['m']\n",
    "        x_sols = out['x_sols']\n",
    "        losses, times = out['losses'], out['times']\n",
    "\n",
    "        assert T_win == out['T_win']\n",
    "\n",
    "        mses = np.zeros(((data.shape[0] - T_win) // T_shift + 1, data.shape[1]))\n",
    "        for i in range(len(mses)):\n",
    "            mse = np.nanmean((x_sols[i:i+1] - data)**2, axis=(-2, -1))\n",
    "            mses[i] = mse[i *T_shift]\n",
    "\n",
    "        if ifplot:\n",
    "\n",
    "            xx = np.arange(0, data.shape[0] - T_win, T_shift)\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot(xx, mses, 'o-', color=clrs[eid], linewidth=2.5)\n",
    "            plt.xlim(0, len(data))\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.plot(xx, np.nanmean(mses, axis=1), 'o-', color=clrs[eid], linewidth=2.5)\n",
    "            plt.xlim(0, len(data))\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.plot(xx, mses, 'o-', color=clrs[eid], linewidth=2.5)\n",
    "            plt.axis([0, len(data)-1, 0, 2])\n",
    "            print(np.nanmean(mses[1:]))\n",
    "            lgnd.append('window length='+str(T_win))\n",
    "\n",
    "        rmses_total[eid] = np.sqrt(np.nanmean(mses))\n",
    "        win_lens[eid] = T_win\n",
    "\n",
    "    if ifplot: \n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title('individial trials')\n",
    "        plt.ylabel('initial state MSE')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title('averages over trials')\n",
    "        plt.legend(lgnd[:3])\n",
    "        plt.xlabel('time t')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.title('inidividual trials, zoom-in on small MSEs')\n",
    "        plt.show()\n",
    "        \n",
    "    return win_lens, rmses_total\n",
    "\n",
    "\n",
    "def get_pred_rmses_4DVar_exp(exp_id):\n",
    "\n",
    "    exp_names = os.listdir('experiments_DA/')   \n",
    "    conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "    args = setup_4DVar(conf_exp=f'experiments_DA/{conf_exp}.yml')\n",
    "    args.pop('conf_exp')\n",
    "\n",
    "    assert args['T_win'] == 64 # we want 4d integration window here\n",
    "\n",
    "    K,J = args['K'], args['J']\n",
    "    T_win = args['T_win']\n",
    "\n",
    "    model_pars = {\n",
    "        'exp_id' : args['model_exp_id'],\n",
    "        'model_forwarder' : 'rk4_default',\n",
    "        'K_net' : args['K'],\n",
    "        'J_net' : args['J'],\n",
    "        'dt_net' : args['dt']\n",
    "    }\n",
    "\n",
    "    model, model_forwarder, _ = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "\n",
    "    obs_operator = args['obs_operator']\n",
    "    obs_pars = {}\n",
    "    if obs_operator=='ObsOp_subsampleGaussian':\n",
    "        obs_pars['obs_operator'] = ObsOp_subsampleGaussian\n",
    "        obs_pars['obs_operator_args'] = {'r' : args['obs_operator_r'], 'sigma2' : args['obs_operator_sig2']}\n",
    "    elif obs_operator=='ObsOp_identity':\n",
    "        obs_pars['obs_operator'] = ObsOp_identity\n",
    "        obs_pars['obs_operator_args'] = {}\n",
    "    elif obs_operator=='ObsOp_rotsampleGaussian':\n",
    "        obs_pars['obs_operator'] = ObsOp_rotsampleGaussian\n",
    "        obs_pars['obs_operator_args'] = {'frq' : args['obs_operator_frq'], \n",
    "                                         'sigma2' : args['obs_operator_sig2']}\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    model_observer = obs_pars['obs_operator'](**obs_pars['obs_operator_args'])\n",
    "\n",
    "    prior = torch.distributions.normal.Normal(loc=torch.zeros((1,J+1,K)), \n",
    "                                              scale=1.*torch.ones((1,J+1,K)))\n",
    "\n",
    "    # ### define generative model for observed data\n",
    "    gen = GenModel(model_forwarder, model_observer, prior, T=T_win, x_init=None)\n",
    "\n",
    "    forecast_win = int(120/1.5) # 5d forecast\n",
    "    eval_every = int(6/1.5) # every 6h\n",
    "\n",
    "\n",
    "    save_dir = 'results/data_assimilation/' + args['exp_id'] + '/'\n",
    "    fn = save_dir + 'out.npy'\n",
    "\n",
    "    out = np.load(res_dir + fn, allow_pickle=True)[()]\n",
    "\n",
    "    J = args['J']\n",
    "    n_steps = args['n_steps']\n",
    "    T_win = args['T_win'] \n",
    "    T_shift = args['T_shift'] if args['T_shift'] >= 0 else T_win\n",
    "    dt = args['dt']\n",
    "\n",
    "    data = out['out']\n",
    "    y, m = out['y'], out['m']\n",
    "    x_sols = out['x_sols']\n",
    "    losses, times = out['losses'], out['times']\n",
    "\n",
    "    assert T_win == out['T_win']\n",
    "\n",
    "    mses = np.zeros(((data.shape[0] - forecast_win - T_win) // T_shift + 1, forecast_win//eval_every+1, y.shape[1]))\n",
    "    for i in range(len(mses)):\n",
    "        forecasts = gen._forward(x=as_tensor(x_sols[i]), T_obs=T_win + np.arange(0,forecast_win+1,eval_every))\n",
    "        n = i * T_shift + T_win\n",
    "        for j in range(mses.shape[1]): # loop over integration windows\n",
    "            forecast = forecasts[j].detach().cpu().numpy()\n",
    "            y_obs = data[n+j*eval_every] # sortL96intoChannels(y[n+j*eval_every],J=J)\n",
    "            mses[i,j] = np.nanmean((forecast - y_obs)**2, axis=(-2, -1))\n",
    "\n",
    "    pred_lens = 1.5/24 * np.arange(0, forecast_win+1, eval_every)\n",
    "\n",
    "\n",
    "    return pred_lens, np.sqrt(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ids_minimalNet = ['14', '15', '16', '17', '18', '19', '20', '21']\n",
    "exp_ids_bilinNet = ['22', '23', '24', '25', '26', '27', '28', '29']\n",
    "exp_ids_analyticNet = ['30', '31', '32', '33', '34', '35', '36', '37']\n",
    "\n",
    "win_lens_minimalNet, rmses_analysis_minimalNet = get_analysis_rmses_4DVar_exp(exp_ids=exp_ids_minimalNet)\n",
    "win_lens_bilinNet, rmses_analysis_bilinNet = get_analysis_rmses_4DVar_exp(exp_ids=exp_ids_bilinNet)\n",
    "win_lens_analyticNet, rmses_analysis_analyticNet = get_analysis_rmses_4DVar_exp(exp_ids=exp_ids_analyticNet)\n",
    "\n",
    "pred_lens_minimalNet, rmses_pred__minimalNet = get_pred_rmses_4DVar_exp(exp_id='21')\n",
    "pred_lens_bilinNet, rmses_pred__bilinNet = get_pred_rmses_4DVar_exp(exp_id='29')\n",
    "pred_lens_analyticNet, rmses_pred_analyticNet = get_pred_rmses_4DVar_exp(exp_id='37')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=14\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(win_lens_analyticNet*1.5/24, rmses_analysis_analyticNet, \n",
    "         ':', color='k', linewidth=2.5, label='analyticNet')\n",
    "plt.plot(win_lens_minimalNet*1.5/24, rmses_analysis_minimalNet, \n",
    "         '-', color='k', linewidth=2.5, label='minimalNet')\n",
    "plt.plot(win_lens_bilinNet*1.5/24, rmses_analysis_bilinNet, \n",
    "         '--', color='k', linewidth=2.5, label='bilinNet')\n",
    "plt.xlabel('integration window length [d]', fontsize=fontsize)\n",
    "plt.ylabel('RMSE', fontsize=fontsize)\n",
    "plt.yticks([0.4, 0.5, 0.6, 0.7], fontsize=fontsize)\n",
    "plt.xticks(0.5*np.arange(1, 8.1), fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(pred_lens_analyticNet,np.nanmean(rmses_pred_analyticNet,axis=(0,-1)), \n",
    "         ':', color='k', linewidth=2.5, label='analyticNet')\n",
    "plt.plot(pred_lens_minimalNet,np.nanmean(rmses_pred__minimalNet,axis=(0,-1)), \n",
    "         '-', color='k', linewidth=2.5, label='minimalNet')\n",
    "plt.plot(pred_lens_bilinNet,np.nanmean(rmses_pred__bilinNet,axis=(0,-1)), \n",
    "         '--', color='k', linewidth=2.5, label='bilinNet')\n",
    "plt.xlabel('forecast time [d]', fontsize=fontsize)\n",
    "plt.ylabel('RMSE', fontsize=fontsize)\n",
    "plt.yticks([0.5, 1.0, 1.5], fontsize=fontsize)\n",
    "plt.xticks(0.5*np.arange(10.1), fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize)\n",
    "plt.savefig(res_dir + 'figs/4DVar.pdf', bbox_inches='tight', pad_inches=0, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "from L96_emulator.parametrization import Parametrized_twoLevel_L96, Parametrization_lin\n",
    "from L96_emulator.networks import Model_forwarder_rk4default\n",
    "from L96_emulator.run_parametrization import setup_parametrization\n",
    "from L96_emulator.data_assimilation import get_model\n",
    "from L96_emulator.util import as_tensor, dtype_np, sortL96intoChannels, sortL96fromChannels\n",
    "from L96sim.L96_base import f1, f2, pf2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "exp_id = '01'\n",
    "\n",
    "exp_names = os.listdir('experiments_parametrization/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "args = setup_parametrization(conf_exp=f'experiments_parametrization/{conf_exp}.yml')\n",
    "args.pop('conf_exp')\n",
    "\n",
    "save_dir = 'results/parametrization/' + args['exp_id'] + '/'\n",
    "out = np.load(res_dir + save_dir + 'out.npy', allow_pickle=True)[()]\n",
    "\n",
    "T_dur = 5000\n",
    "X_init = out['X_init']\n",
    "\n",
    "model_pars = {\n",
    "    'exp_id' : args['model_exp_id'],\n",
    "    'model_forwarder' : args['model_forwarder'],\n",
    "    'K_net' : args['K'],\n",
    "    'J_net' : 0,\n",
    "    'dt_net' : args['dt']\n",
    "}\n",
    "# trained parametrized model\n",
    "model, model_forwarder, _ = get_model(model_pars, res_dir=res_dir, exp_dir='')\n",
    "param_train = Parametrization_lin(a=as_tensor(out['param_train_state_dict']['a']), \n",
    "                                  b=as_tensor(out['param_train_state_dict']['b']))\n",
    "model_parametrized_train = Parametrized_twoLevel_L96(emulator=model, parametrization=param_train)\n",
    "model_forwarder_parametrized_train = Model_forwarder_rk4default(model=model_parametrized_train, dt=args['dt'])\n",
    "\n",
    "# initial and reference parametrized models\n",
    "param_ref = Parametrization_lin(a=as_tensor(np.array([-0.31])), b=as_tensor(np.array([-0.2])))\n",
    "param_init = Parametrization_lin(a=as_tensor(np.array([-0.75])), b=as_tensor(np.array([-0.4])))\n",
    "model_parametrized_init = Parametrized_twoLevel_L96(emulator=model, parametrization=param_init)\n",
    "model_forwarder_parametrized_init = Model_forwarder_rk4default(model=model_parametrized_init, dt=args['dt'])\n",
    "model_parametrized_ref = Parametrized_twoLevel_L96(emulator=model, parametrization=param_ref)\n",
    "model_forwarder_parametrized_ref = Model_forwarder_rk4default(model=model_parametrized_ref, dt=args['dt'])\n",
    "\n",
    "# ground-truth high-res model\n",
    "dX_dt = np.empty(args['K']*(args['J']+1), dtype=dtype_np)\n",
    "def fun(t, x):\n",
    "    return f2(x, args['l96_F'], args['l96_h'], args['l96_b'], args['l96_c'], dX_dt, args['K'], args['J'])\n",
    "class Torch_solver(torch.nn.Module):\n",
    "    # numerical solver (from numpy/numba/Julia)\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "    def forward(self, x):\n",
    "        x = sortL96fromChannels(x.detach().cpu().numpy()).flatten()\n",
    "        return sortL96intoChannels(np.atleast_2d(self.fun(0., x)), J=args['J'])\n",
    "model_forwarder_np = Model_forwarder_rk4default(Torch_solver(fun), dt=args['dt'])\n",
    "\n",
    "model_forwarders = [Model_forwarder_rk4default(model, dt=args['dt']),\n",
    "                    model_forwarder_parametrized_init, \n",
    "                    model_forwarder_parametrized_train,\n",
    "                    model_forwarder_parametrized_ref,\n",
    "                    model_forwarder_np]\n",
    "X_inits = [X_init[:,:args['K']].copy(), \n",
    "           X_init[:,:args['K']].copy(), \n",
    "           X_init[:,:args['K']].copy(), \n",
    "           X_init[:,:args['K']].copy(), \n",
    "           X_init.copy()]\n",
    "Js = [0, 0, 0, 0, args['J']]\n",
    "panel_titles=['one-level L96', \n",
    "              'initial param.', \n",
    "              'learned param.', \n",
    "              'reference param.', \n",
    "              'two-level L96']\n",
    "sols = [np.nan for n in range(len(model_forwarders))]\n",
    "for i_model in range(len(model_forwarders)): \n",
    "    \n",
    "    model_forwarder_i, X_init_i, J_i = model_forwarders[i_model], X_inits[i_model], Js[i_model]\n",
    "\n",
    "    def model_simulate(y0, dy0, n_steps):\n",
    "        x = np.empty((n_steps+1, *y0.shape[1:]))\n",
    "        x[0] = y0.copy()\n",
    "        xx = as_tensor(x[0]).reshape(1,1,-1)\n",
    "        for i in range(1,n_steps+1):\n",
    "            xx = model_forwarder_i(xx.reshape(1,J_i+1,-1))\n",
    "            x[i] = xx.detach().cpu().numpy().copy()\n",
    "        return x\n",
    "\n",
    "    print('forwarding model ' + panel_titles[i_model])\n",
    "    sols[i_model] = model_simulate(y0=sortL96intoChannels(X_init_i,J=J_i), dy0=None, n_steps=T_dur)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=14\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "rmses = np.zeros((len(model_forwarders), T_dur+1))\n",
    "for i_model in range(len(model_forwarders)): \n",
    "    \n",
    "    plt.subplot(2,len(model_forwarders),i_model+1)\n",
    "    plt.imshow(sortL96fromChannels(sols[i_model][:,:1,:]).T, aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(panel_titles[i_model], fontsize=fontsize)\n",
    "    \n",
    "    if i_model == 0:\n",
    "        plt.ylabel('location k', fontsize=fontsize)\n",
    "    if i_model == 2:\n",
    "        plt.xlabel('time [steps]', fontsize=fontsize)\n",
    "        \n",
    "    rmses[i_model,:] = np.sqrt(np.mean((sols[i_model][:,0,:] - sols[-1][:,0,:])**2, axis=1))\n",
    "    plt.yticks([], fontsize=fontsize)\n",
    "    plt.xticks([0, T_dur/2, T_dur], fontsize=fontsize)\n",
    "    \n",
    "plt.subplot(2,2,3)\n",
    "plt.text(0.5, 0.5, 'tbd')\n",
    "plt.ylabel('parametrization parameters', fontsize=fontsize)\n",
    "plt.xlabel('dataset size', fontsize=fontsize)\n",
    "plt.xticks([], fontsize=fontsize)\n",
    "plt.yticks([], fontsize=fontsize)\n",
    "\n",
    "    \n",
    "plt.subplot(2,2,4)\n",
    "for i_model in range(len(model_forwarders)-1):\n",
    "    plt.plot(rmses[i_model], label=panel_titles[i_model])\n",
    "plt.legend(fontsize=fontsize, frameon=False)\n",
    "plt.ylabel('RMSE', fontsize=fontsize)\n",
    "plt.xlabel('time [steps]', fontsize=fontsize)\n",
    "plt.xticks([0, T_dur/2, T_dur], fontsize=fontsize)\n",
    "plt.yticks([0, 3, 6], fontsize=fontsize)\n",
    "plt.savefig(res_dir + 'figs/param.pdf', bbox_inches='tight', pad_inches=0, frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
