{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with implicit solvers\n",
    "\n",
    "- mostly Forward- vs Backwards-Euler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device, as_tensor\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick a (trained) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "# experiments to use: \n",
    "# dt=0.05  : exp_id = 26 for minimal, exp_id=27 for bilinear net\n",
    "# dt=0.0125: exp_id = 28 for minimal, exp_id=29 for bilinear net\n",
    "\n",
    "exp_id = 92\n",
    "exp_id = 70 # best Bilinear Network\n",
    "\n",
    "exp_names = os.listdir('experiments/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')\n",
    "\n",
    "\n",
    "K,J = args['K'], args['J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args['dt_net'] == args['dt']\n",
    "if J > 0:\n",
    "    F, h, b, c = 10., 1., 10., 10.\n",
    "else:\n",
    "    F, h, b, c = 8., 1., 10., 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and instantiate the emulator model and an 'optimal' comparison available for L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "from L96_emulator.networks import named_network\n",
    "\n",
    "\n",
    "args['model_forwarder'] = 'rk4_default'\n",
    "if args['padding_mode'] == 'valid':\n",
    "    print('switching from local training to global evaluation')\n",
    "    args['padding_mode'] = 'circular'\n",
    "model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "if args['model_name'] in ['MinimalConvNetL96','BilinearConvNetL96']:\n",
    "    model_ubo = args['model_name']\n",
    "else:\n",
    "    model_ubo = 'MinimalConvNetL96'\n",
    "# upper bound: model re-implementation of L96 in torch (conv1d->pointwise_square->conv1d)\n",
    "model_ubo, model_forwarder_ubo =named_network(\n",
    "        model_name=model_ubo,\n",
    "        n_input_channels=J+1,\n",
    "        n_output_channels=J+1,\n",
    "        seq_length=1,\n",
    "        **{'filters': [0],\n",
    "           'kernel_sizes': [4],\n",
    "           'init_net': 'analytical',\n",
    "           'K_net': K,\n",
    "           'J_net': J,\n",
    "           'dt_net': args['dt'],\n",
    "           'F_net' : F, \n",
    "           'h_net' : h, \n",
    "           'b_net' : b, \n",
    "           'c_net' : c, \n",
    "           'model_forwarder': 'rk4_default',\n",
    "           'padding_mode' : 'circular'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import Model_forwarder_predictorCorrector, Model_forwarder_rk4default, Model_forwarder_forwardEuler\n",
    "\n",
    "dts = {Model_forwarder_predictorCorrector : args['dt']/10,\n",
    "       Model_forwarder_forwardEuler : args['dt'],\n",
    "       Model_forwarder_rk4default : args['dt']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load some data to get sensible test system state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class\n",
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data\n",
    "\n",
    "spin_up_time, train_frac = args['spin_up_time'], args['train_frac']\n",
    "normalize_data = bool(args['normalize_data'])\n",
    "T, N_trials, dt = args['T'], args['N_trials'], args['dt']\n",
    "\n",
    "out, _ = get_data(K=K, J=J, T=T, dt=dt, N_trials=N_trials, F=F, h=h, b=b, c=c, \n",
    "                  resimulate=True, solver=rk4_default,\n",
    "                  save_sim=False, data_dir=data_dir)\n",
    "out = out.reshape(1, *out.shape) if len(out.shape)==2 else out\n",
    "\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task, N_trials=N_trials, local=False)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(T/dt*train_frac)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up ground-truth simulator code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, pf2\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implicit solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwards_euler(model, x, x_next, dt):\n",
    "\n",
    "    return x + dt * model.forward(x_next)\n",
    "\n",
    "def loss_fun(model, x, x_next, dt):\n",
    "    \n",
    "    return torch.mean( (x_next - backwards_euler(model, x, x_next, dt))**2 )\n",
    "\n",
    "def implicit_forwarder(x_next, f_loss, n_steps=1):\n",
    "    \n",
    "    loss_vals = np.zeros(n_steps)\n",
    "    optimizer = torch.optim.LBFGS(params=[x_next],\n",
    "                                  lr=0.1,\n",
    "                                  max_iter=1000,\n",
    "                                  max_eval=None,\n",
    "                                  tolerance_grad=1e-12,\n",
    "                                  tolerance_change=1e-15,\n",
    "                                  history_size=100,\n",
    "                                  line_search_fn='strong_wolfe')\n",
    "\n",
    "    for i_n in range(n_steps):\n",
    "\n",
    "        def closure():\n",
    "            loss = f_loss(x_next)\n",
    "            if torch.is_grad_enabled():\n",
    "                optimizer.zero_grad()\n",
    "            if loss.requires_grad:\n",
    "                loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        with torch.no_grad():\n",
    "            loss = f_loss(x_next)        \n",
    "        loss_vals[i_n] = loss.detach().cpu().numpy()\n",
    "        #time_vals[i_n] = time.time() - time_vals[i_+i_n,n]\n",
    "        \n",
    "    print('final loss:',  loss_vals[-1])\n",
    "\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error on resolvent (1-step integration error) for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_start = np.arange(int(spin_up_time/dt), int(spin_up_time/dt)+10000, 100)\n",
    "i_trial = np.random.choice(N_trials, size=T_start.shape)\n",
    "\n",
    "idx_show = np.arange(0,len(T_start)-1, len(T_start)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "class Torch_solver(torch.nn.Module):\n",
    "    # numerical solver (from numpy/numba/Julia)\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "    def forward(self, x):\n",
    "        x = sortL96fromChannels(x.detach().cpu().numpy()).flatten()\n",
    "        return as_tensor(sortL96intoChannels(np.atleast_2d(self.fun(0., x)), J=J))\n",
    "\n",
    "#for MFWD in [Model_forwarder_predictorCorrector, Model_forwarder_rk4default]:\n",
    "for MFWD in [Model_forwarder_forwardEuler]:\n",
    "    model_forwarder_np = Model_forwarder_rk4default(Torch_solver(fun), dt=dts[MFWD])\n",
    "    model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "    model_forwarder_ubo = Model_forwarder_rk4default(model=model, dt=dts[MFWD])\n",
    "\n",
    "    MSEs = np.zeros(len(T_start))\n",
    "    MSEs_ubo = np.zeros(len(T_start))\n",
    "    MSEs_imp = np.zeros(len(T_start))\n",
    "    for i in range(len(T_start)):\n",
    "        inputs = out[i_trial[i], T_start[i]]\n",
    "        inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "\n",
    "        out_np = model_forwarder_np(inputs_torch)\n",
    "        out_model = model_forwarder(inputs_torch)\n",
    "        out_ubo = model_forwarder_ubo(inputs_torch)\n",
    "        \n",
    "        def f_loss(x_next):\n",
    "            return loss_fun(model=model, x=inputs_torch, x_next=x_next, dt=dts[MFWD])\n",
    "        out_imp = implicit_forwarder(x_next= torch.nn.Parameter(1.*inputs_torch), f_loss=f_loss, n_steps=1)\n",
    "        #out_imp = out_imp.detach().cpu().numpy()\n",
    "\n",
    "        MSEs[i] = ((out_np - out_model)**2).mean().detach().cpu().numpy()\n",
    "        MSEs_imp[i] = ((out_np - sortL96fromChannels(out_imp))**2).mean().detach().cpu().numpy()\n",
    "        MSEs_ubo[i] = ((out_np - out_ubo)**2).mean().detach().cpu().numpy()\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "    print('\\n')\n",
    "\n",
    "    print('MSEs                  ', MSEs[idx_show])\n",
    "    print('MSEs - implicit method', MSEs_imp[idx_show])\n",
    "    print('MSEs - upper bound    ', MSEs_ubo[idx_show])\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.sort(MSEs), label='learned')\n",
    "    plt.plot(np.sort(MSEs_imp), label='learned - implicit')\n",
    "    plt.plot(np.sort(MSEs_ubo), label='analytic')\n",
    "    plt.title('comparison of MSEs (sorted), learned and analyical')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-step integration error (rollout error) for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "MTU = 5 # rollout time in time units, should be rough estimate of first Lyapunov exponent\n",
    "\n",
    "for i in idx_show[-1:]:\n",
    "    print(f'integrating for starting point {i+1} / {len(T_start)}')\n",
    "    for MFWD in [Model_forwarder_forwardEuler]:\n",
    "\n",
    "        print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "        T_dur = int(MTU/dts[MFWD])\n",
    "\n",
    "        model_forwarder_np = Model_forwarder_rk4default(Torch_solver(fun), \n",
    "                                  dt=dts[MFWD])\n",
    "        model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "        model_forwarder_ubo = Model_forwarder_rk4default(model=Torch_solver(fun), dt=dts[MFWD]/10.)  # 1/10-th step size\n",
    "        \n",
    "        class Model_forwarder_implicit(torch.nn.Module):\n",
    "            \n",
    "            def __init__(self, model, dt):\n",
    "                super(Model_forwarder_implicit, self).__init__()\n",
    "                self.model = model\n",
    "                self.dt = dt\n",
    "                \n",
    "            def forward(self, x):\n",
    "                def f_loss(x_next):\n",
    "                    return loss_fun(model=self.model, x=x, x_next=x_next, dt=self.dt)\n",
    "                return implicit_forwarder(x_next= torch.nn.Parameter(1.*x), f_loss=f_loss, n_steps=1)                \n",
    "        model_forwarder_imp = Model_forwarder_implicit(model=model, dt=dts[MFWD])\n",
    "        model_forwarder_imp_10 = Model_forwarder_implicit(model=model, dt=dts[MFWD]/10) # 1/10-th step size\n",
    "\n",
    "        model_simulate = get_rollout_fun(dg_train, model_forwarder_imp_10, prediction_task)\n",
    "        imp_simulate = get_rollout_fun(dg_train, model_forwarder_imp, prediction_task)\n",
    "        ubo_simulate = get_rollout_fun(dg_train, model_forwarder_ubo, prediction_task)\n",
    "        np_simulate = get_rollout_fun(dg_train, model_forwarder_np, prediction_task)\n",
    "\n",
    "        out_np = np_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                             dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                             n_steps=T_dur)\n",
    "        out_np = sortL96fromChannels(out_np * dg_train.std + dg_train.mean)\n",
    "\n",
    "        out_model = model_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                                   dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur*10)\n",
    "        out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "        out_imp = imp_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                               dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                               n_steps=T_dur)\n",
    "        out_imp = sortL96fromChannels(out_imp * dg_train.std + dg_train.mean)\n",
    "\n",
    "        out_ubo = ubo_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                               dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur*10)\n",
    "        out_ubo = sortL96fromChannels(out_ubo * dg_train.std + dg_train.mean)\n",
    "\n",
    "        fig = plot_rollout(out_np, out_imp, out_comparison=out_model, n_start=0, n_steps=T_dur, K=K)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.legend(['trained model', '(only slow vars)', 'upper-bound model', '(only slow vars)'])\n",
    "        plt.suptitle('integration scheme: ' + str(MFWD))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "\n",
    "fontsize=14\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(out_np.T, aspect='auto')\n",
    "plt.title('RK4 (true simulator)', fontsize=fontsize)\n",
    "plt.ylabel('location k', fontsize=fontsize)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(out_ubo.T, aspect='auto')\n",
    "plt.title('RK4 (true simulator), 1/10 step size', fontsize=fontsize)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(out_imp.T, aspect='auto')\n",
    "plt.title('backwards-Euler (emulator)', fontsize=fontsize)\n",
    "plt.xlabel('integration step n', fontsize=fontsize)\n",
    "plt.ylabel('location k', fontsize=fontsize)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(out_model.T, aspect='auto')\n",
    "plt.title('backwards-Euler (emulator), 1/10 step size', fontsize=fontsize)\n",
    "plt.xlabel('integration step n', fontsize=fontsize)\n",
    "plt.colorbar()\n",
    "\n",
    "#plt.savefig(res_dir + 'figs/implicit.pdf', bbox_inches='tight', pad_inches=0, frameon=False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_rollout(out_np, out_model, out_comparison=out_ubo, n_start=0, n_steps=T_dur)\n",
    "plt.subplot(1,2,2)\n",
    "plt.legend(['deepNet', 'sqrNet (analytic)'])\n",
    "plt.suptitle('integration scheme: ' + str(MFWD))\n",
    "#plt.savefig(res_dir + 'figs/deepNet_rollout.pdf', bbox_inches='tight', pad_inches=0, frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
