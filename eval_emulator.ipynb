{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Build & evaluate an Emulator\n",
    "\n",
    "Given a dynamical system $F: x_t \\rightarrow x_{t_1} := F(x_t)$, we want to learn an emulator $G \\approx F$ as a neural network from $F$-sampled trajectories.\n",
    "\n",
    "Evaluate the quality of $G$ using:\n",
    "- L2 or other error metric on 1-step updates\n",
    "- L2 or other metric on longer rollouts\n",
    "- Compare Lyapunov spectrum to original model\n",
    "- Compare PSD etc. to original model\n",
    "For each of these metrics, there are baselines one can compare to from the literature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device, as_tensor\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick a (trained) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import setup\n",
    "\n",
    "# experiments to use: \n",
    "# dt=0.05  : exp_id = 26 for minimal, exp_id=27 for bilinear net\n",
    "# dt=0.0125: exp_id = 28 for minimal, exp_id=29 for bilinear net\n",
    "\n",
    "\n",
    "exp_id = 30\n",
    "\n",
    "exp_names = os.listdir('experiments/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')\n",
    "\n",
    "K,J = args['K'], args['J']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert args['dt_net'] == args['dt']\n",
    "if J > 0:\n",
    "    F, h, b, c = 10., 1., 10., 10.\n",
    "else:\n",
    "    F, h, b, c = 8., 1., 10., 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load and instantiate the emulator model and an 'optimal' comparison available for L96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "from L96_emulator.networks import named_network\n",
    "\n",
    "\n",
    "args['model_forwarder'] = 'rk4_default'\n",
    "if args['padding_mode'] == 'valid':\n",
    "    print('switching from local training to global evaluation')\n",
    "    args['padding_mode'] = 'circular'\n",
    "model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()\n",
    "\n",
    "# upper bound: model re-implementation of L96 in torch (conv1d->pointwise_square->conv1d)\n",
    "model_ubo, model_forwarder_ubo =named_network(\n",
    "        model_name='MinimalConvNetL96',\n",
    "        n_input_channels=J+1,\n",
    "        n_output_channels=J+1,\n",
    "        seq_length=1,\n",
    "        **{'filters': [0],\n",
    "           'kernel_sizes': [4],\n",
    "           'init_net': 'analytical',\n",
    "           'K_net': K,\n",
    "           'J_net': J,\n",
    "           'dt_net': args['dt'],\n",
    "           'l96_F' : F, \n",
    "           'l96_h' : h, \n",
    "           'l96_b' : b, \n",
    "           'l96_c' : c, \n",
    "           'model_forwarder': 'rk4_default',\n",
    "           'padding_mode' : 'circular'}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.eval import sortL96fromChannels, sortL96intoChannels\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import Model_forwarder_predictorCorrector, Model_forwarder_rk4default\n",
    "\n",
    "dts = {Model_forwarder_predictorCorrector : args['dt']/10,\n",
    "       Model_forwarder_rk4default : args['dt']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load some data to get sensible test system state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class\n",
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data\n",
    "\n",
    "spin_up_time, train_frac = args['spin_up_time'], args['train_frac']\n",
    "normalize_data = bool(args['normalize_data'])\n",
    "T, N_trials, dt = args['T'], args['N_trials'], args['dt']\n",
    "\n",
    "out, _ = get_data(K=K, J=J, T=T, dt=dt, N_trials=N_trials, F=F, h=h, b=b, c=c, \n",
    "                  resimulate=True, solver=rk4_default,\n",
    "                  save_sim=False, data_dir=data_dir)\n",
    "\n",
    "\n",
    "prediction_task = 'state'\n",
    "lead_time = 1\n",
    "DatasetClass = sel_dataset_class(prediction_task=prediction_task, N_trials=N_trials, local=False)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=lead_time, normalize=normalize_data, \n",
    "                   start=int(spin_up_time/dt), \n",
    "                   end=int(np.floor(T/dt*train_frac)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up ground-truth simulator code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96sim.L96_base import f1, f2, pf2\n",
    "\n",
    "if J > 0:\n",
    "    def fun(t, x):\n",
    "        return f2(x, F, h, b, c, dX_dt, K, J)\n",
    "else:\n",
    "    def fun(t, x):\n",
    "        return f1(x, F, dX_dt, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## direct error on tendencies (rhs of diff.eq.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_start = np.arange(int(T/dt)) # grab initial states for rollout from long-running simulations\n",
    "i_trial = np.random.choice(N_trials, size=T_start.shape)\n",
    "idx_show = np.arange(0,len(T_start)-1, len(T_start)//3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs = np.zeros(len(T_start))\n",
    "MSEs_ubo = np.zeros(len(T_start))\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on differential equation (tendencies) !')\n",
    "print('\\n')\n",
    "\n",
    "for i in range(len(T_start)): # diff.eq. implementaion in numpy cannot necessarily handle parallel solving\n",
    "    inputs = out[i_trial[i], T_start[i]] if N_trials > 1 else out[T_start[i]]\n",
    "    inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "\n",
    "    out_np = fun(0., inputs)\n",
    "    out_model = model.forward(inputs_torch).detach().cpu().numpy()\n",
    "    out_ubo = model_ubo.forward(inputs_torch).detach().cpu().numpy()\n",
    "    \n",
    "    MSEs[i] = ((out_np - sortL96fromChannels(out_model))**2).mean()\n",
    "    MSEs_ubo[i] = ((out_np - sortL96fromChannels(out_ubo))**2).mean()\n",
    "\n",
    "    \n",
    "print('MSEs              ', MSEs[idx_show])\n",
    "print('MSEs - upper bound', MSEs_ubo[idx_show])\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs (* dt)              ', MSEs[idx_show]*dt**2)\n",
    "print('MSEs (* dt) - upper bound', MSEs_ubo[idx_show]*dt**2)\n",
    "\n",
    "\n",
    "plt.plot(np.sort(MSEs), label='learned')\n",
    "plt.plot(np.sort(MSEs_ubo), label='analytic')\n",
    "plt.title('comparison of MSEs (sorted), learned and analyical')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error on resolvent (1-step integration error) for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import Model_forwarder_predictorCorrector, Model_forwarder_rk4default\n",
    "import torch \n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "MSEs = np.zeros(len(T_start))\n",
    "MSEs_ubo = np.zeros(len(T_start))\n",
    "\n",
    "class Torch_solver(torch.nn.Module):\n",
    "    # numerical solver (from numpy/numba/Julia)\n",
    "    def __init__(self, fun):\n",
    "        self.fun = fun\n",
    "    def forward(self, x):\n",
    "        x = sortL96fromChannels(x.detach().cpu().numpy()).flatten()\n",
    "        return sortL96intoChannels(np.atleast_2d(self.fun(0., x)), J=J)\n",
    "\n",
    "for MFWD in [Model_forwarder_predictorCorrector, Model_forwarder_rk4default]:\n",
    "    model_forwarder_np = MFWD(Torch_solver(fun), \n",
    "                              dt=dts[MFWD])\n",
    "    model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "    model_forwarder_ubo = MFWD(model=model_ubo, dt=dts[MFWD])\n",
    "\n",
    "    MSEs = np.zeros(len(T_start))\n",
    "    for i in range(len(T_start)):\n",
    "        inputs = out[i_trial[i], T_start[i]] if N_trials > 1 else out[T_start[i]]\n",
    "        inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "\n",
    "        out_np = model_forwarder_np(inputs_torch)\n",
    "        out_model = model_forwarder(inputs_torch)\n",
    "        out_ubo = model_forwarder_ubo(inputs_torch)\n",
    "\n",
    "        MSEs[i] = ((out_np - out_model)**2).mean().detach().cpu().numpy()\n",
    "        MSEs_ubo[i] = ((out_np - out_ubo)**2).mean().detach().cpu().numpy()\n",
    "\n",
    "    print('\\n')\n",
    "    print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "    print('\\n')\n",
    "\n",
    "    print('MSEs              ', MSEs)\n",
    "    print('MSEs - upper bound', MSEs_ubo)\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(np.sort(MSEs), label='learned')\n",
    "    plt.plot(np.sort(MSEs_ubo), label='analytic')\n",
    "    plt.title('comparison of MSEs (sorted), learned and analyical')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-step integration error (rollout error) for different solvers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "\n",
    "print('\\n')\n",
    "print('MSEs are on system state !')\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "MTU = 10 # rollout time in time units, should be rough estimate of first Lyapunov exponent\n",
    "\n",
    "for i in idx_show:\n",
    "    print(f'integrating for starting point {i+1} / {len(T_start)}')\n",
    "    for MFWD in [Model_forwarder_predictorCorrector, Model_forwarder_rk4default]:\n",
    "\n",
    "        print(f'solver {MFWD}, dt = {dts[MFWD]}')\n",
    "        T_dur = int(MTU/dts[MFWD])\n",
    "\n",
    "        model_forwarder_np = MFWD(Torch_solver(fun), \n",
    "                                  dt=dts[MFWD])\n",
    "        model_forwarder = MFWD(model=model, dt=dts[MFWD])\n",
    "        model_forwarder_ubo = MFWD(model=model_ubo, dt=dts[MFWD])\n",
    "\n",
    "        model_simulate = get_rollout_fun(dg_train, model_forwarder, prediction_task)\n",
    "        ubo_simulate = get_rollout_fun(dg_train, model_forwarder_ubo, prediction_task)\n",
    "        np_simulate = get_rollout_fun(dg_train, model_forwarder_np, prediction_task)\n",
    "\n",
    "        out_np = np_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                             dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                             n_steps=T_dur)\n",
    "        out_np = sortL96fromChannels(out_np * dg_train.std + dg_train.mean)\n",
    "        out_model = model_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                                   dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur)\n",
    "        out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n",
    "\n",
    "        out_ubo = ubo_simulate(y0=dg_train[T_start[i]].copy(), \n",
    "                                   dy0=dg_train[T_start[i]]-dg_train[T_start[i]-dg_train.offset],\n",
    "                                   n_steps=T_dur)\n",
    "        out_ubo = sortL96fromChannels(out_ubo * dg_train.std + dg_train.mean)\n",
    "\n",
    "        fig = plot_rollout(out_np, out_model, out_comparison=out_ubo, n_start=0, n_steps=T_dur, K=K)\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.legend(['trained model', '(only slow vars)', 'upper-bound model', '(only slow vars)'])\n",
    "        plt.suptitle('integration scheme: ' + str(MFWD))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check long-term stability\n",
    "- simulate long trajectories using different emulators/simulators\n",
    "- do so in small segments, checking repeatedly for divergence and keeping memory footprint limited\n",
    "- also accumulate statistics along trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = -1 # pick one starting point for a long simulation\n",
    "n_rep = 100\n",
    "T_dur = 10 if J>0 else 100\n",
    "\n",
    "\n",
    "bins_K = np.linspace(out[0,int(spin_up_time/dt):, :K].min(),\n",
    "                     out[0,int(spin_up_time/dt):, :K].max(),\n",
    "                     50)\n",
    "\n",
    "if J > 0:\n",
    "    bins_J = np.linspace(out[0,int(spin_up_time/dt):, K:].min(),\n",
    "                         out[0,int(spin_up_time/dt):, K:].max(),\n",
    "                         50)\n",
    "else: \n",
    "    bins_J = np.array([])\n",
    "\n",
    "def calc_state_pdf(out, T_start=0, T_end=-1, bins_K=None, bins_J=None, n_bins= 100):\n",
    "    # assuming out.shape = (T, K*(J+1))\n",
    "    out_K = out[T_start:T_end][:K]\n",
    "    bins_K = np.linspace(out_K.min(), out_K.max(), n_bins) if bins_K is None else bins_K\n",
    "    pdf_K, bins_K = np.histogram(out_K, bins=bins_K, density=True)\n",
    "\n",
    "    if J > 0:\n",
    "        out_J = out[T_start:T_end][K:]\n",
    "        bins_J = np.linspace(out_J.min(), out_J.max(), n_bins) if bins_J is None else bins_J\n",
    "        pdf_J, bins_J = np.histogram(out_J, bins=bins_J, density=True)\n",
    "    else: \n",
    "        pdf_J, bins_J = None, np.array([])\n",
    "    return pdf_K, pdf_J, bins_K, bins_J\n",
    "\n",
    "def iter_solve_and_stats(out, simulate_fun, T_dur, n_rep, bins_K, bins_J):\n",
    "\n",
    "    pdf_Ks, pdf_Js = [], []\n",
    "    for n in range(n_rep):\n",
    "        print(f'- {n+1} / {n_rep}')\n",
    "        out = simulate_fun(y0=out[-1:].copy(),\n",
    "                          dy0=None,\n",
    "                          n_steps=T_dur)\n",
    "        out = out * dg_train.std + dg_train.mean\n",
    "        assert not np.any(np.isnan(out))        \n",
    "        pdf_K_n, pdf_J_n, _, _ = calc_state_pdf(sortL96fromChannels(out), bins_K=bins_K, bins_J=bins_J)\n",
    "        pdf_Ks.append(pdf_K_n)\n",
    "        if J > 0:\n",
    "            pdf_Js.append(pdf_J_n)\n",
    "    return out, pdf_Ks, pdf_Js\n",
    "            \n",
    "print('simulating from simulator')\n",
    "out_np, pdf_K_np, pdf_J_np = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  np_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "print('simulating from emulator')\n",
    "out_model, pdf_K_model, pdf_J_model = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  model_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "print('simulating from reference emulator')\n",
    "out_ubo, pdf_K_ubo, pdf_J_ubo = iter_solve_and_stats(dg_train[T_start[i]].copy().reshape(1,J+1,K), \n",
    "                                                  ubo_simulate, \n",
    "                                                  int(T_dur/dt), \n",
    "                                                  n_rep, \n",
    "                                                  bins_K, \n",
    "                                                  bins_J)\n",
    "\n",
    "# quick visual inspection\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(sortL96fromChannels(out_np).T, aspect='auto')\n",
    "plt.title('simulator')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(sortL96fromChannels(out_model).T, aspect='auto')\n",
    "plt.title('learned emulator')\n",
    "plt.colorbar()\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(sortL96fromChannels(out_ubo).T, aspect='auto')\n",
    "plt.title('upper-bound emulator')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare distribution of state values (PSD)\n",
    "- cf. figure 9 from Chattopadhyay, Hassanzadeh, Subramanian (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "xx = np.repeat(bins_K[:-1],2)\n",
    "xx[::2] -= np.diff(bins_K).mean()\n",
    "\n",
    "Q = 4 # divide long simulation into 4 quarters, track distributions invididually\n",
    "o = n_rep//Q\n",
    "\n",
    "if J > 0:\n",
    "    plt.figure(figsize=(16,12))\n",
    "    plt.subplot(1,2,1)\n",
    "else:\n",
    "    plt.figure(figsize=(12,12))\n",
    "\n",
    "\n",
    "for q in range(Q):\n",
    "    if J > 0:\n",
    "        plt.subplot(2,4,2*q+1)\n",
    "    else:\n",
    "        plt.subplot(2,2,q+1)\n",
    "    # histogram of all data is average of (normalized!) histograms\n",
    "    pdf_K_np_q = np.stack(pdf_K_np[q*o:(q+1)*o]).mean(axis=0)\n",
    "    pdf_K_model_q = np.stack(pdf_K_model[q*o:(q+1)*o]).mean(axis=0)\n",
    "    pdf_K_ubo_q = np.stack(pdf_K_ubo[q*o:(q+1)*o]).mean(axis=0)\n",
    "\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_np_q, 2), color='g', label='simulator')\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_ubo_q, 2), color='k', label='upper-bound model')\n",
    "    plt.semilogy(xx, np.repeat(pdf_K_model_q, 2), color='b', label='model')\n",
    "    if J > 0:\n",
    "        plt.xlabel('value of slow variables')\n",
    "    else:\n",
    "        plt.xlabel('state value')        \n",
    "    plt.ylabel('relative frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    if J > 0:\n",
    "        plt.subplot(2,4,2*(q+1))\n",
    "        xx = np.repeat(bins_J[:-1],2)\n",
    "        xx[::2] -= np.diff(bins_J).mean()\n",
    "        pdf_J_np_q = np.stack(pdf_J_np[q*o:(q+1)*o]).mean(axis=0)\n",
    "        pdf_J_model_q = np.stack(pdf_J_model[q*o:(q+1)*o]).mean(axis=0)\n",
    "        pdf_J_ubo_q = np.stack(pdf_J_ubo[q*o:(q+1)*o]).mean(axis=0)\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_np_q, 2), color='g')\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_ubo_q, 2), color='k')\n",
    "        plt.semilogy(xx, np.repeat(pdf_J_model_q, 2), color='b')\n",
    "        plt.xlabel('value of fast variables')\n",
    "        plt.ylabel('relative frequency')\n",
    "    plt.title(f'Quarter {q+1} / {Q}')\n",
    "    plt.suptitle('distribution of state values for long simulation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tbd: compare Lyapunov spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah, hf fiddling the pytorch model into Julia..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# share notebook results via html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir='/gpfs/home/nonnenma/projects/lab_coord/mdml_wiki/marcel/emulators' --to html eval_emulator.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
