{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import L96sim\n",
    "\n",
    "from L96_emulator.util import dtype, dtype_np, device, as_tensor\n",
    "\n",
    "res_dir = '/gpfs/work/nonnenma/results/emulators/L96/'\n",
    "data_dir = '/gpfs/work/nonnenma/data/emulators/L96/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick a (trained) emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from L96_emulator.eval import load_model_from_exp_conf\n",
    "from L96_emulator.networks import named_network\n",
    "from L96_emulator.run import setup\n",
    "\n",
    "exp_id = 77 # 77: deepNet trained on 9.600 datapoints, best of 3 random network intitializations\n",
    "\n",
    "exp_names = os.listdir('experiments/')   \n",
    "conf_exp = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id))[0][0]][:-4]\n",
    "\n",
    "args = setup(conf_exp=f'experiments/{conf_exp}.yml')\n",
    "args.pop('conf_exp')\n",
    "\n",
    "K,J = args['K'], args['J']\n",
    "args['model_forwarder'] = 'rk4_default'\n",
    "if args['padding_mode'] == 'valid':\n",
    "    print('switching from local training to global evaluation')\n",
    "    args['padding_mode'] = 'circular'\n",
    "model, model_forwarder, training_outputs = load_model_from_exp_conf(res_dir, args)\n",
    "\n",
    "if not training_outputs is None:\n",
    "    training_loss, validation_loss = training_outputs['training_loss'], training_outputs['validation_loss']\n",
    "\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    seq_length = args['seq_length']\n",
    "    plt.semilogy(validation_loss, label=conf_exp+ f' ({seq_length * (J+1)}-dim)')\n",
    "    plt.title('training')\n",
    "    plt.ylabel('validation error')\n",
    "    plt.legend()\n",
    "    fig.patch.set_facecolor('xkcd:white')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick the reference simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id_gt = 35 # 35: bilinear network with analytical weights\n",
    "\n",
    "conf_exp_gt = exp_names[np.where(np.array([name[:2] for name in exp_names])==str(exp_id_gt))[0][0]][:-4]\n",
    "args_gt = setup(conf_exp=f'experiments/{conf_exp_gt}.yml')\n",
    "args_gt.pop('conf_exp')\n",
    "assert args_gt['model_forwarder'] == 'rk4_default' and args_gt['dt'] == args['dt']\n",
    "model_gt, model_forwarder_gt, _ = load_model_from_exp_conf(res_dir, args_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reference simulation (also to get initial point for rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.run import sel_dataset_class\n",
    "from L96_emulator.util import predictor_corrector, rk4_default, get_data\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "dt = args['dt']\n",
    "T, spinup = 5, 10\n",
    "\n",
    "N_trials = 1 \n",
    "F, h, b, c = 8., 1., 10., 10.\n",
    "\n",
    "out, _ = get_data(K=K, J=J, T=T+spinup, dt=dt, N_trials=N_trials, F=F, h=h, b=b, c=c, \n",
    "                  resimulate=True, solver=rk4_default,\n",
    "                  save_sim=False, data_dir=data_dir)\n",
    "out = out.reshape(1, *out.shape) if len(out.shape)==2 else out\n",
    "\n",
    "\n",
    "DatasetClass = sel_dataset_class(prediction_task='state', N_trials=N_trials, local=False)\n",
    "dg_train = DatasetClass(data=out, J=J, offset=1, normalize=False, \n",
    "                   start=int(spinup/dt), \n",
    "                   end=int(np.floor((T+spinup)/dt)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi-step integration (rollout) for different solvers: simulator and emulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from L96_emulator.eval import get_rollout_fun, plot_rollout\n",
    "from L96_emulator.networks import Model_forwarder_rk4default\n",
    "from L96_emulator.util import sortL96fromChannels, sortL96intoChannels\n",
    "\n",
    "\n",
    "T_dur = int(T/dt)\n",
    "t = int(spinup/dt)\n",
    "\n",
    "\n",
    "model_forwarder = Model_forwarder_rk4default(model=model, dt=dt)\n",
    "model_simulate = get_rollout_fun(dg_train, model_forwarder, prediction_task='state')\n",
    "out_model = model_simulate(y0=dg_train[t].copy(), dy0=None, n_steps=T_dur)\n",
    "out_model = sortL96fromChannels(out_model * dg_train.std + dg_train.mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input-output Jacobians of next-state predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from L96_emulator.networks import Model_forwarder_predictorCorrector, Model_forwarder_rk4default\n",
    "from L96_emulator.util import calc_jakobian_onelevelL96_tendencies, calc_jakobian_rk4, get_jacobian_torch\n",
    "from L96sim.L96_base import f1\n",
    "import torch \n",
    "\n",
    "dX_dt = np.empty(K*(J+1), dtype=dtype_np)\n",
    "def fun(t, x):\n",
    "    return f1(x, F, dX_dt, K)\n",
    "\n",
    "def model_np(inputs):\n",
    "    return fun(0., inputs).copy()\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "        \n",
    "model_forwarder = Model_forwarder_rk4default(model=model, dt=dt)\n",
    "        \n",
    "inputs = out[0][t] \n",
    "inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "inputs_torch.requires_grad = True\n",
    "            \n",
    "\n",
    "J_model = get_jacobian_torch(model_forwarder, inputs=inputs_torch, n=K)\n",
    "\n",
    "J_np = calc_jakobian_rk4(inputs, calc_f=model_np, \n",
    "             calc_J_f=calc_jakobian_onelevelL96_tendencies, dt=dt, n=K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some partial derivaties evaluated along example direction in state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "def get_partials_torch(model, inputs, i,j):\n",
    "    inputs.grad = None\n",
    "    L = model(inputs).flatten()[i] # f_i(x)\n",
    "    L.backward()\n",
    "    dfdx = inputs.grad.detach().cpu().numpy().flatten()[j] #df_i/dx_j ? \n",
    "    return dfdx\n",
    "\n",
    "\n",
    "inputs_base = out[0,t]\n",
    "inputs_end = out[0, -1]\n",
    "\n",
    "inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "inputs_torch.requires_grad = True\n",
    "\n",
    "nbh = 2\n",
    "offsets  = np.concatenate(((np.arange(-nbh,0)), np.arange(nbh)+1))\n",
    "\n",
    "locations = 0 * np.ones(len(offsets), dtype=np.int)\n",
    "clrs = ['b', 'r', 'g', 'k', 'c', 'orange']\n",
    "\n",
    "xx = np.linspace(-0.5, 1.5, 51)\n",
    "dfdx = np.zeros((len(offsets), len(xx)))\n",
    "dfdx_gt = np.zeros((len(offsets), len(xx)))\n",
    "\n",
    "for i in range(len(offsets)):\n",
    "\n",
    "    j = locations[i] + offsets[i]\n",
    "                     \n",
    "\n",
    "    for n in range(len(xx)):\n",
    "        inputs = (1-xx[n]) * inputs_base  + xx[n] * inputs_end\n",
    "        inputs_torch = as_tensor(sortL96intoChannels(np.atleast_2d(inputs.copy()),J=J))\n",
    "        inputs_torch.requires_grad = True\n",
    "                     \n",
    "        dfdx[i, n] = get_partials_torch(model_forwarder, inputs_torch, \n",
    "                                     i=locations[i], \n",
    "                                     j=locations[i] + offsets[i])\n",
    "        dfdx_gt[i, n] = get_partials_torch(model_forwarder_gt, inputs_torch,\n",
    "                                        i=locations[i], \n",
    "                                        j=locations[i] + offsets[i])\n",
    "                     \n",
    "    plt.plot(xx, dfdx[i], color=clrs[i])\n",
    "    plt.plot(xx, dfdx_gt[i], '--', color=clrs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compose figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "\n",
    "fontsize = 14\n",
    "\n",
    "sol_np = sortL96fromChannels(dg_train.data[t:t+T_dur+1]).T\n",
    "sol_model = out_model.T\n",
    "sols = [sol_np, sol_model, sol_model - sol_np]  \n",
    "cmaps = ['viridis', 'viridis', 'bwr']\n",
    "labels = ['model', 'emulator', 'difference']\n",
    "\n",
    "vmin, vmax = np.min(np.stack((sol_np, sol_model))), np.max(np.stack((sol_np, sol_model)))\n",
    "clims = [[vmin, vmax], [vmin,vmax], [-np.max(np.abs(sol_np-sol_model)), np.max(np.abs(sol_np-sol_model))]]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "############\n",
    "# rollouts #\n",
    "############\n",
    "for i in range(len(sols)):\n",
    "    plt.subplot(len(sols),2,2*i+1)\n",
    "    plt.imshow(sols[i], aspect='auto', cmap=cmaps[i], vmin=clims[i][0], vmax=clims[i][1])\n",
    "    plt.colorbar()\n",
    "    plt.yticks([], fontsize=fontsize)\n",
    "    if i < len(sols) - 1:\n",
    "        plt.xticks([], fontsize=fontsize)\n",
    "    else:\n",
    "        plt.xticks(np.arange(0, sols[i].shape[1],50),\n",
    "                  dt * np.arange(0, sols[i].shape[1],50),\n",
    "                  fontsize=fontsize)\n",
    "        plt.xlabel('time [au]', fontsize=fontsize)\n",
    "    plt.ylabel(labels[i], fontsize=fontsize)\n",
    "\n",
    "    \n",
    "#############\n",
    "# Jacobians #\n",
    "#############\n",
    "\n",
    "jacobians = [J_np, J_model, J_model-J_np]\n",
    "vmin, vmax = np.min(np.stack((J_np, J_model))), np.max(np.stack((J_np, J_model)))\n",
    "clims = [[vmin, vmax], [vmin,vmax], [-np.max(np.abs(J_np-J_model)), np.max(np.abs(J_np-J_model))]]\n",
    "cmaps = ['viridis', 'viridis', 'bwr']\n",
    "labels = ['model', 'emulator', 'difference']\n",
    "\n",
    "for i in range(len(jacobians)):\n",
    "    ax =  plt.subplot(2,2*len(jacobians),len(jacobians)+1+i)\n",
    "    plt.imshow(jacobians[i], cmap=cmaps[i], vmin=clims[i][0], vmax=clims[i][1])\n",
    "    plt.xticks([], fontsize=fontsize)\n",
    "    plt.yticks([], fontsize=fontsize)\n",
    "    plt.xlabel(labels[i], fontsize=fontsize)\n",
    "    if i == 1:\n",
    "        plt.title(r'state-update Jacobians $\\frac{dx_{t+1}}{dx_t}$',fontsize=fontsize)\n",
    "        axins = inset_axes(ax, width=\"5%\", height=\"100%\", loc='lower left',\n",
    "                           bbox_to_anchor=(1.05, 0., 1, 1), bbox_transform=ax.transAxes, borderpad=0)\n",
    "        plt.colorbar(cax=axins)\n",
    "    if i == 2:\n",
    "        axins = inset_axes(ax, width=\"5%\", height=\"100%\", loc='lower left',\n",
    "                           bbox_to_anchor=(1.05, 0., 1, 1), bbox_transform=ax.transAxes, borderpad=0)\n",
    "        plt.colorbar(cax=axins)\n",
    "\n",
    "\n",
    "#######################\n",
    "# partial derivatives #\n",
    "#######################\n",
    "\n",
    "ax = plt.subplot(2,2,4)\n",
    "plt.title(r'partial derivatives ',fontsize=fontsize)\n",
    "plt.ylabel(r'$\\frac{\\partial{}x_{k,t+1}}{\\partial{}x_{k+l,t}}$', fontsize=1.5*fontsize)\n",
    "plt.xlabel(r'$\\alpha$', fontsize=fontsize)\n",
    "\n",
    "clrs = ['b', 'c', 'g', 'y']\n",
    "labels = [r'$l = -2$', r'$l = -1$', r'$l = +1$', r'$l = +2$']\n",
    "for i in range(len(dfdx)):\n",
    "    plt.plot(xx, dfdx[i], color=clrs[i], label=labels[i])\n",
    "    plt.plot(xx, dfdx_gt[i], '--', color=clrs[i])\n",
    "plt.plot(xx.min()-1, 0, 'k--', label='emulator')\n",
    "\n",
    "plt.axis([xx.min(), xx.max(), 1.05*dfdx.min(), 1.05*dfdx.max()])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.xticks([0, 1], fontsize=fontsize)\n",
    "plt.yticks([-0.5, 0, 0.5], fontsize=fontsize)\n",
    "plt.legend(fontsize=fontsize, bbox_to_anchor=(1.0, 1), frameon=False)\n",
    "plt.plot([0,0], [-0.5, 0.5], 'k:')\n",
    "plt.plot([1,1], [-0.5, 0.5], 'k:')\n",
    "\n",
    "plt.savefig(res_dir + 'figs/emulator_intro.pdf', bbox_inches='tight', pad_inches=0, frameon=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifiy that Jacobian has 12 off-diagonal elements per row (8x locations to the left and 4x to the right): \n",
    "#plt.imshow(np.log(np.abs(J_model-np.eye(K))))\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
